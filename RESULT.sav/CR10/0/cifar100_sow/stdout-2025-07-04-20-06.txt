============================================================================================================
Arguments =
	approach: sow
	batch_size: 64
	clipping: 1.0
	datasets: ['cifar100']
	eval_on_train: True
	exp_name: None
	fix_bn: True
	gpu: 0
	gridsearch_tasks: -1
	keep_existing_head: False
	last_layer_analysis: False
	log: ['disk']
	lr: 0.0263039750973134
	lr_factor: 3.0
	lr_first: None
	lr_min: 1e-05
	lr_patience: 30
	momentum: 0.9
	multi_softmax: True
	nc_first_task: None
	nepochs: 60
	network: resnet50_32sow
	no_cudnn_deterministic: False
	num_tasks: 10
	num_workers: 4
	pin_memory: True
	pretrained: False
	results_path: ../RESULT_AAAI2025/CR10/0
	save_models: True
	seed: 0
	stop_at_task: 0
	validation: 0.1
	warmup_lr_factor: 1.0
	warmup_nepochs: 0
	weight_decay: 0.0
============================================================================================================
	device: cuda:0
============================================================================================================
Network arguments =
	dropout: 0.417598542370663
	fix_features: True
	load_features: True
	pretrained_path: ../Conv-Model/ResNet50-TinyImageNet.pt
============================================================================================================
../src/networks/INIT/1024.dict: loading..
ResNet50_32SOW,__init__: features is loaded (../Conv-Model/ResNet50-TinyImageNet.pt)
Fix_Features: Done
Approach arguments =
	acc_margin: 0.004
	all_outputs: False
	loss_margin: 0.006
	sow_lr: 0.0263039750973134
	sow_mo: 0.9
============================================================================================================
Exemplars dataset arguments =
	exemplar_selection: random
	num_exemplars: 0
	num_exemplars_per_class: 0
============================================================================================================
class_indices: [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
[(0, 10), (1, 10), (2, 10), (3, 10), (4, 10), (5, 10), (6, 10), (7, 10), (8, 10), (9, 10)]
************************************************************************************************************
Task  0
************************************************************************************************************
LLL_Net(
  (model): ResNet50_32SOW(
    in_channels=3, in_H=32, in_W=32, training=True, fix_features=True, fix_classifier=False
    (features): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
      (3): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (4): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (3): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (5): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (3): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (4): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (5): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (6): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (7): AdaptiveAvgPool2d(output_size=(1, 1))
    )
    (classifier): Sequential(
      (0): Dropout(p=0.417598542370663, inplace=False)
      (1): SOW_V3(in_features=1024, out_features=1024, num_tasks=10, dtype=torch.float64, device=cuda:0)
      (2): ReLU()
    )
    (fc): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1024, out_features=10, bias=True)
  )
)
SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.0263039750973134
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0

Parameter Group 1
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.0263039750973134
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0
)
 Group[0]  param_num=161, free_param_num=2
 Group[1]  param_num=3, free_param_num=3
sow: rank=1024, freezed_rank=0
 a:        ['-6.76e+00', '+4.95e+00'] .... ['+1.29e+00', '-2.74e+00']
| Epoch   1, lr=2.6e-02 time=  4.4s/  2.1s | Train: loss=1.380, TAw acc= 53.3% | Valid: time=  0.4s loss=1.087, TAw acc= 59.6% | *
| Epoch   2, lr=2.6e-02 time=  3.5s/  2.5s | Train: loss=1.288, TAw acc= 56.8% | Valid: time=  0.5s loss=0.915, TAw acc= 68.8% | *
| Epoch   3, lr=2.6e-02 time=  3.9s/  2.4s | Train: loss=1.192, TAw acc= 59.1% | Valid: time=  0.5s loss=0.804, TAw acc= 71.2% | *
| Epoch   4, lr=2.6e-02 time=  3.1s/  1.9s | Train: loss=1.230, TAw acc= 57.3% | Valid: time=  0.4s loss=0.798, TAw acc= 71.0% | *
| Epoch   5, lr=2.6e-02 time=  3.0s/  2.0s | Train: loss=1.136, TAw acc= 62.9% | Valid: time=  0.4s loss=0.796, TAw acc= 72.8% | *
| Epoch   6, lr=2.6e-02 time=  4.3s/  1.9s | Train: loss=1.112, TAw acc= 63.0% | Valid: time=  0.4s loss=0.731, TAw acc= 73.0% | *
| Epoch   7, lr=2.6e-02 time=  3.7s/  2.4s | Train: loss=1.221, TAw acc= 59.3% | Valid: time=  0.5s loss=0.782, TAw acc= 72.8% |
| Epoch   8, lr=2.6e-02 time=  3.9s/  2.4s | Train: loss=1.138, TAw acc= 61.8% | Valid: time=  0.5s loss=0.763, TAw acc= 73.8% |
| Epoch   9, lr=2.6e-02 time=  3.9s/  2.4s | Train: loss=1.122, TAw acc= 62.5% | Valid: time=  0.5s loss=0.772, TAw acc= 71.6% |
| Epoch  10, lr=2.6e-02 time=  3.9s/  2.4s | Train: loss=1.160, TAw acc= 60.6% | Valid: time=  0.5s loss=0.866, TAw acc= 71.0% |
| Epoch  11, lr=2.6e-02 time=  3.9s/  2.4s | Train: loss=1.093, TAw acc= 64.1% | Valid: time=  0.5s loss=0.713, TAw acc= 75.0% | *
| Epoch  12, lr=2.6e-02 time=  3.9s/  2.4s | Train: loss=1.106, TAw acc= 62.2% | Valid: time=  0.5s loss=0.753, TAw acc= 73.4% |
| Epoch  13, lr=2.6e-02 time=  3.9s/  2.4s | Train: loss=1.137, TAw acc= 61.7% | Valid: time=  0.5s loss=0.808, TAw acc= 72.6% |
| Epoch  14, lr=2.6e-02 time=  3.9s/  2.4s | Train: loss=1.080, TAw acc= 63.6% | Valid: time=  0.5s loss=0.743, TAw acc= 72.8% |
| Epoch  15, lr=2.6e-02 time=  3.9s/  2.4s | Train: loss=1.085, TAw acc= 62.8% | Valid: time=  0.5s loss=0.735, TAw acc= 74.6% |
| Epoch  16, lr=2.6e-02 time=  3.8s/  2.4s | Train: loss=1.052, TAw acc= 64.5% | Valid: time=  0.5s loss=0.684, TAw acc= 74.8% | *
| Epoch  17, lr=2.6e-02 time=  4.0s/  2.5s | Train: loss=1.009, TAw acc= 66.5% | Valid: time=  0.5s loss=0.751, TAw acc= 74.6% |
| Epoch  18, lr=2.6e-02 time=  3.8s/  2.4s | Train: loss=1.004, TAw acc= 66.2% | Valid: time=  0.5s loss=0.666, TAw acc= 77.4% | *
| Epoch  19, lr=2.6e-02 time=  3.9s/  2.4s | Train: loss=1.050, TAw acc= 65.2% | Valid: time=  0.5s loss=0.693, TAw acc= 76.8% |
| Epoch  20, lr=2.6e-02 time=  4.0s/  2.4s | Train: loss=1.007, TAw acc= 65.9% | Valid: time=  0.5s loss=0.721, TAw acc= 73.2% |
| Epoch  21, lr=2.6e-02 time=  3.9s/  2.4s | Train: loss=1.070, TAw acc= 64.0% | Valid: time=  0.5s loss=0.689, TAw acc= 75.6% |
| Epoch  22, lr=2.6e-02 time=  3.9s/  2.4s | Train: loss=1.024, TAw acc= 65.2% | Valid: time=  0.5s loss=0.612, TAw acc= 80.2% | *
| Epoch  23, lr=2.6e-02 time=  3.8s/  2.4s | Train: loss=1.002, TAw acc= 66.8% | Valid: time=  0.5s loss=0.700, TAw acc= 76.0% |
| Epoch  24, lr=2.6e-02 time=  3.8s/  2.4s | Train: loss=1.001, TAw acc= 66.0% | Valid: time=  0.5s loss=0.693, TAw acc= 75.8% |
| Epoch  25, lr=2.6e-02 time=  4.0s/  2.4s | Train: loss=1.025, TAw acc= 64.9% | Valid: time=  0.5s loss=0.675, TAw acc= 76.8% |
| Epoch  26, lr=2.6e-02 time=  3.9s/  2.4s | Train: loss=1.008, TAw acc= 66.3% | Valid: time=  0.5s loss=0.680, TAw acc= 74.6% |
| Epoch  27, lr=2.6e-02 time=  3.9s/  2.4s | Train: loss=0.988, TAw acc= 67.2% | Valid: time=  0.5s loss=0.701, TAw acc= 74.0% |
| Epoch  28, lr=2.6e-02 time=  3.9s/  2.4s | Train: loss=1.037, TAw acc= 65.8% | Valid: time=  0.5s loss=0.673, TAw acc= 76.6% |
| Epoch  29, lr=2.6e-02 time=  3.9s/  2.4s | Train: loss=0.973, TAw acc= 68.1% | Valid: time=  0.5s loss=0.676, TAw acc= 75.6% |
| Epoch  30, lr=2.6e-02 time=  3.9s/  2.4s | Train: loss=0.997, TAw acc= 66.9% | Valid: time=  0.5s loss=0.673, TAw acc= 77.0% |
| Epoch  31, lr=2.6e-02 time=  3.9s/  2.4s | Train: loss=0.973, TAw acc= 67.0% | Valid: time=  0.5s loss=0.661, TAw acc= 77.4% |
| Epoch  32, lr=2.6e-02 time=  4.0s/  2.4s | Train: loss=1.034, TAw acc= 65.2% | Valid: time=  0.5s loss=0.663, TAw acc= 76.2% |
| Epoch  33, lr=2.6e-02 time=  3.9s/  2.4s | Train: loss=0.968, TAw acc= 66.9% | Valid: time=  0.5s loss=0.680, TAw acc= 75.6% |
| Epoch  34, lr=2.6e-02 time=  3.8s/  2.4s | Train: loss=1.013, TAw acc= 65.0% | Valid: time=  0.5s loss=0.650, TAw acc= 77.2% |
| Epoch  35, lr=2.6e-02 time=  3.9s/  2.4s | Train: loss=0.993, TAw acc= 66.0% | Valid: time=  0.5s loss=0.710, TAw acc= 75.6% |
| Epoch  36, lr=2.6e-02 time=  3.9s/  2.4s | Train: loss=0.982, TAw acc= 67.3% | Valid: time=  0.5s loss=0.642, TAw acc= 77.8% |
| Epoch  37, lr=2.6e-02 time=  3.9s/  2.4s | Train: loss=1.004, TAw acc= 65.8% | Valid: time=  0.5s loss=0.714, TAw acc= 74.8% |
| Epoch  38, lr=2.6e-02 time=  3.9s/  2.4s | Train: loss=0.945, TAw acc= 67.8% | Valid: time=  0.5s loss=0.651, TAw acc= 76.0% |
| Epoch  39, lr=2.6e-02 time=  3.9s/  2.5s | Train: loss=0.965, TAw acc= 67.5% | Valid: time=  0.5s loss=0.683, TAw acc= 76.8% |
| Epoch  40, lr=2.6e-02 time=  3.9s/  2.5s | Train: loss=0.978, TAw acc= 67.3% | Valid: time=  0.5s loss=0.614, TAw acc= 78.2% |
| Epoch  41, lr=2.6e-02 time=  3.9s/  2.5s | Train: loss=0.935, TAw acc= 68.6% | Valid: time=  0.5s loss=0.635, TAw acc= 79.2% |
| Epoch  42, lr=2.6e-02 time=  3.9s/  2.5s | Train: loss=0.980, TAw acc= 67.2% | Valid: time=  0.5s loss=0.643, TAw acc= 79.2% |
| Epoch  43, lr=2.6e-02 time=  3.9s/  2.5s | Train: loss=0.991, TAw acc= 65.9% | Valid: time=  0.5s loss=0.710, TAw acc= 75.0% |
| Epoch  44, lr=2.6e-02 time=  3.9s/  2.4s | Train: loss=0.957, TAw acc= 68.7% | Valid: time=  0.5s loss=0.665, TAw acc= 78.2% |
| Epoch  45, lr=2.6e-02 time=  3.9s/  2.4s | Train: loss=0.981, TAw acc= 67.0% | Valid: time=  0.5s loss=0.643, TAw acc= 76.6% |
| Epoch  46, lr=2.6e-02 time=  3.9s/  2.4s | Train: loss=0.980, TAw acc= 66.4% | Valid: time=  0.5s loss=0.708, TAw acc= 74.0% |
| Epoch  47, lr=2.6e-02 time=  3.9s/  2.4s | Train: loss=0.925, TAw acc= 69.4% | Valid: time=  0.5s loss=0.642, TAw acc= 79.0% |
| Epoch  48, lr=2.6e-02 time=  3.9s/  2.4s | Train: loss=0.955, TAw acc= 68.3% | Valid: time=  0.5s loss=0.653, TAw acc= 75.6% |
| Epoch  49, lr=2.6e-02 time=  3.9s/  2.4s | Train: loss=0.981, TAw acc= 68.0% | Valid: time=  0.5s loss=0.668, TAw acc= 79.0% |
| Epoch  50, lr=2.6e-02 time=  3.9s/  2.4s | Train: loss=0.940, TAw acc= 68.1% | Valid: time=  0.5s loss=0.665, TAw acc= 77.4% |
| Epoch  51, lr=2.6e-02 time=  3.9s/  2.4s | Train: loss=0.939, TAw acc= 68.8% | Valid: time=  0.5s loss=0.615, TAw acc= 78.6% |
| Epoch  52, lr=2.6e-02 time=  3.9s/  2.4s | Train: loss=0.922, TAw acc= 69.2% | Valid: time=  0.5s loss=0.606, TAw acc= 79.6% | *
| Epoch  53, lr=2.6e-02 time=  3.9s/  2.4s | Train: loss=0.945, TAw acc= 68.2% | Valid: time=  0.5s loss=0.587, TAw acc= 80.4% | *
| Epoch  54, lr=2.6e-02 time=  3.9s/  2.5s | Train: loss=1.036, TAw acc= 64.9% | Valid: time=  0.5s loss=0.675, TAw acc= 76.0% |
| Epoch  55, lr=2.6e-02 time=  3.9s/  2.4s | Train: loss=0.969, TAw acc= 67.4% | Valid: time=  0.5s loss=0.666, TAw acc= 77.4% |
| Epoch  56, lr=2.6e-02 time=  3.9s/  2.4s | Train: loss=1.016, TAw acc= 65.0% | Valid: time=  0.5s loss=0.656, TAw acc= 77.2% |
| Epoch  57, lr=2.6e-02 time=  3.9s/  2.4s | Train: loss=0.946, TAw acc= 68.6% | Valid: time=  0.5s loss=0.712, TAw acc= 76.8% |
| Epoch  58, lr=2.6e-02 time=  3.9s/  2.4s | Train: loss=0.937, TAw acc= 68.5% | Valid: time=  0.5s loss=0.660, TAw acc= 77.2% |
| Epoch  59, lr=2.6e-02 time=  3.9s/  2.4s | Train: loss=0.920, TAw acc= 69.3% | Valid: time=  0.5s loss=0.594, TAw acc= 79.0% |
| Epoch  60, lr=2.6e-02 time=  3.9s/  2.4s | Train: loss=0.922, TAw acc= 68.2% | Valid: time=  0.5s loss=0.681, TAw acc= 77.6% |
== Rank Reduction [task:0] ==
Debug-0:
  best_loss=0.587,   best_acc=0.804
loss_margin=0.006, acc_margin=0.004
sow: rank=1024, freezed_rank=0
 a:        ['-5.80e+00', '+4.96e+00'] .... ['+1.29e+00', '-2.74e+00']
 r=256, loss=1.456, acc=0.500 (rank=256)
 r=288, loss=1.470, acc=0.474 (rank=288)
 r=320, loss=1.197, acc=0.566 (rank=320)
 r=352, loss=1.035, acc=0.634 (rank=352)
 r=384, loss=0.909, acc=0.682 (rank=384)
 r=416, loss=0.789, acc=0.728 (rank=416)
 r=448, loss=0.678, acc=0.762 (rank=448)
 r=480, loss=0.637, acc=0.780 (rank=480)
 r=512, loss=0.576, acc=0.814 (rank=512)
 best_r=512, loss=0.576, acc=0.814
== Header Training for Low Rank [task:0] ==
Fix_Classifier: Done (training=False, fix_classifier=True)
Debug-1: loss=0.576, acc=0.814
sow: rank=512, freezed_rank=512
 a:        ['-5.80e+00', '+4.96e+00'] .... ['+7.31e+00', '-1.80e+308'] .... ['-1.80e+308', '-1.80e+308']
 a_backup: ['-5.80e+00', '+4.96e+00'] .... ['+7.31e+00', '+6.89e+00'] .... ['+1.29e+00', '-2.74e+00']
SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.008767991699104466
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0

Parameter Group 1
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.0263039750973134
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0
)
 Group[0]  param_num=161, free_param_num=2
 Group[1]  param_num=3, free_param_num=0
sow: rank=512, freezed_rank=512
 a:        ['-5.80e+00', '+4.96e+00'] .... ['+7.31e+00', '-1.80e+308'] .... ['-1.80e+308', '-1.80e+308']
 a_backup: ['-5.80e+00', '+4.96e+00'] .... ['+7.31e+00', '+6.89e+00'] .... ['+1.29e+00', '-2.74e+00']
| Epoch   1, lr=8.8e-03 time=  2.0s/  2.3s | Train: loss=0.892, TAw acc= 70.2% | Valid: time=  0.5s loss=0.621, TAw acc= 78.4% |
| Epoch   2, lr=8.8e-03 time=  2.1s/  2.4s | Train: loss=0.898, TAw acc= 70.7% | Valid: time=  0.5s loss=0.614, TAw acc= 78.4% |
| Epoch   3, lr=8.8e-03 time=  2.0s/  2.3s | Train: loss=0.874, TAw acc= 70.9% | Valid: time=  0.4s loss=0.622, TAw acc= 78.2% |
| Epoch   4, lr=8.8e-03 time=  2.0s/  2.3s | Train: loss=0.878, TAw acc= 71.8% | Valid: time=  0.5s loss=0.613, TAw acc= 77.8% |
| Epoch   5, lr=8.8e-03 time=  2.0s/  2.3s | Train: loss=0.869, TAw acc= 71.7% | Valid: time=  0.5s loss=0.633, TAw acc= 78.4% |
| Epoch   6, lr=8.8e-03 time=  2.1s/  2.4s | Train: loss=0.877, TAw acc= 71.2% | Valid: time=  0.5s loss=0.620, TAw acc= 79.2% |
| Epoch   7, lr=8.8e-03 time=  2.1s/  2.3s | Train: loss=0.869, TAw acc= 70.8% | Valid: time=  0.5s loss=0.630, TAw acc= 78.2% |
| Epoch   8, lr=8.8e-03 time=  2.0s/  2.3s | Train: loss=0.871, TAw acc= 70.1% | Valid: time=  0.5s loss=0.598, TAw acc= 79.6% |
| Epoch   9, lr=8.8e-03 time=  2.1s/  2.3s | Train: loss=0.835, TAw acc= 72.4% | Valid: time=  0.5s loss=0.643, TAw acc= 77.0% |
| Epoch  10, lr=8.8e-03 time=  2.0s/  2.3s | Train: loss=0.866, TAw acc= 71.6% | Valid: time=  0.5s loss=0.617, TAw acc= 78.2% |
| Epoch  11, lr=8.8e-03 time=  2.0s/  2.3s | Train: loss=0.864, TAw acc= 71.3% | Valid: time=  0.5s loss=0.644, TAw acc= 76.8% |
| Epoch  12, lr=8.8e-03 time=  2.0s/  2.3s | Train: loss=0.872, TAw acc= 71.1% | Valid: time=  0.5s loss=0.628, TAw acc= 77.8% |
| Epoch  13, lr=8.8e-03 time=  2.1s/  2.4s | Train: loss=0.847, TAw acc= 72.0% | Valid: time=  0.4s loss=0.601, TAw acc= 80.0% |
| Epoch  14, lr=8.8e-03 time=  2.1s/  2.3s | Train: loss=0.829, TAw acc= 72.9% | Valid: time=  0.5s loss=0.602, TAw acc= 80.4% |
| Epoch  15, lr=8.8e-03 time=  2.1s/  2.2s | Train: loss=0.855, TAw acc= 71.4% | Valid: time=  0.4s loss=0.636, TAw acc= 78.4% |
| Epoch  16, lr=8.8e-03 time=  1.6s/  1.8s | Train: loss=0.819, TAw acc= 73.2% | Valid: time=  0.4s loss=0.613, TAw acc= 79.0% |
| Epoch  17, lr=8.8e-03 time=  1.8s/  2.6s | Train: loss=0.842, TAw acc= 71.9% | Valid: time=  0.4s loss=0.628, TAw acc= 77.4% |
| Epoch  18, lr=8.8e-03 time=  1.6s/  2.0s | Train: loss=0.831, TAw acc= 72.2% | Valid: time=  0.4s loss=0.609, TAw acc= 79.8% |
| Epoch  19, lr=8.8e-03 time=  1.7s/  1.8s | Train: loss=0.824, TAw acc= 72.5% | Valid: time=  0.4s loss=0.620, TAw acc= 78.2% |
| Epoch  20, lr=8.8e-03 time=  1.6s/  1.8s | Train: loss=0.815, TAw acc= 73.3% | Valid: time=  0.4s loss=0.605, TAw acc= 80.2% |
| Epoch  21, lr=8.8e-03 time=  1.6s/  3.0s | Train: loss=0.831, TAw acc= 72.7% | Valid: time=  0.4s loss=0.620, TAw acc= 80.0% |
| Epoch  22, lr=8.8e-03 time=  1.6s/  2.1s | Train: loss=0.810, TAw acc= 72.9% | Valid: time=  0.5s loss=0.599, TAw acc= 80.2% |
| Epoch  23, lr=8.8e-03 time=  2.1s/  2.3s | Train: loss=0.838, TAw acc= 71.6% | Valid: time=  0.5s loss=0.629, TAw acc= 79.0% |
| Epoch  24, lr=8.8e-03 time=  1.9s/  1.8s | Train: loss=0.805, TAw acc= 73.8% | Valid: time=  0.4s loss=0.629, TAw acc= 78.2% |
| Epoch  25, lr=8.8e-03 time=  1.6s/  2.4s | Train: loss=0.825, TAw acc= 72.3% | Valid: time=  0.4s loss=0.629, TAw acc= 79.4% |
| Epoch  26, lr=8.8e-03 time=  1.7s/  1.8s | Train: loss=0.833, TAw acc= 72.2% | Valid: time=  0.4s loss=0.613, TAw acc= 80.0% |
| Epoch  27, lr=8.8e-03 time=  2.0s/  1.8s | Train: loss=0.812, TAw acc= 73.5% | Valid: time=  0.4s loss=0.631, TAw acc= 78.0% |
| Epoch  28, lr=8.8e-03 time=  1.6s/  1.8s | Train: loss=0.812, TAw acc= 72.8% | Valid: time=  0.4s loss=0.641, TAw acc= 79.0% |
| Epoch  29, lr=8.8e-03 time=  1.6s/  2.7s | Train: loss=0.817, TAw acc= 72.6% | Valid: time=  0.4s loss=0.607, TAw acc= 80.6% |
| Epoch  30, lr=8.8e-03 time=  1.6s/  1.8s | Train: loss=0.810, TAw acc= 72.2% | Valid: time=  0.4s loss=0.645, TAw acc= 78.6% | lr=2.9e-03
| Epoch  31, lr=2.9e-03 time=  1.6s/  1.8s | Train: loss=0.927, TAw acc= 69.2% | Valid: time=  0.9s loss=0.614, TAw acc= 78.8% |
| Epoch  32, lr=2.9e-03 time=  1.9s/  1.8s | Train: loss=0.909, TAw acc= 69.8% | Valid: time=  0.4s loss=0.634, TAw acc= 77.8% |
| Epoch  33, lr=2.9e-03 time=  2.0s/  2.3s | Train: loss=0.897, TAw acc= 70.3% | Valid: time=  0.4s loss=0.625, TAw acc= 78.6% |
| Epoch  34, lr=2.9e-03 time=  2.0s/  2.3s | Train: loss=0.883, TAw acc= 71.4% | Valid: time=  0.5s loss=0.629, TAw acc= 78.2% |
| Epoch  35, lr=2.9e-03 time=  2.1s/  2.3s | Train: loss=0.907, TAw acc= 70.5% | Valid: time=  0.4s loss=0.636, TAw acc= 78.0% |
| Epoch  36, lr=2.9e-03 time=  1.9s/  1.8s | Train: loss=0.879, TAw acc= 70.9% | Valid: time=  0.4s loss=0.631, TAw acc= 78.2% |
| Epoch  37, lr=2.9e-03 time=  1.6s/  1.8s | Train: loss=0.890, TAw acc= 71.0% | Valid: time=  0.4s loss=0.634, TAw acc= 78.4% |
| Epoch  38, lr=2.9e-03 time=  1.6s/  1.9s | Train: loss=0.901, TAw acc= 69.7% | Valid: time=  0.4s loss=0.624, TAw acc= 78.6% |
| Epoch  39, lr=2.9e-03 time=  1.6s/  1.8s | Train: loss=0.872, TAw acc= 72.6% | Valid: time=  0.9s loss=0.627, TAw acc= 78.6% |
| Epoch  40, lr=2.9e-03 time=  1.9s/  1.8s | Train: loss=0.865, TAw acc= 71.5% | Valid: time=  0.4s loss=0.617, TAw acc= 78.6% |
| Epoch  41, lr=2.9e-03 time=  2.0s/  2.3s | Train: loss=0.864, TAw acc= 71.9% | Valid: time=  0.5s loss=0.619, TAw acc= 78.8% |
| Epoch  42, lr=2.9e-03 time=  2.1s/  2.3s | Train: loss=0.881, TAw acc= 69.6% | Valid: time=  0.5s loss=0.625, TAw acc= 78.2% |
| Epoch  43, lr=2.9e-03 time=  2.0s/  2.3s | Train: loss=0.876, TAw acc= 71.0% | Valid: time=  0.5s loss=0.626, TAw acc= 78.2% |
| Epoch  44, lr=2.9e-03 time=  2.1s/  2.3s | Train: loss=0.885, TAw acc= 71.2% | Valid: time=  0.5s loss=0.624, TAw acc= 78.2% |
| Epoch  45, lr=2.9e-03 time=  2.1s/  2.3s | Train: loss=0.873, TAw acc= 70.7% | Valid: time=  0.5s loss=0.629, TAw acc= 77.8% |
| Epoch  46, lr=2.9e-03 time=  1.6s/  1.8s | Train: loss=0.869, TAw acc= 71.3% | Valid: time=  0.4s loss=0.637, TAw acc= 77.4% |
| Epoch  47, lr=2.9e-03 time=  1.6s/  1.8s | Train: loss=0.867, TAw acc= 71.9% | Valid: time=  0.4s loss=0.618, TAw acc= 78.4% |
| Epoch  48, lr=2.9e-03 time=  1.7s/  1.8s | Train: loss=0.876, TAw acc= 71.5% | Valid: time=  0.4s loss=0.630, TAw acc= 78.4% |
| Epoch  49, lr=2.9e-03 time=  2.9s/  1.8s | Train: loss=0.863, TAw acc= 71.4% | Valid: time=  0.4s loss=0.628, TAw acc= 78.4% |
| Epoch  50, lr=2.9e-03 time=  1.9s/  2.3s | Train: loss=0.868, TAw acc= 71.2% | Valid: time=  0.5s loss=0.627, TAw acc= 78.0% |
| Epoch  51, lr=2.9e-03 time=  2.0s/  2.4s | Train: loss=0.868, TAw acc= 71.2% | Valid: time=  0.5s loss=0.618, TAw acc= 78.6% |
| Epoch  52, lr=2.9e-03 time=  2.1s/  2.3s | Train: loss=0.858, TAw acc= 71.3% | Valid: time=  0.5s loss=0.625, TAw acc= 77.8% |
| Epoch  53, lr=2.9e-03 time=  2.0s/  2.3s | Train: loss=0.853, TAw acc= 72.2% | Valid: time=  0.5s loss=0.612, TAw acc= 78.8% |
| Epoch  54, lr=2.9e-03 time=  2.1s/  2.3s | Train: loss=0.863, TAw acc= 70.8% | Valid: time=  0.5s loss=0.613, TAw acc= 78.8% |
| Epoch  55, lr=2.9e-03 time=  1.6s/  1.8s | Train: loss=0.845, TAw acc= 71.5% | Valid: time=  0.4s loss=0.619, TAw acc= 77.8% |
| Epoch  56, lr=2.9e-03 time=  2.1s/  1.8s | Train: loss=0.845, TAw acc= 72.0% | Valid: time=  0.4s loss=0.622, TAw acc= 78.0% |
| Epoch  57, lr=2.9e-03 time=  1.6s/  1.8s | Train: loss=0.839, TAw acc= 72.9% | Valid: time=  0.4s loss=0.622, TAw acc= 78.0% |
| Epoch  58, lr=2.9e-03 time=  2.7s/  1.8s | Train: loss=0.849, TAw acc= 73.0% | Valid: time=  0.4s loss=0.620, TAw acc= 78.4% |
| Epoch  59, lr=2.9e-03 time=  2.0s/  2.3s | Train: loss=0.844, TAw acc= 72.2% | Valid: time=  0.5s loss=0.614, TAw acc= 78.6% |
| Epoch  60, lr=2.9e-03 time=  2.0s/  2.3s | Train: loss=0.830, TAw acc= 72.1% | Valid: time=  0.5s loss=0.625, TAw acc= 78.6% | lr=9.7e-04
Debug-2: loss=0.576, acc=0.814
sow: rank=512, freezed_rank=512
 a:        ['-5.80e+00', '+4.96e+00'] .... ['+7.31e+00', '-1.80e+308'] .... ['-1.80e+308', '-1.80e+308']
 a_backup: ['-5.80e+00', '+4.96e+00'] .... ['+7.31e+00', '+6.89e+00'] .... ['+1.29e+00', '-2.74e+00']
Free_Classifier: Done (training=True, fix_classifier=False)
Debug-3: loss=0.587 acc=0.804
sow: rank=1024, freezed_rank=512
 a:        ['-5.80e+00', '+4.96e+00'] .... ['+7.31e+00', '+6.89e+00'] .... ['+1.29e+00', '-2.74e+00']
 a_backup: ['-5.80e+00', '+4.96e+00'] .... ['+7.31e+00', '+6.89e+00'] .... ['+1.29e+00', '-2.74e+00']
Header Training: Finished.
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.592 | TAw acc= 80.3%, forg=  0.0%| TAg acc= 80.3%, forg=  0.0% <<<
Save at ../RESULT_AAAI2025/CR10/0/cifar100_sow
************************************************************************************************************
Task  1
************************************************************************************************************
LLL_Net(
  (model): ResNet50_32SOW(
    in_channels=3, in_H=32, in_W=32, training=False, fix_features=True, fix_classifier=False
    (features): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
      (3): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (4): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (3): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (5): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (3): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (4): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (5): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (6): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (7): AdaptiveAvgPool2d(output_size=(1, 1))
    )
    (classifier): Sequential(
      (0): Dropout(p=0.417598542370663, inplace=False)
      (1): SOW_V3(in_features=1024, out_features=1024, num_tasks=10, dtype=torch.float64, device=cuda:0)
      (2): ReLU()
    )
    (fc): Sequential()
  )
  (heads): ModuleList(
    (0-1): 2 x Linear(in_features=1024, out_features=10, bias=True)
  )
)
SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.0263039750973134
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0

Parameter Group 1
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.0263039750973134
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0
)
 Group[0]  param_num=161, free_param_num=2
 Group[1]  param_num=3, free_param_num=3
sow: rank=1024, freezed_rank=512
 a:        ['-5.80e+00', '+4.96e+00'] .... ['+7.31e+00', '+6.89e+00'] .... ['+1.29e+00', '-2.74e+00']
 a_backup: ['-5.80e+00', '+4.96e+00'] .... ['+7.31e+00', '+6.89e+00'] .... ['+1.29e+00', '-2.74e+00']
| Epoch   1, lr=2.6e-02 time=  3.4s/  1.9s | Train: loss=1.483, TAw acc= 48.0% | Valid: time=  0.4s loss=1.193, TAw acc= 57.8% | *
| Epoch   2, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=1.401, TAw acc= 51.2% | Valid: time=  0.4s loss=1.123, TAw acc= 62.0% | *
| Epoch   3, lr=2.6e-02 time=  2.8s/  3.0s | Train: loss=1.381, TAw acc= 52.5% | Valid: time=  0.4s loss=1.058, TAw acc= 63.0% | *
| Epoch   4, lr=2.6e-02 time=  3.2s/  2.5s | Train: loss=1.354, TAw acc= 53.0% | Valid: time=  0.5s loss=1.075, TAw acc= 63.2% |
| Epoch   5, lr=2.6e-02 time=  4.0s/  2.4s | Train: loss=1.356, TAw acc= 52.9% | Valid: time=  0.5s loss=1.045, TAw acc= 61.2% | *
| Epoch   6, lr=2.6e-02 time=  3.7s/  2.5s | Train: loss=1.365, TAw acc= 52.7% | Valid: time=  0.5s loss=1.032, TAw acc= 63.4% | *
| Epoch   7, lr=2.6e-02 time=  3.7s/  1.9s | Train: loss=1.306, TAw acc= 54.8% | Valid: time=  0.4s loss=1.040, TAw acc= 62.8% |
| Epoch   8, lr=2.6e-02 time=  2.9s/  2.2s | Train: loss=1.307, TAw acc= 54.6% | Valid: time=  0.9s loss=1.006, TAw acc= 64.2% | *
| Epoch   9, lr=2.6e-02 time=  2.8s/  2.3s | Train: loss=1.330, TAw acc= 54.0% | Valid: time=  0.5s loss=0.974, TAw acc= 66.6% | *
| Epoch  10, lr=2.6e-02 time=  3.7s/  2.4s | Train: loss=1.309, TAw acc= 55.1% | Valid: time=  0.5s loss=1.015, TAw acc= 65.4% |
| Epoch  11, lr=2.6e-02 time=  3.6s/  2.4s | Train: loss=1.309, TAw acc= 56.2% | Valid: time=  0.5s loss=0.982, TAw acc= 66.0% |
| Epoch  12, lr=2.6e-02 time=  3.2s/  2.0s | Train: loss=1.311, TAw acc= 54.2% | Valid: time=  0.4s loss=1.008, TAw acc= 65.2% |
| Epoch  13, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=1.275, TAw acc= 56.5% | Valid: time=  0.5s loss=1.020, TAw acc= 66.6% |
| Epoch  14, lr=2.6e-02 time=  2.8s/  3.2s | Train: loss=1.269, TAw acc= 56.3% | Valid: time=  0.4s loss=0.962, TAw acc= 66.4% | *
| Epoch  15, lr=2.6e-02 time=  3.0s/  2.4s | Train: loss=1.279, TAw acc= 55.2% | Valid: time=  0.5s loss=0.980, TAw acc= 66.6% |
| Epoch  16, lr=2.6e-02 time=  3.6s/  2.4s | Train: loss=1.266, TAw acc= 57.5% | Valid: time=  0.5s loss=0.966, TAw acc= 66.4% |
| Epoch  17, lr=2.6e-02 time=  3.7s/  2.4s | Train: loss=1.264, TAw acc= 56.5% | Valid: time=  0.5s loss=0.961, TAw acc= 65.4% | *
| Epoch  18, lr=2.6e-02 time=  3.2s/  2.0s | Train: loss=1.274, TAw acc= 55.9% | Valid: time=  0.4s loss=0.995, TAw acc= 65.8% |
| Epoch  19, lr=2.6e-02 time=  2.8s/  1.8s | Train: loss=1.250, TAw acc= 56.5% | Valid: time=  0.4s loss=0.960, TAw acc= 66.8% | *
| Epoch  20, lr=2.6e-02 time=  3.8s/  1.9s | Train: loss=1.279, TAw acc= 56.7% | Valid: time=  0.4s loss=0.945, TAw acc= 67.8% | *
| Epoch  21, lr=2.6e-02 time=  3.0s/  2.1s | Train: loss=1.273, TAw acc= 57.2% | Valid: time=  0.5s loss=0.950, TAw acc= 65.4% |
| Epoch  22, lr=2.6e-02 time=  2.8s/  2.0s | Train: loss=1.260, TAw acc= 56.5% | Valid: time=  0.4s loss=0.981, TAw acc= 66.2% |
| Epoch  23, lr=2.6e-02 time=  3.0s/  2.6s | Train: loss=1.253, TAw acc= 57.1% | Valid: time=  0.4s loss=0.946, TAw acc= 66.2% |
| Epoch  24, lr=2.6e-02 time=  3.2s/  2.4s | Train: loss=1.248, TAw acc= 57.0% | Valid: time=  0.5s loss=0.928, TAw acc= 67.0% | *
| Epoch  25, lr=2.6e-02 time=  3.7s/  2.4s | Train: loss=1.270, TAw acc= 55.4% | Valid: time=  0.5s loss=0.991, TAw acc= 65.8% |
| Epoch  26, lr=2.6e-02 time=  3.7s/  2.5s | Train: loss=1.235, TAw acc= 57.6% | Valid: time=  0.5s loss=0.994, TAw acc= 65.0% |
| Epoch  27, lr=2.6e-02 time=  3.7s/  2.5s | Train: loss=1.268, TAw acc= 55.5% | Valid: time=  0.5s loss=0.965, TAw acc= 67.0% |
| Epoch  28, lr=2.6e-02 time=  3.0s/  1.9s | Train: loss=1.234, TAw acc= 57.3% | Valid: time=  0.4s loss=0.948, TAw acc= 65.6% |
| Epoch  29, lr=2.6e-02 time=  2.8s/  2.0s | Train: loss=1.243, TAw acc= 56.6% | Valid: time=  0.4s loss=1.008, TAw acc= 66.2% |
| Epoch  30, lr=2.6e-02 time=  4.1s/  1.9s | Train: loss=1.246, TAw acc= 56.6% | Valid: time=  0.4s loss=0.997, TAw acc= 66.0% |
| Epoch  31, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=1.229, TAw acc= 57.6% | Valid: time=  0.5s loss=1.029, TAw acc= 64.8% |
| Epoch  32, lr=2.6e-02 time=  3.4s/  1.9s | Train: loss=1.232, TAw acc= 56.9% | Valid: time=  0.4s loss=0.962, TAw acc= 67.2% |
| Epoch  33, lr=2.6e-02 time=  3.2s/  1.9s | Train: loss=1.233, TAw acc= 58.4% | Valid: time=  0.5s loss=0.938, TAw acc= 67.6% |
| Epoch  34, lr=2.6e-02 time=  2.8s/  2.0s | Train: loss=1.230, TAw acc= 56.9% | Valid: time=  0.4s loss=0.947, TAw acc= 67.2% |
| Epoch  35, lr=2.6e-02 time=  3.4s/  1.9s | Train: loss=1.228, TAw acc= 57.6% | Valid: time=  0.4s loss=0.959, TAw acc= 67.8% |
| Epoch  36, lr=2.6e-02 time=  3.5s/  2.0s | Train: loss=1.233, TAw acc= 57.1% | Valid: time=  0.4s loss=0.989, TAw acc= 66.6% |
| Epoch  37, lr=2.6e-02 time=  2.9s/  1.9s | Train: loss=1.213, TAw acc= 57.5% | Valid: time=  0.4s loss=0.969, TAw acc= 68.6% |
| Epoch  38, lr=2.6e-02 time=  2.8s/  2.0s | Train: loss=1.244, TAw acc= 56.9% | Valid: time=  0.4s loss=0.945, TAw acc= 67.6% |
| Epoch  39, lr=2.6e-02 time=  2.8s/  3.2s | Train: loss=1.204, TAw acc= 58.2% | Valid: time=  0.4s loss=0.970, TAw acc= 67.6% |
| Epoch  40, lr=2.6e-02 time=  3.1s/  2.5s | Train: loss=1.248, TAw acc= 57.6% | Valid: time=  0.5s loss=0.964, TAw acc= 67.2% |
| Epoch  41, lr=2.6e-02 time=  3.7s/  2.5s | Train: loss=1.208, TAw acc= 58.7% | Valid: time=  0.5s loss=0.954, TAw acc= 67.8% |
| Epoch  42, lr=2.6e-02 time=  3.5s/  1.9s | Train: loss=1.217, TAw acc= 58.0% | Valid: time=  0.5s loss=0.984, TAw acc= 67.8% |
| Epoch  43, lr=2.6e-02 time=  2.9s/  3.1s | Train: loss=1.244, TAw acc= 56.3% | Valid: time=  0.4s loss=0.966, TAw acc= 66.2% |
| Epoch  44, lr=2.6e-02 time=  2.8s/  2.5s | Train: loss=1.226, TAw acc= 57.2% | Valid: time=  0.5s loss=0.949, TAw acc= 67.4% |
| Epoch  45, lr=2.6e-02 time=  3.7s/  2.4s | Train: loss=1.213, TAw acc= 58.8% | Valid: time=  0.5s loss=0.998, TAw acc= 66.4% |
| Epoch  46, lr=2.6e-02 time=  3.7s/  2.3s | Train: loss=1.242, TAw acc= 56.8% | Valid: time=  0.4s loss=0.983, TAw acc= 67.0% |
| Epoch  47, lr=2.6e-02 time=  2.9s/  2.1s | Train: loss=1.202, TAw acc= 58.5% | Valid: time=  0.4s loss=0.976, TAw acc= 67.6% |
| Epoch  48, lr=2.6e-02 time=  3.0s/  1.9s | Train: loss=1.207, TAw acc= 58.7% | Valid: time=  0.4s loss=0.946, TAw acc= 67.6% |
| Epoch  49, lr=2.6e-02 time=  2.8s/  2.0s | Train: loss=1.217, TAw acc= 57.9% | Valid: time=  0.4s loss=0.925, TAw acc= 68.4% | *
| Epoch  50, lr=2.6e-02 time=  3.9s/  1.9s | Train: loss=1.237, TAw acc= 57.5% | Valid: time=  0.4s loss=0.903, TAw acc= 68.8% | *
| Epoch  51, lr=2.6e-02 time=  3.3s/  2.4s | Train: loss=1.231, TAw acc= 58.0% | Valid: time=  0.5s loss=0.940, TAw acc= 67.0% |
| Epoch  52, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.217, TAw acc= 57.8% | Valid: time=  0.5s loss=0.959, TAw acc= 68.2% |
| Epoch  53, lr=2.6e-02 time=  3.3s/  2.0s | Train: loss=1.215, TAw acc= 58.1% | Valid: time=  0.5s loss=1.003, TAw acc= 67.4% |
| Epoch  54, lr=2.6e-02 time=  3.0s/  2.2s | Train: loss=1.221, TAw acc= 58.5% | Valid: time=  0.4s loss=0.978, TAw acc= 67.0% |
| Epoch  55, lr=2.6e-02 time=  2.9s/  1.9s | Train: loss=1.243, TAw acc= 56.2% | Valid: time=  0.4s loss=1.027, TAw acc= 64.0% |
| Epoch  56, lr=2.6e-02 time=  2.8s/  2.0s | Train: loss=1.206, TAw acc= 57.7% | Valid: time=  0.4s loss=0.992, TAw acc= 68.4% |
| Epoch  57, lr=2.6e-02 time=  3.1s/  2.6s | Train: loss=1.206, TAw acc= 58.7% | Valid: time=  0.4s loss=0.966, TAw acc= 68.8% |
| Epoch  58, lr=2.6e-02 time=  3.2s/  2.5s | Train: loss=1.170, TAw acc= 59.2% | Valid: time=  0.5s loss=0.946, TAw acc= 68.4% |
| Epoch  59, lr=2.6e-02 time=  3.7s/  2.4s | Train: loss=1.206, TAw acc= 57.9% | Valid: time=  0.5s loss=0.928, TAw acc= 68.8% |
| Epoch  60, lr=2.6e-02 time=  3.8s/  2.4s | Train: loss=1.192, TAw acc= 58.7% | Valid: time=  0.4s loss=0.932, TAw acc= 68.2% |
== Rank Reduction [task:1] ==
Debug-0:
  best_loss=0.903,   best_acc=0.688
loss_margin=0.006, acc_margin=0.004
sow: rank=1024, freezed_rank=512
 a:        ['-6.35e+00', '+4.96e+00'] .... ['+7.32e+00', '+6.91e+00'] .... ['+1.29e+00', '-2.74e+00']
 a_backup: ['-6.35e+00', '+4.96e+00'] .... ['+7.32e+00', '+6.91e+00'] .... ['+1.29e+00', '-2.74e+00']
 r=256, loss=1.943, acc=0.300 (rank=256)
 r=288, loss=1.820, acc=0.376 (rank=288)
 r=320, loss=1.679, acc=0.424 (rank=320)
 r=352, loss=1.636, acc=0.454 (rank=352)
 r=384, loss=1.564, acc=0.466 (rank=384)
 r=416, loss=1.340, acc=0.538 (rank=416)
 r=448, loss=1.153, acc=0.604 (rank=448)
 r=480, loss=1.049, acc=0.632 (rank=480)
 r=512, loss=0.918, acc=0.688 (rank=512)
 r=544, loss=0.922, acc=0.688 (rank=544)
 r=576, loss=0.924, acc=0.688 (rank=576)
 r=608, loss=0.917, acc=0.692 (rank=608)
 r=640, loss=0.910, acc=0.692 (rank=640)
 r=672, loss=0.896, acc=0.688 (rank=672)
 best_r=672, loss=0.896, acc=0.688
== Header Training for Low Rank [task:1] ==
Fix_Classifier: Done (training=False, fix_classifier=True)
Debug-1: loss=0.896, acc=0.688
sow: rank=672, freezed_rank=672
 a:        ['-6.35e+00', '+4.96e+00'] .... ['+6.23e+00', '-1.80e+308'] .... ['-1.80e+308', '-1.80e+308']
 a_backup: ['-6.35e+00', '+4.96e+00'] .... ['+6.23e+00', '+6.96e+00'] .... ['+1.29e+00', '-2.74e+00']
SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.008767991699104466
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0

Parameter Group 1
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.0263039750973134
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0
)
 Group[0]  param_num=161, free_param_num=2
 Group[1]  param_num=3, free_param_num=0
sow: rank=672, freezed_rank=672
 a:        ['-6.35e+00', '+4.96e+00'] .... ['+6.23e+00', '-1.80e+308'] .... ['-1.80e+308', '-1.80e+308']
 a_backup: ['-6.35e+00', '+4.96e+00'] .... ['+6.23e+00', '+6.96e+00'] .... ['+1.29e+00', '-2.74e+00']
| Epoch   1, lr=8.8e-03 time=  1.6s/  1.9s | Train: loss=1.272, TAw acc= 56.6% | Valid: time=  0.4s loss=0.936, TAw acc= 67.6% |
| Epoch   2, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=1.254, TAw acc= 57.0% | Valid: time=  0.4s loss=0.943, TAw acc= 68.0% |
| Epoch   3, lr=8.8e-03 time=  1.6s/  2.7s | Train: loss=1.264, TAw acc= 56.6% | Valid: time=  0.6s loss=0.946, TAw acc= 67.8% |
| Epoch   4, lr=8.8e-03 time=  1.6s/  1.9s | Train: loss=1.228, TAw acc= 59.5% | Valid: time=  0.5s loss=0.955, TAw acc= 67.2% |
| Epoch   5, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=1.230, TAw acc= 58.7% | Valid: time=  0.5s loss=0.963, TAw acc= 67.4% |
| Epoch   6, lr=8.8e-03 time=  2.1s/  2.4s | Train: loss=1.232, TAw acc= 58.3% | Valid: time=  0.5s loss=0.962, TAw acc= 66.8% |
| Epoch   7, lr=8.8e-03 time=  2.1s/  1.9s | Train: loss=1.211, TAw acc= 58.3% | Valid: time=  0.4s loss=0.953, TAw acc= 68.2% |
| Epoch   8, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=1.222, TAw acc= 58.2% | Valid: time=  0.9s loss=0.947, TAw acc= 67.0% |
| Epoch   9, lr=8.8e-03 time=  1.9s/  1.9s | Train: loss=1.211, TAw acc= 59.2% | Valid: time=  0.4s loss=0.956, TAw acc= 68.0% |
| Epoch  10, lr=8.8e-03 time=  2.1s/  2.4s | Train: loss=1.211, TAw acc= 58.8% | Valid: time=  0.5s loss=0.959, TAw acc= 67.2% |
| Epoch  11, lr=8.8e-03 time=  2.1s/  2.4s | Train: loss=1.223, TAw acc= 58.5% | Valid: time=  0.5s loss=0.938, TAw acc= 68.4% |
| Epoch  12, lr=8.8e-03 time=  2.0s/  1.8s | Train: loss=1.211, TAw acc= 59.0% | Valid: time=  0.4s loss=0.947, TAw acc= 68.0% |
| Epoch  13, lr=8.8e-03 time=  1.6s/  1.9s | Train: loss=1.201, TAw acc= 58.3% | Valid: time=  0.5s loss=0.954, TAw acc= 68.0% |
| Epoch  14, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=1.217, TAw acc= 58.3% | Valid: time=  0.4s loss=0.937, TAw acc= 68.6% |
| Epoch  15, lr=8.8e-03 time=  2.7s/  1.9s | Train: loss=1.208, TAw acc= 59.4% | Valid: time=  0.4s loss=0.947, TAw acc= 67.8% |
| Epoch  16, lr=8.8e-03 time=  2.1s/  2.4s | Train: loss=1.209, TAw acc= 58.3% | Valid: time=  0.5s loss=0.940, TAw acc= 67.8% |
| Epoch  17, lr=8.8e-03 time=  2.1s/  2.4s | Train: loss=1.201, TAw acc= 59.0% | Valid: time=  0.5s loss=0.953, TAw acc= 68.2% |
| Epoch  18, lr=8.8e-03 time=  1.7s/  2.2s | Train: loss=1.191, TAw acc= 59.8% | Valid: time=  0.4s loss=0.957, TAw acc= 67.6% |
| Epoch  19, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=1.194, TAw acc= 59.0% | Valid: time=  0.5s loss=0.948, TAw acc= 68.0% |
| Epoch  20, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=1.196, TAw acc= 59.2% | Valid: time=  0.9s loss=0.946, TAw acc= 68.4% |
| Epoch  21, lr=8.8e-03 time=  2.0s/  1.9s | Train: loss=1.193, TAw acc= 59.0% | Valid: time=  0.4s loss=0.963, TAw acc= 67.0% |
| Epoch  22, lr=8.8e-03 time=  2.1s/  2.4s | Train: loss=1.200, TAw acc= 59.2% | Valid: time=  0.5s loss=0.958, TAw acc= 67.6% |
| Epoch  23, lr=8.8e-03 time=  2.1s/  2.4s | Train: loss=1.194, TAw acc= 59.1% | Valid: time=  0.5s loss=0.940, TAw acc= 68.2% |
| Epoch  24, lr=8.8e-03 time=  2.1s/  2.4s | Train: loss=1.199, TAw acc= 59.4% | Valid: time=  0.5s loss=0.946, TAw acc= 68.2% |
| Epoch  25, lr=8.8e-03 time=  2.1s/  1.9s | Train: loss=1.206, TAw acc= 59.2% | Valid: time=  0.7s loss=0.950, TAw acc= 67.8% |
| Epoch  26, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=1.186, TAw acc= 59.0% | Valid: time=  0.4s loss=0.952, TAw acc= 68.0% |
| Epoch  27, lr=8.8e-03 time=  1.7s/  2.9s | Train: loss=1.181, TAw acc= 59.7% | Valid: time=  0.4s loss=0.954, TAw acc= 67.8% |
| Epoch  28, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=1.179, TAw acc= 59.7% | Valid: time=  0.4s loss=0.942, TAw acc= 67.8% |
| Epoch  29, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=1.171, TAw acc= 60.6% | Valid: time=  0.4s loss=0.943, TAw acc= 68.0% |
| Epoch  30, lr=8.8e-03 time=  1.6s/  1.8s | Train: loss=1.181, TAw acc= 60.0% | Valid: time=  0.4s loss=0.936, TAw acc= 69.0% | lr=2.9e-03
| Epoch  31, lr=2.9e-03 time=  1.6s/  1.9s | Train: loss=1.258, TAw acc= 58.3% | Valid: time=  0.4s loss=0.918, TAw acc= 69.2% |
| Epoch  32, lr=2.9e-03 time=  2.9s/  1.9s | Train: loss=1.239, TAw acc= 59.2% | Valid: time=  0.4s loss=0.935, TAw acc= 67.8% |
| Epoch  33, lr=2.9e-03 time=  2.0s/  2.4s | Train: loss=1.267, TAw acc= 57.4% | Valid: time=  0.5s loss=0.936, TAw acc= 68.0% |
| Epoch  34, lr=2.9e-03 time=  2.1s/  2.4s | Train: loss=1.259, TAw acc= 58.5% | Valid: time=  0.5s loss=0.943, TAw acc= 68.0% |
| Epoch  35, lr=2.9e-03 time=  2.1s/  2.4s | Train: loss=1.243, TAw acc= 58.4% | Valid: time=  0.5s loss=0.946, TAw acc= 67.8% |
| Epoch  36, lr=2.9e-03 time=  2.1s/  2.4s | Train: loss=1.256, TAw acc= 57.4% | Valid: time=  0.5s loss=0.944, TAw acc= 67.6% |
| Epoch  37, lr=2.9e-03 time=  2.1s/  2.4s | Train: loss=1.242, TAw acc= 58.3% | Valid: time=  0.5s loss=0.941, TAw acc= 67.8% |
| Epoch  38, lr=2.9e-03 time=  2.1s/  2.0s | Train: loss=1.252, TAw acc= 57.4% | Valid: time=  0.4s loss=0.943, TAw acc= 67.4% |
| Epoch  39, lr=2.9e-03 time=  1.9s/  1.8s | Train: loss=1.242, TAw acc= 58.6% | Valid: time=  0.4s loss=0.943, TAw acc= 67.4% |
| Epoch  40, lr=2.9e-03 time=  1.6s/  1.9s | Train: loss=1.244, TAw acc= 57.9% | Valid: time=  0.4s loss=0.942, TAw acc= 67.6% |
| Epoch  41, lr=2.9e-03 time=  1.7s/  3.1s | Train: loss=1.230, TAw acc= 58.5% | Valid: time=  0.4s loss=0.942, TAw acc= 67.6% |
| Epoch  42, lr=2.9e-03 time=  1.6s/  2.3s | Train: loss=1.232, TAw acc= 58.4% | Valid: time=  0.5s loss=0.945, TAw acc= 67.6% |
| Epoch  43, lr=2.9e-03 time=  2.1s/  2.4s | Train: loss=1.232, TAw acc= 59.0% | Valid: time=  0.5s loss=0.941, TAw acc= 68.0% |
| Epoch  44, lr=2.9e-03 time=  2.1s/  2.4s | Train: loss=1.218, TAw acc= 58.8% | Valid: time=  0.5s loss=0.938, TAw acc= 68.4% |
| Epoch  45, lr=2.9e-03 time=  2.1s/  1.8s | Train: loss=1.232, TAw acc= 58.4% | Valid: time=  0.4s loss=0.943, TAw acc= 67.6% |
| Epoch  46, lr=2.9e-03 time=  1.6s/  1.8s | Train: loss=1.210, TAw acc= 59.4% | Valid: time=  0.4s loss=0.941, TAw acc= 68.0% |
| Epoch  47, lr=2.9e-03 time=  1.7s/  1.8s | Train: loss=1.214, TAw acc= 59.7% | Valid: time=  0.4s loss=0.943, TAw acc= 67.8% |
| Epoch  48, lr=2.9e-03 time=  2.9s/  1.9s | Train: loss=1.230, TAw acc= 58.2% | Valid: time=  0.4s loss=0.947, TAw acc= 67.6% |
| Epoch  49, lr=2.9e-03 time=  1.8s/  2.4s | Train: loss=1.217, TAw acc= 58.6% | Valid: time=  0.5s loss=0.947, TAw acc= 67.8% |
| Epoch  50, lr=2.9e-03 time=  2.1s/  2.4s | Train: loss=1.216, TAw acc= 59.6% | Valid: time=  0.5s loss=0.941, TAw acc= 68.2% |
| Epoch  51, lr=2.9e-03 time=  2.1s/  2.4s | Train: loss=1.209, TAw acc= 59.6% | Valid: time=  0.5s loss=0.952, TAw acc= 67.0% |
| Epoch  52, lr=2.9e-03 time=  2.1s/  2.4s | Train: loss=1.227, TAw acc= 58.5% | Valid: time=  0.4s loss=0.951, TAw acc= 66.8% |
| Epoch  53, lr=2.9e-03 time=  1.6s/  1.9s | Train: loss=1.210, TAw acc= 58.9% | Valid: time=  0.4s loss=0.943, TAw acc= 67.6% |
| Epoch  54, lr=2.9e-03 time=  1.6s/  3.1s | Train: loss=1.233, TAw acc= 58.1% | Valid: time=  0.4s loss=0.946, TAw acc= 67.6% |
| Epoch  55, lr=2.9e-03 time=  1.6s/  2.2s | Train: loss=1.208, TAw acc= 58.8% | Valid: time=  0.5s loss=0.950, TAw acc= 67.4% |
| Epoch  56, lr=2.9e-03 time=  2.1s/  2.4s | Train: loss=1.218, TAw acc= 57.8% | Valid: time=  0.5s loss=0.947, TAw acc= 67.8% |
| Epoch  57, lr=2.9e-03 time=  2.1s/  2.4s | Train: loss=1.213, TAw acc= 58.6% | Valid: time=  0.4s loss=0.948, TAw acc= 67.4% |
| Epoch  58, lr=2.9e-03 time=  1.6s/  1.9s | Train: loss=1.212, TAw acc= 59.0% | Valid: time=  0.4s loss=0.942, TAw acc= 67.8% |
| Epoch  59, lr=2.9e-03 time=  2.6s/  1.9s | Train: loss=1.226, TAw acc= 58.7% | Valid: time=  0.4s loss=0.943, TAw acc= 68.2% |
| Epoch  60, lr=2.9e-03 time=  1.7s/  1.8s | Train: loss=1.222, TAw acc= 58.5% | Valid: time=  0.4s loss=0.944, TAw acc= 68.0% | lr=9.7e-04
Debug-2: loss=0.896, acc=0.688
sow: rank=672, freezed_rank=672
 a:        ['-6.35e+00', '+4.96e+00'] .... ['+6.23e+00', '-1.80e+308'] .... ['-1.80e+308', '-1.80e+308']
 a_backup: ['-6.35e+00', '+4.96e+00'] .... ['+6.23e+00', '+6.96e+00'] .... ['+1.29e+00', '-2.74e+00']
Free_Classifier: Done (training=True, fix_classifier=False)
Debug-3: loss=0.903 acc=0.688
sow: rank=1024, freezed_rank=672
 a:        ['-6.35e+00', '+4.96e+00'] .... ['+6.23e+00', '+6.96e+00'] .... ['+1.29e+00', '-2.74e+00']
 a_backup: ['-6.35e+00', '+4.96e+00'] .... ['+6.23e+00', '+6.96e+00'] .... ['+1.29e+00', '-2.74e+00']
Header Training: Finished.
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.592 | TAw acc= 80.3%, forg=  0.0%| TAg acc= 57.4%, forg= 22.9% <<<
>>> Test on task  1 : loss=0.953 | TAw acc= 67.2%, forg=  0.0%| TAg acc= 53.5%, forg=  0.0% <<<
Save at ../RESULT_AAAI2025/CR10/0/cifar100_sow
************************************************************************************************************
Task  2
************************************************************************************************************
LLL_Net(
  (model): ResNet50_32SOW(
    in_channels=3, in_H=32, in_W=32, training=False, fix_features=True, fix_classifier=False
    (features): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
      (3): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (4): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (3): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (5): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (3): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (4): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (5): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (6): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (7): AdaptiveAvgPool2d(output_size=(1, 1))
    )
    (classifier): Sequential(
      (0): Dropout(p=0.417598542370663, inplace=False)
      (1): SOW_V3(in_features=1024, out_features=1024, num_tasks=10, dtype=torch.float64, device=cuda:0)
      (2): ReLU()
    )
    (fc): Sequential()
  )
  (heads): ModuleList(
    (0-2): 3 x Linear(in_features=1024, out_features=10, bias=True)
  )
)
SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.0263039750973134
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0

Parameter Group 1
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.0263039750973134
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0
)
 Group[0]  param_num=161, free_param_num=2
 Group[1]  param_num=3, free_param_num=3
sow: rank=1024, freezed_rank=672
 a:        ['-6.35e+00', '+4.96e+00'] .... ['+6.23e+00', '+6.96e+00'] .... ['+1.29e+00', '-2.74e+00']
 a_backup: ['-6.35e+00', '+4.96e+00'] .... ['+6.23e+00', '+6.96e+00'] .... ['+1.29e+00', '-2.74e+00']
| Epoch   1, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=1.162, TAw acc= 61.3% | Valid: time=  0.4s loss=0.724, TAw acc= 75.4% | *
| Epoch   2, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=1.114, TAw acc= 62.6% | Valid: time=  0.4s loss=0.707, TAw acc= 77.4% | *
| Epoch   3, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=1.063, TAw acc= 63.8% | Valid: time=  0.4s loss=0.693, TAw acc= 76.2% | *
| Epoch   4, lr=2.6e-02 time=  4.1s/  1.9s | Train: loss=1.049, TAw acc= 64.5% | Valid: time=  0.5s loss=0.692, TAw acc= 76.8% | *
| Epoch   5, lr=2.6e-02 time=  3.7s/  2.4s | Train: loss=1.023, TAw acc= 65.9% | Valid: time=  0.5s loss=0.641, TAw acc= 77.2% | *
| Epoch   6, lr=2.6e-02 time=  3.3s/  1.9s | Train: loss=1.007, TAw acc= 66.0% | Valid: time=  0.7s loss=0.625, TAw acc= 78.4% | *
| Epoch   7, lr=2.6e-02 time=  2.8s/  2.3s | Train: loss=1.029, TAw acc= 65.4% | Valid: time=  0.5s loss=0.645, TAw acc= 76.8% |
| Epoch   8, lr=2.6e-02 time=  2.9s/  1.9s | Train: loss=0.981, TAw acc= 67.1% | Valid: time=  0.4s loss=0.571, TAw acc= 79.4% | *
| Epoch   9, lr=2.6e-02 time=  3.5s/  1.9s | Train: loss=1.032, TAw acc= 64.3% | Valid: time=  0.4s loss=0.685, TAw acc= 76.0% |
| Epoch  10, lr=2.6e-02 time=  2.7s/  1.9s | Train: loss=0.959, TAw acc= 67.7% | Valid: time=  0.4s loss=0.647, TAw acc= 78.4% |
| Epoch  11, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=0.973, TAw acc= 66.3% | Valid: time=  0.4s loss=0.617, TAw acc= 78.2% |
| Epoch  12, lr=2.6e-02 time=  4.1s/  1.9s | Train: loss=1.001, TAw acc= 66.1% | Valid: time=  0.4s loss=0.612, TAw acc= 79.2% |
| Epoch  13, lr=2.6e-02 time=  2.8s/  2.2s | Train: loss=0.968, TAw acc= 66.8% | Valid: time=  0.4s loss=0.611, TAw acc= 79.8% |
| Epoch  14, lr=2.6e-02 time=  2.9s/  1.9s | Train: loss=0.964, TAw acc= 67.3% | Valid: time=  0.4s loss=0.650, TAw acc= 78.2% |
| Epoch  15, lr=2.6e-02 time=  2.8s/  2.0s | Train: loss=0.968, TAw acc= 68.1% | Valid: time=  0.4s loss=0.600, TAw acc= 78.4% |
| Epoch  16, lr=2.6e-02 time=  4.0s/  1.9s | Train: loss=0.950, TAw acc= 68.3% | Valid: time=  0.4s loss=0.573, TAw acc= 79.6% |
| Epoch  17, lr=2.6e-02 time=  3.3s/  2.4s | Train: loss=0.935, TAw acc= 68.5% | Valid: time=  0.5s loss=0.568, TAw acc= 80.6% | *
| Epoch  18, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.938, TAw acc= 68.4% | Valid: time=  0.5s loss=0.551, TAw acc= 80.4% | *
| Epoch  19, lr=2.6e-02 time=  3.2s/  2.1s | Train: loss=0.964, TAw acc= 67.7% | Valid: time=  0.4s loss=0.640, TAw acc= 77.4% |
| Epoch  20, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=0.961, TAw acc= 68.1% | Valid: time=  0.4s loss=0.586, TAw acc= 78.2% |
| Epoch  21, lr=2.6e-02 time=  4.1s/  1.9s | Train: loss=0.954, TAw acc= 68.5% | Valid: time=  0.4s loss=0.568, TAw acc= 81.4% |
| Epoch  22, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.937, TAw acc= 68.9% | Valid: time=  0.5s loss=0.615, TAw acc= 78.0% |
| Epoch  23, lr=2.6e-02 time=  3.6s/  2.4s | Train: loss=0.957, TAw acc= 67.6% | Valid: time=  0.4s loss=0.590, TAw acc= 79.0% |
| Epoch  24, lr=2.6e-02 time=  2.8s/  2.2s | Train: loss=0.934, TAw acc= 68.8% | Valid: time=  0.7s loss=0.589, TAw acc= 79.8% |
| Epoch  25, lr=2.6e-02 time=  2.8s/  2.0s | Train: loss=0.928, TAw acc= 68.4% | Valid: time=  0.5s loss=0.572, TAw acc= 80.8% |
| Epoch  26, lr=2.6e-02 time=  3.7s/  1.9s | Train: loss=0.945, TAw acc= 68.4% | Valid: time=  0.5s loss=0.591, TAw acc= 78.6% |
| Epoch  27, lr=2.6e-02 time=  3.1s/  1.9s | Train: loss=0.916, TAw acc= 69.5% | Valid: time=  0.4s loss=0.602, TAw acc= 79.0% |
| Epoch  28, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=0.934, TAw acc= 68.2% | Valid: time=  0.4s loss=0.586, TAw acc= 79.4% |
| Epoch  29, lr=2.6e-02 time=  2.9s/  1.9s | Train: loss=0.923, TAw acc= 68.8% | Valid: time=  0.4s loss=0.577, TAw acc= 80.0% |
| Epoch  30, lr=2.6e-02 time=  4.0s/  2.1s | Train: loss=0.952, TAw acc= 67.4% | Valid: time=  0.5s loss=0.575, TAw acc= 79.2% |
| Epoch  31, lr=2.6e-02 time=  3.7s/  2.5s | Train: loss=0.923, TAw acc= 69.1% | Valid: time=  0.5s loss=0.575, TAw acc= 80.6% |
| Epoch  32, lr=2.6e-02 time=  3.6s/  2.4s | Train: loss=0.957, TAw acc= 68.0% | Valid: time=  0.5s loss=0.603, TAw acc= 78.2% |
| Epoch  33, lr=2.6e-02 time=  3.7s/  2.2s | Train: loss=0.919, TAw acc= 68.8% | Valid: time=  0.4s loss=0.540, TAw acc= 82.2% | *
| Epoch  34, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=0.915, TAw acc= 69.8% | Valid: time=  0.4s loss=0.583, TAw acc= 79.6% |
| Epoch  35, lr=2.6e-02 time=  2.8s/  3.2s | Train: loss=0.922, TAw acc= 69.1% | Valid: time=  0.5s loss=0.560, TAw acc= 80.4% |
| Epoch  36, lr=2.6e-02 time=  3.0s/  2.5s | Train: loss=0.975, TAw acc= 67.4% | Valid: time=  0.5s loss=0.566, TAw acc= 80.4% |
| Epoch  37, lr=2.6e-02 time=  3.7s/  2.1s | Train: loss=0.927, TAw acc= 68.8% | Valid: time=  0.4s loss=0.603, TAw acc= 79.4% |
| Epoch  38, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=0.925, TAw acc= 69.5% | Valid: time=  0.4s loss=0.585, TAw acc= 80.0% |
| Epoch  39, lr=2.6e-02 time=  4.1s/  1.9s | Train: loss=0.919, TAw acc= 68.9% | Valid: time=  0.4s loss=0.601, TAw acc= 77.4% |
| Epoch  40, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.922, TAw acc= 68.1% | Valid: time=  0.5s loss=0.582, TAw acc= 79.0% |
| Epoch  41, lr=2.6e-02 time=  3.7s/  2.2s | Train: loss=0.905, TAw acc= 69.5% | Valid: time=  0.4s loss=0.581, TAw acc= 79.4% |
| Epoch  42, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=0.938, TAw acc= 67.9% | Valid: time=  0.4s loss=0.553, TAw acc= 81.2% |
| Epoch  43, lr=2.6e-02 time=  4.1s/  1.9s | Train: loss=0.925, TAw acc= 69.8% | Valid: time=  0.5s loss=0.557, TAw acc= 81.2% |
| Epoch  44, lr=2.6e-02 time=  3.7s/  2.4s | Train: loss=0.898, TAw acc= 68.9% | Valid: time=  0.5s loss=0.579, TAw acc= 79.2% |
| Epoch  45, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.869, TAw acc= 70.9% | Valid: time=  0.5s loss=0.570, TAw acc= 80.6% |
| Epoch  46, lr=2.6e-02 time=  3.1s/  2.2s | Train: loss=0.894, TAw acc= 69.3% | Valid: time=  0.4s loss=0.590, TAw acc= 78.4% |
| Epoch  47, lr=2.6e-02 time=  2.9s/  1.9s | Train: loss=0.904, TAw acc= 69.4% | Valid: time=  0.4s loss=0.589, TAw acc= 79.8% |
| Epoch  48, lr=2.6e-02 time=  2.8s/  3.2s | Train: loss=0.930, TAw acc= 69.1% | Valid: time=  0.5s loss=0.557, TAw acc= 81.6% |
| Epoch  49, lr=2.6e-02 time=  3.0s/  2.4s | Train: loss=0.913, TAw acc= 70.1% | Valid: time=  0.5s loss=0.571, TAw acc= 80.0% |
| Epoch  50, lr=2.6e-02 time=  3.7s/  2.5s | Train: loss=0.906, TAw acc= 70.0% | Valid: time=  0.5s loss=0.564, TAw acc= 80.2% |
| Epoch  51, lr=2.6e-02 time=  3.0s/  1.9s | Train: loss=0.900, TAw acc= 70.2% | Valid: time=  0.4s loss=0.559, TAw acc= 80.4% |
| Epoch  52, lr=2.6e-02 time=  2.8s/  2.0s | Train: loss=0.918, TAw acc= 68.2% | Valid: time=  0.5s loss=0.561, TAw acc= 79.8% |
| Epoch  53, lr=2.6e-02 time=  4.1s/  1.9s | Train: loss=0.879, TAw acc= 70.5% | Valid: time=  0.4s loss=0.559, TAw acc= 79.4% |
| Epoch  54, lr=2.6e-02 time=  3.6s/  2.4s | Train: loss=0.901, TAw acc= 70.4% | Valid: time=  0.5s loss=0.568, TAw acc= 79.6% |
| Epoch  55, lr=2.6e-02 time=  3.6s/  2.4s | Train: loss=0.910, TAw acc= 68.7% | Valid: time=  0.5s loss=0.596, TAw acc= 79.4% |
| Epoch  56, lr=2.6e-02 time=  3.6s/  2.1s | Train: loss=0.895, TAw acc= 70.1% | Valid: time=  0.4s loss=0.544, TAw acc= 80.2% |
| Epoch  57, lr=2.6e-02 time=  2.7s/  1.9s | Train: loss=0.892, TAw acc= 69.7% | Valid: time=  0.4s loss=0.591, TAw acc= 80.0% |
| Epoch  58, lr=2.6e-02 time=  2.8s/  2.0s | Train: loss=0.922, TAw acc= 68.5% | Valid: time=  0.4s loss=0.597, TAw acc= 79.4% |
| Epoch  59, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=0.880, TAw acc= 70.4% | Valid: time=  0.4s loss=0.550, TAw acc= 81.0% |
| Epoch  60, lr=2.6e-02 time=  2.8s/  2.0s | Train: loss=0.897, TAw acc= 70.4% | Valid: time=  0.4s loss=0.551, TAw acc= 80.6% |
== Rank Reduction [task:2] ==
Debug-0:
  best_loss=0.540,   best_acc=0.822
loss_margin=0.006, acc_margin=0.004
sow: rank=1024, freezed_rank=672
 a:        ['-6.08e+00', '+4.96e+00'] .... ['+6.23e+00', '+6.96e+00'] .... ['+1.29e+00', '-2.74e+00']
 a_backup: ['-6.08e+00', '+4.96e+00'] .... ['+6.23e+00', '+6.96e+00'] .... ['+1.29e+00', '-2.74e+00']
 r=256, loss=1.631, acc=0.416 (rank=256)
 r=288, loss=1.517, acc=0.478 (rank=288)
 r=320, loss=1.366, acc=0.554 (rank=320)
 r=352, loss=1.286, acc=0.564 (rank=352)
 r=384, loss=1.062, acc=0.650 (rank=384)
 r=416, loss=0.882, acc=0.720 (rank=416)
 r=448, loss=0.729, acc=0.762 (rank=448)
 r=480, loss=0.633, acc=0.780 (rank=480)
 r=512, loss=0.576, acc=0.800 (rank=512)
 r=544, loss=0.572, acc=0.810 (rank=544)
 r=576, loss=0.584, acc=0.802 (rank=576)
 r=608, loss=0.578, acc=0.804 (rank=608)
 r=640, loss=0.589, acc=0.798 (rank=640)
 r=672, loss=0.566, acc=0.804 (rank=672)
 r=704, loss=0.556, acc=0.808 (rank=704)
 r=736, loss=0.561, acc=0.816 (rank=736)
 r=768, loss=0.546, acc=0.822 (rank=768)
 best_r=768, loss=0.546, acc=0.822
== Header Training for Low Rank [task:2] ==
Fix_Classifier: Done (training=False, fix_classifier=True)
Debug-1: loss=0.546, acc=0.822
sow: rank=768, freezed_rank=768
 a:        ['-6.08e+00', '+4.96e+00'] .... ['+6.18e+00', '-1.80e+308'] .... ['-1.80e+308', '-1.80e+308']
 a_backup: ['-6.08e+00', '+4.96e+00'] .... ['+6.18e+00', '+4.68e+00'] .... ['+1.29e+00', '-2.74e+00']
SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.008767991699104466
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0

Parameter Group 1
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.0263039750973134
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0
)
 Group[0]  param_num=161, free_param_num=2
 Group[1]  param_num=3, free_param_num=0
sow: rank=768, freezed_rank=768
 a:        ['-6.08e+00', '+4.96e+00'] .... ['+6.18e+00', '-1.80e+308'] .... ['-1.80e+308', '-1.80e+308']
 a_backup: ['-6.08e+00', '+4.96e+00'] .... ['+6.18e+00', '+4.68e+00'] .... ['+1.29e+00', '-2.74e+00']
| Epoch   1, lr=8.8e-03 time=  2.1s/  2.4s | Train: loss=0.917, TAw acc= 69.7% | Valid: time=  0.5s loss=0.556, TAw acc= 81.4% |
| Epoch   2, lr=8.8e-03 time=  2.1s/  2.4s | Train: loss=0.915, TAw acc= 69.4% | Valid: time=  0.5s loss=0.550, TAw acc= 80.8% |
| Epoch   3, lr=8.8e-03 time=  2.1s/  2.6s | Train: loss=0.910, TAw acc= 69.7% | Valid: time=  0.5s loss=0.552, TAw acc= 81.2% |
| Epoch   4, lr=8.8e-03 time=  2.1s/  2.5s | Train: loss=0.923, TAw acc= 69.3% | Valid: time=  0.5s loss=0.548, TAw acc= 81.8% |
| Epoch   5, lr=8.8e-03 time=  2.1s/  1.9s | Train: loss=0.911, TAw acc= 69.2% | Valid: time=  0.4s loss=0.556, TAw acc= 81.2% |
| Epoch   6, lr=8.8e-03 time=  1.6s/  1.9s | Train: loss=0.910, TAw acc= 69.7% | Valid: time=  0.4s loss=0.558, TAw acc= 80.4% |
| Epoch   7, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.885, TAw acc= 71.4% | Valid: time=  0.5s loss=0.545, TAw acc= 81.0% | *
| Epoch   8, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.889, TAw acc= 69.8% | Valid: time=  0.9s loss=0.550, TAw acc= 80.0% |
| Epoch   9, lr=8.8e-03 time=  2.0s/  1.9s | Train: loss=0.897, TAw acc= 70.0% | Valid: time=  0.5s loss=0.561, TAw acc= 79.6% |
| Epoch  10, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=0.882, TAw acc= 70.9% | Valid: time=  0.5s loss=0.549, TAw acc= 80.6% |
| Epoch  11, lr=8.8e-03 time=  2.1s/  2.5s | Train: loss=0.891, TAw acc= 70.4% | Valid: time=  0.5s loss=0.555, TAw acc= 80.6% |
| Epoch  12, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=0.876, TAw acc= 71.0% | Valid: time=  0.5s loss=0.537, TAw acc= 80.8% | *
| Epoch  13, lr=8.8e-03 time=  2.2s/  2.2s | Train: loss=0.860, TAw acc= 71.1% | Valid: time=  0.4s loss=0.549, TAw acc= 80.8% |
| Epoch  14, lr=8.8e-03 time=  1.8s/  2.1s | Train: loss=0.886, TAw acc= 70.5% | Valid: time=  0.4s loss=0.549, TAw acc= 81.0% |
| Epoch  15, lr=8.8e-03 time=  1.7s/  2.1s | Train: loss=0.878, TAw acc= 70.9% | Valid: time=  0.4s loss=0.547, TAw acc= 80.2% |
| Epoch  16, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.869, TAw acc= 70.6% | Valid: time=  0.4s loss=0.558, TAw acc= 80.8% |
| Epoch  17, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.867, TAw acc= 71.2% | Valid: time=  0.5s loss=0.546, TAw acc= 80.4% |
| Epoch  18, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.856, TAw acc= 71.9% | Valid: time=  0.5s loss=0.549, TAw acc= 80.8% |
| Epoch  19, lr=8.8e-03 time=  3.0s/  1.9s | Train: loss=0.849, TAw acc= 71.6% | Valid: time=  0.5s loss=0.550, TAw acc= 81.2% |
| Epoch  20, lr=8.8e-03 time=  2.1s/  2.4s | Train: loss=0.864, TAw acc= 71.6% | Valid: time=  0.5s loss=0.554, TAw acc= 80.6% |
| Epoch  21, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=0.872, TAw acc= 71.0% | Valid: time=  0.5s loss=0.556, TAw acc= 80.6% |
| Epoch  22, lr=8.8e-03 time=  2.1s/  2.4s | Train: loss=0.865, TAw acc= 71.1% | Valid: time=  0.5s loss=0.547, TAw acc= 81.4% |
| Epoch  23, lr=8.8e-03 time=  1.9s/  1.9s | Train: loss=0.871, TAw acc= 71.0% | Valid: time=  0.4s loss=0.539, TAw acc= 81.0% |
| Epoch  24, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.877, TAw acc= 70.7% | Valid: time=  0.4s loss=0.545, TAw acc= 80.8% |
| Epoch  25, lr=8.8e-03 time=  2.2s/  1.9s | Train: loss=0.837, TAw acc= 72.1% | Valid: time=  0.4s loss=0.556, TAw acc= 80.6% |
| Epoch  26, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.856, TAw acc= 71.2% | Valid: time=  0.4s loss=0.547, TAw acc= 80.6% |
| Epoch  27, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.850, TAw acc= 72.5% | Valid: time=  0.5s loss=0.532, TAw acc= 80.6% | *
| Epoch  28, lr=8.8e-03 time=  1.7s/  3.1s | Train: loss=0.841, TAw acc= 72.0% | Valid: time=  0.5s loss=0.550, TAw acc= 80.8% |
| Epoch  29, lr=8.8e-03 time=  1.8s/  2.4s | Train: loss=0.857, TAw acc= 70.9% | Valid: time=  0.6s loss=0.543, TAw acc= 80.4% |
| Epoch  30, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=0.854, TAw acc= 71.6% | Valid: time=  0.5s loss=0.539, TAw acc= 81.0% |
| Epoch  31, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=0.827, TAw acc= 72.5% | Valid: time=  0.5s loss=0.538, TAw acc= 80.8% |
| Epoch  32, lr=8.8e-03 time=  1.7s/  2.1s | Train: loss=0.864, TAw acc= 71.6% | Valid: time=  0.4s loss=0.539, TAw acc= 80.8% |
| Epoch  33, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.848, TAw acc= 72.0% | Valid: time=  0.4s loss=0.536, TAw acc= 80.8% |
| Epoch  34, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.832, TAw acc= 72.2% | Valid: time=  0.5s loss=0.534, TAw acc= 81.0% |
| Epoch  35, lr=8.8e-03 time=  1.7s/  3.0s | Train: loss=0.848, TAw acc= 70.7% | Valid: time=  0.5s loss=0.523, TAw acc= 81.2% | *
| Epoch  36, lr=8.8e-03 time=  1.7s/  2.4s | Train: loss=0.837, TAw acc= 71.4% | Valid: time=  0.5s loss=0.533, TAw acc= 81.0% |
| Epoch  37, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=0.845, TAw acc= 71.7% | Valid: time=  0.5s loss=0.533, TAw acc= 80.8% |
| Epoch  38, lr=8.8e-03 time=  2.1s/  2.2s | Train: loss=0.844, TAw acc= 71.5% | Valid: time=  0.4s loss=0.535, TAw acc= 80.8% |
| Epoch  39, lr=8.8e-03 time=  1.9s/  1.9s | Train: loss=0.827, TAw acc= 72.4% | Valid: time=  0.4s loss=0.535, TAw acc= 81.0% |
| Epoch  40, lr=8.8e-03 time=  2.2s/  1.9s | Train: loss=0.838, TAw acc= 72.5% | Valid: time=  0.4s loss=0.534, TAw acc= 81.0% |
| Epoch  41, lr=8.8e-03 time=  1.7s/  2.0s | Train: loss=0.835, TAw acc= 71.8% | Valid: time=  0.4s loss=0.531, TAw acc= 80.8% |
| Epoch  42, lr=8.8e-03 time=  1.7s/  2.9s | Train: loss=0.816, TAw acc= 72.1% | Valid: time=  0.5s loss=0.523, TAw acc= 81.2% | *
| Epoch  43, lr=8.8e-03 time=  1.7s/  2.4s | Train: loss=0.848, TAw acc= 70.9% | Valid: time=  0.5s loss=0.530, TAw acc= 81.4% |
| Epoch  44, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=0.828, TAw acc= 72.1% | Valid: time=  0.5s loss=0.538, TAw acc= 80.8% |
| Epoch  45, lr=8.8e-03 time=  2.1s/  2.4s | Train: loss=0.843, TAw acc= 71.2% | Valid: time=  0.5s loss=0.547, TAw acc= 81.0% |
| Epoch  46, lr=8.8e-03 time=  2.1s/  2.4s | Train: loss=0.829, TAw acc= 72.1% | Valid: time=  0.5s loss=0.544, TAw acc= 80.6% |
| Epoch  47, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=0.818, TAw acc= 72.8% | Valid: time=  0.5s loss=0.533, TAw acc= 81.0% |
| Epoch  48, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=0.835, TAw acc= 71.9% | Valid: time=  0.5s loss=0.536, TAw acc= 80.8% |
| Epoch  49, lr=8.8e-03 time=  2.1s/  2.4s | Train: loss=0.824, TAw acc= 72.1% | Valid: time=  0.5s loss=0.534, TAw acc= 81.0% |
| Epoch  50, lr=8.8e-03 time=  2.1s/  2.4s | Train: loss=0.827, TAw acc= 72.4% | Valid: time=  0.5s loss=0.545, TAw acc= 80.8% |
| Epoch  51, lr=8.8e-03 time=  2.1s/  2.4s | Train: loss=0.820, TAw acc= 72.4% | Valid: time=  0.5s loss=0.536, TAw acc= 81.0% |
| Epoch  52, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=0.832, TAw acc= 72.4% | Valid: time=  0.5s loss=0.527, TAw acc= 81.0% |
| Epoch  53, lr=8.8e-03 time=  2.1s/  2.4s | Train: loss=0.818, TAw acc= 72.8% | Valid: time=  0.5s loss=0.535, TAw acc= 81.2% |
| Epoch  54, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=0.806, TAw acc= 73.0% | Valid: time=  0.5s loss=0.528, TAw acc= 81.8% |
| Epoch  55, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=0.816, TAw acc= 72.8% | Valid: time=  0.5s loss=0.539, TAw acc= 81.0% |
| Epoch  56, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=0.825, TAw acc= 71.7% | Valid: time=  0.5s loss=0.527, TAw acc= 81.4% |
| Epoch  57, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=0.818, TAw acc= 72.7% | Valid: time=  0.5s loss=0.530, TAw acc= 81.2% |
| Epoch  58, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=0.834, TAw acc= 71.7% | Valid: time=  0.5s loss=0.528, TAw acc= 81.0% |
| Epoch  59, lr=8.8e-03 time=  2.1s/  2.4s | Train: loss=0.813, TAw acc= 72.6% | Valid: time=  0.5s loss=0.536, TAw acc= 81.2% |
| Epoch  60, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=0.845, TAw acc= 71.2% | Valid: time=  0.5s loss=0.539, TAw acc= 81.8% |
Debug-2: loss=0.523, acc=0.812
sow: rank=768, freezed_rank=768
 a:        ['-6.08e+00', '+4.96e+00'] .... ['+6.18e+00', '-1.80e+308'] .... ['-1.80e+308', '-1.80e+308']
 a_backup: ['-6.08e+00', '+4.96e+00'] .... ['+6.18e+00', '+4.68e+00'] .... ['+1.29e+00', '-2.74e+00']
Free_Classifier: Done (training=True, fix_classifier=False)
Debug-3: loss=0.523 acc=0.820
sow: rank=1024, freezed_rank=768
 a:        ['-6.08e+00', '+4.96e+00'] .... ['+6.18e+00', '+4.68e+00'] .... ['+1.29e+00', '-2.74e+00']
 a_backup: ['-6.08e+00', '+4.96e+00'] .... ['+6.18e+00', '+4.68e+00'] .... ['+1.29e+00', '-2.74e+00']
Header Training: Finished.
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.592 | TAw acc= 80.3%, forg=  0.0%| TAg acc= 45.3%, forg= 35.0% <<<
>>> Test on task  1 : loss=0.953 | TAw acc= 67.2%, forg=  0.0%| TAg acc= 45.8%, forg=  7.7% <<<
>>> Test on task  2 : loss=0.628 | TAw acc= 79.9%, forg=  0.0%| TAg acc= 57.4%, forg=  0.0% <<<
Save at ../RESULT_AAAI2025/CR10/0/cifar100_sow
************************************************************************************************************
Task  3
************************************************************************************************************
LLL_Net(
  (model): ResNet50_32SOW(
    in_channels=3, in_H=32, in_W=32, training=False, fix_features=True, fix_classifier=False
    (features): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
      (3): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (4): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (3): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (5): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (3): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (4): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (5): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (6): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (7): AdaptiveAvgPool2d(output_size=(1, 1))
    )
    (classifier): Sequential(
      (0): Dropout(p=0.417598542370663, inplace=False)
      (1): SOW_V3(in_features=1024, out_features=1024, num_tasks=10, dtype=torch.float64, device=cuda:0)
      (2): ReLU()
    )
    (fc): Sequential()
  )
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=1024, out_features=10, bias=True)
  )
)
SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.0263039750973134
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0

Parameter Group 1
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.0263039750973134
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0
)
 Group[0]  param_num=161, free_param_num=2
 Group[1]  param_num=3, free_param_num=3
sow: rank=1024, freezed_rank=768
 a:        ['-6.08e+00', '+4.96e+00'] .... ['+6.18e+00', '+4.68e+00'] .... ['+1.29e+00', '-2.74e+00']
 a_backup: ['-6.08e+00', '+4.96e+00'] .... ['+6.18e+00', '+4.68e+00'] .... ['+1.29e+00', '-2.74e+00']
| Epoch   1, lr=2.6e-02 time=  3.7s/  2.5s | Train: loss=1.194, TAw acc= 58.0% | Valid: time=  0.5s loss=0.856, TAw acc= 66.8% | *
| Epoch   2, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.112, TAw acc= 61.7% | Valid: time=  0.5s loss=0.760, TAw acc= 69.2% | *
| Epoch   3, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.071, TAw acc= 63.1% | Valid: time=  0.5s loss=0.757, TAw acc= 69.0% | *
| Epoch   4, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.077, TAw acc= 63.0% | Valid: time=  0.5s loss=0.718, TAw acc= 72.4% | *
| Epoch   5, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.050, TAw acc= 63.6% | Valid: time=  0.5s loss=0.759, TAw acc= 72.6% |
| Epoch   6, lr=2.6e-02 time=  3.6s/  2.4s | Train: loss=1.072, TAw acc= 62.5% | Valid: time=  0.5s loss=0.723, TAw acc= 72.4% |
| Epoch   7, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.060, TAw acc= 63.2% | Valid: time=  0.5s loss=0.743, TAw acc= 72.2% |
| Epoch   8, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.056, TAw acc= 62.8% | Valid: time=  0.5s loss=0.766, TAw acc= 71.8% |
| Epoch   9, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.048, TAw acc= 64.4% | Valid: time=  0.5s loss=0.734, TAw acc= 71.0% |
| Epoch  10, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.069, TAw acc= 62.8% | Valid: time=  0.5s loss=0.756, TAw acc= 70.0% |
| Epoch  11, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.002, TAw acc= 65.5% | Valid: time=  0.5s loss=0.706, TAw acc= 71.6% | *
| Epoch  12, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.033, TAw acc= 65.0% | Valid: time=  0.5s loss=0.710, TAw acc= 71.2% |
| Epoch  13, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.006, TAw acc= 65.1% | Valid: time=  0.5s loss=0.732, TAw acc= 71.4% |
| Epoch  14, lr=2.6e-02 time=  3.6s/  2.4s | Train: loss=1.039, TAw acc= 63.4% | Valid: time=  0.5s loss=0.692, TAw acc= 71.6% | *
| Epoch  15, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.003, TAw acc= 65.1% | Valid: time=  0.5s loss=0.687, TAw acc= 73.4% | *
| Epoch  16, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.993, TAw acc= 65.8% | Valid: time=  0.5s loss=0.709, TAw acc= 73.2% |
| Epoch  17, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.987, TAw acc= 65.4% | Valid: time=  0.5s loss=0.721, TAw acc= 73.0% |
| Epoch  18, lr=2.6e-02 time=  3.6s/  2.4s | Train: loss=1.028, TAw acc= 64.2% | Valid: time=  0.5s loss=0.698, TAw acc= 73.0% |
| Epoch  19, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.978, TAw acc= 67.5% | Valid: time=  0.5s loss=0.678, TAw acc= 74.2% | *
| Epoch  20, lr=2.6e-02 time=  3.7s/  2.5s | Train: loss=1.026, TAw acc= 64.0% | Valid: time=  0.5s loss=0.746, TAw acc= 71.2% |
| Epoch  21, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.967, TAw acc= 67.2% | Valid: time=  0.5s loss=0.700, TAw acc= 73.0% |
| Epoch  22, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.996, TAw acc= 65.3% | Valid: time=  0.5s loss=0.741, TAw acc= 71.4% |
| Epoch  23, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.997, TAw acc= 65.4% | Valid: time=  0.5s loss=0.705, TAw acc= 72.2% |
| Epoch  24, lr=2.6e-02 time=  3.7s/  2.5s | Train: loss=1.016, TAw acc= 66.6% | Valid: time=  0.5s loss=0.684, TAw acc= 72.6% |
| Epoch  25, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.982, TAw acc= 65.5% | Valid: time=  0.5s loss=0.712, TAw acc= 72.2% |
| Epoch  26, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.972, TAw acc= 66.2% | Valid: time=  0.5s loss=0.730, TAw acc= 71.8% |
| Epoch  27, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.016, TAw acc= 64.9% | Valid: time=  0.5s loss=0.674, TAw acc= 73.6% | *
| Epoch  28, lr=2.6e-02 time=  3.7s/  2.5s | Train: loss=0.985, TAw acc= 65.9% | Valid: time=  0.5s loss=0.678, TAw acc= 74.2% |
| Epoch  29, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.979, TAw acc= 66.5% | Valid: time=  0.5s loss=0.707, TAw acc= 73.0% |
| Epoch  30, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.988, TAw acc= 66.7% | Valid: time=  0.5s loss=0.681, TAw acc= 75.0% |
| Epoch  31, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.993, TAw acc= 67.2% | Valid: time=  0.5s loss=0.674, TAw acc= 72.8% | *
| Epoch  32, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.976, TAw acc= 65.8% | Valid: time=  0.5s loss=0.703, TAw acc= 72.4% |
| Epoch  33, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.984, TAw acc= 65.7% | Valid: time=  0.5s loss=0.702, TAw acc= 72.8% |
| Epoch  34, lr=2.6e-02 time=  3.7s/  2.5s | Train: loss=0.961, TAw acc= 66.6% | Valid: time=  0.6s loss=0.676, TAw acc= 73.4% |
| Epoch  35, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.951, TAw acc= 67.6% | Valid: time=  0.5s loss=0.688, TAw acc= 74.6% |
| Epoch  36, lr=2.6e-02 time=  3.7s/  2.5s | Train: loss=0.982, TAw acc= 66.1% | Valid: time=  0.5s loss=0.690, TAw acc= 73.6% |
| Epoch  37, lr=2.6e-02 time=  3.7s/  2.5s | Train: loss=0.977, TAw acc= 65.7% | Valid: time=  0.5s loss=0.672, TAw acc= 74.8% | *
| Epoch  38, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.958, TAw acc= 67.6% | Valid: time=  0.5s loss=0.686, TAw acc= 73.2% |
| Epoch  39, lr=2.6e-02 time=  3.7s/  2.5s | Train: loss=0.976, TAw acc= 66.7% | Valid: time=  0.5s loss=0.684, TAw acc= 72.2% |
| Epoch  40, lr=2.6e-02 time=  3.7s/  2.5s | Train: loss=0.978, TAw acc= 66.2% | Valid: time=  0.5s loss=0.688, TAw acc= 71.6% |
| Epoch  41, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.955, TAw acc= 67.0% | Valid: time=  0.5s loss=0.664, TAw acc= 76.2% | *
| Epoch  42, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.969, TAw acc= 67.3% | Valid: time=  0.5s loss=0.649, TAw acc= 74.8% | *
| Epoch  43, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.975, TAw acc= 67.6% | Valid: time=  0.5s loss=0.664, TAw acc= 74.8% |
| Epoch  44, lr=2.6e-02 time=  3.7s/  2.4s | Train: loss=0.956, TAw acc= 66.4% | Valid: time=  0.5s loss=0.688, TAw acc= 73.2% |
| Epoch  45, lr=2.6e-02 time=  3.7s/  2.4s | Train: loss=0.970, TAw acc= 66.3% | Valid: time=  0.5s loss=0.690, TAw acc= 72.8% |
| Epoch  46, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.983, TAw acc= 67.1% | Valid: time=  0.5s loss=0.682, TAw acc= 73.2% |
| Epoch  47, lr=2.6e-02 time=  3.7s/  2.5s | Train: loss=0.982, TAw acc= 66.0% | Valid: time=  0.5s loss=0.666, TAw acc= 73.8% |
| Epoch  48, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.987, TAw acc= 66.0% | Valid: time=  0.5s loss=0.665, TAw acc= 74.8% |
| Epoch  49, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.967, TAw acc= 67.1% | Valid: time=  0.5s loss=0.682, TAw acc= 74.6% |
| Epoch  50, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.970, TAw acc= 65.9% | Valid: time=  0.5s loss=0.685, TAw acc= 74.2% |
| Epoch  51, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.944, TAw acc= 67.0% | Valid: time=  0.6s loss=0.663, TAw acc= 74.2% |
| Epoch  52, lr=2.6e-02 time=  3.7s/  2.6s | Train: loss=0.964, TAw acc= 67.5% | Valid: time=  0.6s loss=0.696, TAw acc= 72.8% |
| Epoch  53, lr=2.6e-02 time=  3.4s/  1.9s | Train: loss=0.946, TAw acc= 67.3% | Valid: time=  0.4s loss=0.665, TAw acc= 73.6% |
| Epoch  54, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=0.977, TAw acc= 66.3% | Valid: time=  0.4s loss=0.691, TAw acc= 73.0% |
| Epoch  55, lr=2.6e-02 time=  2.8s/  3.2s | Train: loss=0.942, TAw acc= 67.5% | Valid: time=  0.4s loss=0.691, TAw acc= 73.0% |
| Epoch  56, lr=2.6e-02 time=  3.1s/  2.5s | Train: loss=0.961, TAw acc= 66.7% | Valid: time=  0.5s loss=0.688, TAw acc= 72.8% |
| Epoch  57, lr=2.6e-02 time=  3.7s/  2.5s | Train: loss=0.960, TAw acc= 67.4% | Valid: time=  0.5s loss=0.663, TAw acc= 73.0% |
| Epoch  58, lr=2.6e-02 time=  2.8s/  2.3s | Train: loss=0.954, TAw acc= 67.5% | Valid: time=  0.4s loss=0.667, TAw acc= 74.0% |
| Epoch  59, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=0.967, TAw acc= 67.1% | Valid: time=  0.4s loss=0.678, TAw acc= 73.6% |
| Epoch  60, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=0.984, TAw acc= 66.2% | Valid: time=  0.9s loss=0.691, TAw acc= 73.4% |
== Rank Reduction [task:3] ==
Debug-0:
  best_loss=0.649,   best_acc=0.748
loss_margin=0.006, acc_margin=0.004
sow: rank=1024, freezed_rank=768
 a:        ['-6.26e+00', '+4.95e+00'] .... ['+6.19e+00', '+4.70e+00'] .... ['+1.29e+00', '-2.74e+00']
 a_backup: ['-6.26e+00', '+4.95e+00'] .... ['+6.19e+00', '+4.70e+00'] .... ['+1.29e+00', '-2.74e+00']
 r=256, loss=2.019, acc=0.218 (rank=256)
 r=288, loss=1.874, acc=0.266 (rank=288)
 r=320, loss=1.561, acc=0.400 (rank=320)
 r=352, loss=1.385, acc=0.458 (rank=352)
 r=384, loss=1.266, acc=0.510 (rank=384)
 r=416, loss=1.040, acc=0.618 (rank=416)
 r=448, loss=0.910, acc=0.646 (rank=448)
 r=480, loss=0.806, acc=0.700 (rank=480)
 r=512, loss=0.686, acc=0.738 (rank=512)
 r=544, loss=0.694, acc=0.732 (rank=544)
 r=576, loss=0.696, acc=0.736 (rank=576)
 r=608, loss=0.677, acc=0.744 (rank=608)
 r=640, loss=0.660, acc=0.748 (rank=640)
 r=672, loss=0.651, acc=0.736 (rank=672)
 r=704, loss=0.641, acc=0.730 (rank=704)
 r=736, loss=0.645, acc=0.730 (rank=736)
 r=768, loss=0.656, acc=0.742 (rank=768)
 r=800, loss=0.661, acc=0.740 (rank=800)
 r=832, loss=0.663, acc=0.734 (rank=832)
 r=864, loss=0.658, acc=0.746 (rank=864)
 r=896, loss=0.651, acc=0.740 (rank=896)
 r=928, loss=0.648, acc=0.752 (rank=928)
 best_r=928, loss=0.648, acc=0.752
== Header Training for Low Rank [task:3] ==
Fix_Classifier: Done (training=False, fix_classifier=True)
Debug-1: loss=0.648, acc=0.752
sow: rank=928, freezed_rank=928
 a:        ['-6.26e+00', '+4.95e+00'] .... ['+4.97e+00', '-1.80e+308'] .... ['-1.80e+308', '-1.80e+308']
 a_backup: ['-6.26e+00', '+4.95e+00'] .... ['+4.97e+00', '+4.37e+00'] .... ['+1.29e+00', '-2.74e+00']
SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.008767991699104466
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0

Parameter Group 1
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.0263039750973134
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0
)
 Group[0]  param_num=161, free_param_num=2
 Group[1]  param_num=3, free_param_num=0
sow: rank=928, freezed_rank=928
 a:        ['-6.26e+00', '+4.95e+00'] .... ['+4.97e+00', '-1.80e+308'] .... ['-1.80e+308', '-1.80e+308']
 a_backup: ['-6.26e+00', '+4.95e+00'] .... ['+4.97e+00', '+4.37e+00'] .... ['+1.29e+00', '-2.74e+00']
| Epoch   1, lr=8.8e-03 time=  2.1s/  2.5s | Train: loss=0.958, TAw acc= 67.5% | Valid: time=  0.5s loss=0.643, TAw acc= 74.8% | *
| Epoch   2, lr=8.8e-03 time=  2.1s/  2.2s | Train: loss=0.936, TAw acc= 67.7% | Valid: time=  0.5s loss=0.632, TAw acc= 75.4% | *
| Epoch   3, lr=8.8e-03 time=  1.9s/  2.1s | Train: loss=0.936, TAw acc= 67.5% | Valid: time=  0.4s loss=0.639, TAw acc= 75.4% |
| Epoch   4, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.953, TAw acc= 67.5% | Valid: time=  0.5s loss=0.639, TAw acc= 75.2% |
| Epoch   5, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.928, TAw acc= 68.3% | Valid: time=  0.6s loss=0.637, TAw acc= 75.0% |
| Epoch   6, lr=8.8e-03 time=  2.5s/  1.9s | Train: loss=0.954, TAw acc= 67.5% | Valid: time=  0.4s loss=0.630, TAw acc= 75.4% | *
| Epoch   7, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=0.948, TAw acc= 67.6% | Valid: time=  0.5s loss=0.627, TAw acc= 76.2% | *
| Epoch   8, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=0.909, TAw acc= 69.1% | Valid: time=  0.5s loss=0.632, TAw acc= 75.6% |
| Epoch   9, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=0.918, TAw acc= 68.5% | Valid: time=  0.5s loss=0.633, TAw acc= 75.4% |
| Epoch  10, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=0.921, TAw acc= 69.2% | Valid: time=  0.5s loss=0.635, TAw acc= 76.0% |
| Epoch  11, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=0.914, TAw acc= 69.0% | Valid: time=  0.5s loss=0.630, TAw acc= 75.8% |
| Epoch  12, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=0.926, TAw acc= 68.8% | Valid: time=  0.5s loss=0.633, TAw acc= 76.0% |
| Epoch  13, lr=8.8e-03 time=  2.2s/  2.0s | Train: loss=0.899, TAw acc= 70.0% | Valid: time=  0.4s loss=0.628, TAw acc= 76.4% |
| Epoch  14, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.928, TAw acc= 68.5% | Valid: time=  0.5s loss=0.632, TAw acc= 76.2% |
| Epoch  15, lr=8.8e-03 time=  1.7s/  3.2s | Train: loss=0.915, TAw acc= 69.4% | Valid: time=  0.4s loss=0.632, TAw acc= 76.2% |
| Epoch  16, lr=8.8e-03 time=  1.7s/  2.0s | Train: loss=0.910, TAw acc= 69.7% | Valid: time=  0.4s loss=0.637, TAw acc= 75.6% |
| Epoch  17, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.916, TAw acc= 69.0% | Valid: time=  0.4s loss=0.633, TAw acc= 76.4% |
| Epoch  18, lr=8.8e-03 time=  2.6s/  2.0s | Train: loss=0.928, TAw acc= 67.6% | Valid: time=  0.4s loss=0.625, TAw acc= 76.6% | *
| Epoch  19, lr=8.8e-03 time=  1.8s/  1.9s | Train: loss=0.904, TAw acc= 69.8% | Valid: time=  0.5s loss=0.630, TAw acc= 76.2% |
| Epoch  20, lr=8.8e-03 time=  1.7s/  2.8s | Train: loss=0.894, TAw acc= 69.6% | Valid: time=  0.5s loss=0.627, TAw acc= 76.6% |
| Epoch  21, lr=8.8e-03 time=  1.7s/  2.1s | Train: loss=0.914, TAw acc= 68.5% | Valid: time=  0.4s loss=0.625, TAw acc= 76.4% |
| Epoch  22, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.909, TAw acc= 69.3% | Valid: time=  0.4s loss=0.632, TAw acc= 75.6% |
| Epoch  23, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.882, TAw acc= 69.2% | Valid: time=  0.5s loss=0.628, TAw acc= 77.0% |
| Epoch  24, lr=8.8e-03 time=  1.7s/  2.9s | Train: loss=0.915, TAw acc= 68.6% | Valid: time=  0.4s loss=0.630, TAw acc= 76.0% |
| Epoch  25, lr=8.8e-03 time=  1.7s/  2.1s | Train: loss=0.889, TAw acc= 71.3% | Valid: time=  0.5s loss=0.628, TAw acc= 76.6% |
| Epoch  26, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=0.909, TAw acc= 68.6% | Valid: time=  0.5s loss=0.629, TAw acc= 76.6% |
| Epoch  27, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=0.907, TAw acc= 68.7% | Valid: time=  0.5s loss=0.630, TAw acc= 75.6% |
| Epoch  28, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=0.909, TAw acc= 69.8% | Valid: time=  0.5s loss=0.635, TAw acc= 75.8% |
| Epoch  29, lr=8.8e-03 time=  1.9s/  2.2s | Train: loss=0.893, TAw acc= 69.7% | Valid: time=  0.5s loss=0.631, TAw acc= 76.6% |
| Epoch  30, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.867, TAw acc= 70.4% | Valid: time=  0.4s loss=0.630, TAw acc= 76.2% |
| Epoch  31, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.893, TAw acc= 69.0% | Valid: time=  0.4s loss=0.634, TAw acc= 75.8% |
| Epoch  32, lr=8.8e-03 time=  1.8s/  1.9s | Train: loss=0.894, TAw acc= 69.6% | Valid: time=  0.5s loss=0.632, TAw acc= 76.2% |
| Epoch  33, lr=8.8e-03 time=  2.9s/  1.9s | Train: loss=0.888, TAw acc= 69.2% | Valid: time=  0.5s loss=0.625, TAw acc= 76.8% |
| Epoch  34, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=0.875, TAw acc= 70.4% | Valid: time=  0.5s loss=0.627, TAw acc= 76.8% |
| Epoch  35, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=0.879, TAw acc= 69.4% | Valid: time=  0.5s loss=0.622, TAw acc= 76.4% | *
| Epoch  36, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=0.876, TAw acc= 70.4% | Valid: time=  0.5s loss=0.628, TAw acc= 76.2% |
| Epoch  37, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.883, TAw acc= 69.6% | Valid: time=  0.5s loss=0.639, TAw acc= 75.0% |
| Epoch  38, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.874, TAw acc= 70.5% | Valid: time=  0.8s loss=0.636, TAw acc= 75.6% |
| Epoch  39, lr=8.8e-03 time=  2.3s/  1.9s | Train: loss=0.867, TAw acc= 69.9% | Valid: time=  0.5s loss=0.633, TAw acc= 76.4% |
| Epoch  40, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=0.875, TAw acc= 70.3% | Valid: time=  0.5s loss=0.627, TAw acc= 75.8% |
| Epoch  41, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=0.890, TAw acc= 69.8% | Valid: time=  0.4s loss=0.632, TAw acc= 75.8% |
| Epoch  42, lr=8.8e-03 time=  1.7s/  2.0s | Train: loss=0.862, TAw acc= 70.4% | Valid: time=  0.4s loss=0.638, TAw acc= 75.4% |
| Epoch  43, lr=8.8e-03 time=  1.9s/  1.9s | Train: loss=0.862, TAw acc= 70.3% | Valid: time=  0.4s loss=0.638, TAw acc= 75.8% |
| Epoch  44, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.868, TAw acc= 71.0% | Valid: time=  0.4s loss=0.634, TAw acc= 76.0% |
| Epoch  45, lr=8.8e-03 time=  1.7s/  3.1s | Train: loss=0.877, TAw acc= 70.0% | Valid: time=  0.4s loss=0.632, TAw acc= 75.6% |
| Epoch  46, lr=8.8e-03 time=  1.7s/  2.2s | Train: loss=0.880, TAw acc= 70.0% | Valid: time=  0.5s loss=0.630, TAw acc= 76.2% |
| Epoch  47, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=0.853, TAw acc= 71.5% | Valid: time=  0.5s loss=0.620, TAw acc= 76.8% | *
| Epoch  48, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=0.894, TAw acc= 69.4% | Valid: time=  0.5s loss=0.629, TAw acc= 75.4% |
| Epoch  49, lr=8.8e-03 time=  2.0s/  1.9s | Train: loss=0.874, TAw acc= 70.2% | Valid: time=  0.4s loss=0.628, TAw acc= 76.0% |
| Epoch  50, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.874, TAw acc= 70.1% | Valid: time=  0.4s loss=0.635, TAw acc= 76.0% |
| Epoch  51, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.869, TAw acc= 70.0% | Valid: time=  0.4s loss=0.631, TAw acc= 75.8% |
| Epoch  52, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.860, TAw acc= 70.2% | Valid: time=  0.4s loss=0.632, TAw acc= 76.6% |
| Epoch  53, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.878, TAw acc= 69.6% | Valid: time=  0.4s loss=0.632, TAw acc= 76.2% |
| Epoch  54, lr=8.8e-03 time=  1.7s/  2.0s | Train: loss=0.859, TAw acc= 69.2% | Valid: time=  0.4s loss=0.627, TAw acc= 76.6% |
| Epoch  55, lr=8.8e-03 time=  1.7s/  3.2s | Train: loss=0.864, TAw acc= 70.2% | Valid: time=  0.4s loss=0.631, TAw acc= 76.4% |
| Epoch  56, lr=8.8e-03 time=  1.7s/  2.3s | Train: loss=0.880, TAw acc= 70.3% | Valid: time=  0.5s loss=0.633, TAw acc= 76.8% |
| Epoch  57, lr=8.8e-03 time=  2.2s/  2.3s | Train: loss=0.869, TAw acc= 70.5% | Valid: time=  0.4s loss=0.630, TAw acc= 77.0% |
| Epoch  58, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.861, TAw acc= 70.7% | Valid: time=  0.4s loss=0.631, TAw acc= 76.6% |
| Epoch  59, lr=8.8e-03 time=  1.7s/  2.0s | Train: loss=0.858, TAw acc= 70.4% | Valid: time=  0.4s loss=0.621, TAw acc= 76.8% |
| Epoch  60, lr=8.8e-03 time=  3.0s/  1.9s | Train: loss=0.873, TAw acc= 70.4% | Valid: time=  0.4s loss=0.624, TAw acc= 76.0% |
Debug-2: loss=0.620, acc=0.768
sow: rank=928, freezed_rank=928
 a:        ['-6.26e+00', '+4.95e+00'] .... ['+4.97e+00', '-1.80e+308'] .... ['-1.80e+308', '-1.80e+308']
 a_backup: ['-6.26e+00', '+4.95e+00'] .... ['+4.97e+00', '+4.37e+00'] .... ['+1.29e+00', '-2.74e+00']
Free_Classifier: Done (training=True, fix_classifier=False)
Debug-3: loss=0.623 acc=0.770
sow: rank=1024, freezed_rank=928
 a:        ['-6.26e+00', '+4.95e+00'] .... ['+4.97e+00', '+4.37e+00'] .... ['+1.29e+00', '-2.74e+00']
 a_backup: ['-6.26e+00', '+4.95e+00'] .... ['+4.97e+00', '+4.37e+00'] .... ['+1.29e+00', '-2.74e+00']
Header Training: Finished.
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.592 | TAw acc= 80.3%, forg=  0.0%| TAg acc= 33.7%, forg= 46.6% <<<
>>> Test on task  1 : loss=0.953 | TAw acc= 67.2%, forg=  0.0%| TAg acc= 38.3%, forg= 15.2% <<<
>>> Test on task  2 : loss=0.628 | TAw acc= 79.9%, forg=  0.0%| TAg acc= 49.1%, forg=  8.3% <<<
>>> Test on task  3 : loss=0.612 | TAw acc= 79.6%, forg=  0.0%| TAg acc= 43.7%, forg=  0.0% <<<
Save at ../RESULT_AAAI2025/CR10/0/cifar100_sow
************************************************************************************************************
Task  4
************************************************************************************************************
LLL_Net(
  (model): ResNet50_32SOW(
    in_channels=3, in_H=32, in_W=32, training=False, fix_features=True, fix_classifier=False
    (features): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
      (3): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (4): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (3): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (5): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (3): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (4): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (5): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (6): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (7): AdaptiveAvgPool2d(output_size=(1, 1))
    )
    (classifier): Sequential(
      (0): Dropout(p=0.417598542370663, inplace=False)
      (1): SOW_V3(in_features=1024, out_features=1024, num_tasks=10, dtype=torch.float64, device=cuda:0)
      (2): ReLU()
    )
    (fc): Sequential()
  )
  (heads): ModuleList(
    (0-4): 5 x Linear(in_features=1024, out_features=10, bias=True)
  )
)
SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.0263039750973134
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0

Parameter Group 1
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.0263039750973134
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0
)
 Group[0]  param_num=161, free_param_num=2
 Group[1]  param_num=3, free_param_num=3
sow: rank=1024, freezed_rank=928
 a:        ['-6.26e+00', '+4.95e+00'] .... ['+4.97e+00', '+4.37e+00'] .... ['+1.29e+00', '-2.74e+00']
 a_backup: ['-6.26e+00', '+4.95e+00'] .... ['+4.97e+00', '+4.37e+00'] .... ['+1.29e+00', '-2.74e+00']
| Epoch   1, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.155, TAw acc= 61.3% | Valid: time=  0.5s loss=0.778, TAw acc= 75.4% | *
| Epoch   2, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.038, TAw acc= 65.3% | Valid: time=  0.5s loss=0.620, TAw acc= 80.2% | *
| Epoch   3, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=1.015, TAw acc= 66.8% | Valid: time=  0.4s loss=0.611, TAw acc= 81.8% | *
| Epoch   4, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=1.038, TAw acc= 65.8% | Valid: time=  0.9s loss=0.578, TAw acc= 81.8% | *
| Epoch   5, lr=2.6e-02 time=  3.0s/  2.2s | Train: loss=0.967, TAw acc= 68.4% | Valid: time=  0.5s loss=0.530, TAw acc= 83.6% | *
| Epoch   6, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.953, TAw acc= 68.0% | Valid: time=  0.5s loss=0.556, TAw acc= 81.6% |
| Epoch   7, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=0.946, TAw acc= 69.5% | Valid: time=  0.4s loss=0.586, TAw acc= 81.2% |
| Epoch   8, lr=2.6e-02 time=  2.7s/  1.9s | Train: loss=0.986, TAw acc= 67.4% | Valid: time=  0.4s loss=0.464, TAw acc= 84.0% | *
| Epoch   9, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=0.925, TAw acc= 69.1% | Valid: time=  0.9s loss=0.536, TAw acc= 82.4% |
| Epoch  10, lr=2.6e-02 time=  3.0s/  2.2s | Train: loss=0.950, TAw acc= 68.4% | Valid: time=  0.5s loss=0.528, TAw acc= 83.0% |
| Epoch  11, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.935, TAw acc= 68.5% | Valid: time=  0.5s loss=0.547, TAw acc= 81.2% |
| Epoch  12, lr=2.6e-02 time=  3.6s/  2.4s | Train: loss=0.931, TAw acc= 69.1% | Valid: time=  0.4s loss=0.489, TAw acc= 84.0% |
| Epoch  13, lr=2.6e-02 time=  2.7s/  2.0s | Train: loss=0.904, TAw acc= 70.2% | Valid: time=  0.4s loss=0.483, TAw acc= 84.6% |
| Epoch  14, lr=2.6e-02 time=  3.7s/  2.0s | Train: loss=0.944, TAw acc= 68.9% | Valid: time=  0.4s loss=0.500, TAw acc= 83.6% |
| Epoch  15, lr=2.6e-02 time=  2.7s/  1.9s | Train: loss=0.941, TAw acc= 68.8% | Valid: time=  0.5s loss=0.539, TAw acc= 83.0% |
| Epoch  16, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=0.907, TAw acc= 70.0% | Valid: time=  0.4s loss=0.553, TAw acc= 83.2% |
| Epoch  17, lr=2.6e-02 time=  4.0s/  1.9s | Train: loss=0.966, TAw acc= 67.5% | Valid: time=  0.5s loss=0.521, TAw acc= 84.2% |
| Epoch  18, lr=2.6e-02 time=  3.7s/  2.5s | Train: loss=0.885, TAw acc= 70.2% | Valid: time=  0.5s loss=0.520, TAw acc= 84.0% |
| Epoch  19, lr=2.6e-02 time=  3.6s/  2.0s | Train: loss=0.900, TAw acc= 69.8% | Valid: time=  0.4s loss=0.496, TAw acc= 84.0% |
| Epoch  20, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=0.913, TAw acc= 69.1% | Valid: time=  0.4s loss=0.494, TAw acc= 85.0% |
| Epoch  21, lr=2.6e-02 time=  4.0s/  2.0s | Train: loss=0.907, TAw acc= 69.4% | Valid: time=  0.5s loss=0.479, TAw acc= 85.4% |
| Epoch  22, lr=2.6e-02 time=  3.7s/  2.5s | Train: loss=0.925, TAw acc= 69.6% | Valid: time=  0.5s loss=0.464, TAw acc= 85.0% | *
| Epoch  23, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.894, TAw acc= 70.0% | Valid: time=  0.5s loss=0.453, TAw acc= 85.6% | *
| Epoch  24, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.909, TAw acc= 69.5% | Valid: time=  0.5s loss=0.491, TAw acc= 85.4% |
| Epoch  25, lr=2.6e-02 time=  3.5s/  2.5s | Train: loss=0.889, TAw acc= 71.5% | Valid: time=  0.5s loss=0.492, TAw acc= 84.4% |
| Epoch  26, lr=2.6e-02 time=  2.7s/  1.9s | Train: loss=0.900, TAw acc= 69.5% | Valid: time=  0.4s loss=0.488, TAw acc= 84.4% |
| Epoch  27, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=0.914, TAw acc= 69.8% | Valid: time=  0.4s loss=0.553, TAw acc= 82.4% |
| Epoch  28, lr=2.6e-02 time=  4.0s/  2.0s | Train: loss=0.902, TAw acc= 70.2% | Valid: time=  0.5s loss=0.516, TAw acc= 83.2% |
| Epoch  29, lr=2.6e-02 time=  3.7s/  2.5s | Train: loss=0.876, TAw acc= 71.1% | Valid: time=  0.5s loss=0.533, TAw acc= 83.0% |
| Epoch  30, lr=2.6e-02 time=  3.5s/  2.5s | Train: loss=0.903, TAw acc= 70.2% | Valid: time=  0.5s loss=0.457, TAw acc= 86.0% |
| Epoch  31, lr=2.6e-02 time=  3.6s/  2.0s | Train: loss=0.878, TAw acc= 70.0% | Valid: time=  0.5s loss=0.499, TAw acc= 84.2% |
| Epoch  32, lr=2.6e-02 time=  2.8s/  2.0s | Train: loss=0.883, TAw acc= 71.0% | Valid: time=  0.4s loss=0.481, TAw acc= 85.2% |
| Epoch  33, lr=2.6e-02 time=  2.7s/  2.9s | Train: loss=0.872, TAw acc= 71.0% | Valid: time=  0.5s loss=0.493, TAw acc= 84.6% |
| Epoch  34, lr=2.6e-02 time=  3.1s/  2.5s | Train: loss=0.908, TAw acc= 69.9% | Valid: time=  0.5s loss=0.531, TAw acc= 83.8% |
| Epoch  35, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.887, TAw acc= 70.0% | Valid: time=  0.5s loss=0.525, TAw acc= 83.6% |
| Epoch  36, lr=2.6e-02 time=  3.0s/  2.0s | Train: loss=0.888, TAw acc= 70.9% | Valid: time=  0.5s loss=0.454, TAw acc= 86.2% |
| Epoch  37, lr=2.6e-02 time=  2.7s/  2.0s | Train: loss=0.913, TAw acc= 68.8% | Valid: time=  0.5s loss=0.547, TAw acc= 84.2% |
| Epoch  38, lr=2.6e-02 time=  4.1s/  1.9s | Train: loss=0.887, TAw acc= 71.0% | Valid: time=  0.6s loss=0.496, TAw acc= 84.8% |
| Epoch  39, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.895, TAw acc= 69.9% | Valid: time=  0.5s loss=0.472, TAw acc= 85.4% |
| Epoch  40, lr=2.6e-02 time=  3.7s/  2.5s | Train: loss=0.891, TAw acc= 70.5% | Valid: time=  0.5s loss=0.471, TAw acc= 84.8% |
| Epoch  41, lr=2.6e-02 time=  3.6s/  1.9s | Train: loss=0.899, TAw acc= 70.3% | Valid: time=  0.4s loss=0.475, TAw acc= 84.6% |
| Epoch  42, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=0.886, TAw acc= 70.7% | Valid: time=  1.0s loss=0.475, TAw acc= 85.6% |
| Epoch  43, lr=2.6e-02 time=  2.9s/  2.2s | Train: loss=0.896, TAw acc= 70.5% | Valid: time=  0.5s loss=0.495, TAw acc= 85.4% |
| Epoch  44, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.879, TAw acc= 70.8% | Valid: time=  0.5s loss=0.477, TAw acc= 84.2% |
| Epoch  45, lr=2.6e-02 time=  3.7s/  2.5s | Train: loss=0.892, TAw acc= 70.3% | Valid: time=  0.5s loss=0.529, TAw acc= 84.0% |
| Epoch  46, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=0.907, TAw acc= 69.6% | Valid: time=  0.4s loss=0.497, TAw acc= 85.0% |
| Epoch  47, lr=2.6e-02 time=  2.7s/  1.9s | Train: loss=0.911, TAw acc= 69.8% | Valid: time=  0.5s loss=0.495, TAw acc= 85.6% |
| Epoch  48, lr=2.6e-02 time=  2.7s/  3.2s | Train: loss=0.879, TAw acc= 70.8% | Valid: time=  0.5s loss=0.513, TAw acc= 84.2% |
| Epoch  49, lr=2.6e-02 time=  2.9s/  2.5s | Train: loss=0.899, TAw acc= 70.6% | Valid: time=  0.5s loss=0.490, TAw acc= 83.8% |
| Epoch  50, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.874, TAw acc= 70.2% | Valid: time=  0.6s loss=0.514, TAw acc= 82.4% |
| Epoch  51, lr=2.6e-02 time=  2.7s/  1.9s | Train: loss=0.874, TAw acc= 70.6% | Valid: time=  0.4s loss=0.514, TAw acc= 83.0% |
| Epoch  52, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=0.881, TAw acc= 70.0% | Valid: time=  0.5s loss=0.522, TAw acc= 83.8% |
| Epoch  53, lr=2.6e-02 time=  4.0s/  2.0s | Train: loss=0.896, TAw acc= 70.6% | Valid: time=  0.5s loss=0.480, TAw acc= 84.4% | lr=8.8e-03
| Epoch  54, lr=8.8e-03 time=  3.6s/  2.5s | Train: loss=0.879, TAw acc= 71.1% | Valid: time=  0.5s loss=0.492, TAw acc= 85.4% |
| Epoch  55, lr=8.8e-03 time=  3.6s/  2.4s | Train: loss=0.884, TAw acc= 70.3% | Valid: time=  0.4s loss=0.482, TAw acc= 85.4% |
| Epoch  56, lr=8.8e-03 time=  3.0s/  1.9s | Train: loss=0.884, TAw acc= 71.5% | Valid: time=  0.5s loss=0.483, TAw acc= 84.4% |
| Epoch  57, lr=8.8e-03 time=  3.5s/  2.0s | Train: loss=0.883, TAw acc= 70.2% | Valid: time=  0.4s loss=0.462, TAw acc= 85.0% |
| Epoch  58, lr=8.8e-03 time=  2.7s/  1.9s | Train: loss=0.884, TAw acc= 69.6% | Valid: time=  0.4s loss=0.478, TAw acc= 85.0% |
| Epoch  59, lr=8.8e-03 time=  3.7s/  1.9s | Train: loss=0.899, TAw acc= 70.2% | Valid: time=  0.4s loss=0.487, TAw acc= 84.2% |
| Epoch  60, lr=8.8e-03 time=  2.8s/  2.0s | Train: loss=0.860, TAw acc= 70.9% | Valid: time=  0.4s loss=0.478, TAw acc= 85.2% |
== Rank Reduction [task:4] ==
Debug-0:
  best_loss=0.453,   best_acc=0.856
loss_margin=0.006, acc_margin=0.004
sow: rank=1024, freezed_rank=928
 a:        ['-5.95e+00', '+4.96e+00'] .... ['+4.97e+00', '+4.37e+00'] .... ['+1.29e+00', '-2.74e+00']
 a_backup: ['-5.95e+00', '+4.96e+00'] .... ['+4.97e+00', '+4.37e+00'] .... ['+1.29e+00', '-2.74e+00']
 r=256, loss=1.730, acc=0.362 (rank=256)
 r=288, loss=1.766, acc=0.328 (rank=288)
 r=320, loss=1.575, acc=0.442 (rank=320)
 r=352, loss=1.442, acc=0.516 (rank=352)
 r=384, loss=1.217, acc=0.592 (rank=384)
 r=416, loss=0.956, acc=0.702 (rank=416)
 r=448, loss=0.741, acc=0.774 (rank=448)
 r=480, loss=0.586, acc=0.826 (rank=480)
 r=512, loss=0.490, acc=0.840 (rank=512)
 r=544, loss=0.485, acc=0.838 (rank=544)
 r=576, loss=0.488, acc=0.840 (rank=576)
 r=608, loss=0.486, acc=0.846 (rank=608)
 r=640, loss=0.485, acc=0.852 (rank=640)
 r=672, loss=0.463, acc=0.852 (rank=672)
 r=704, loss=0.454, acc=0.856 (rank=704)
 best_r=704, loss=0.454, acc=0.856
== Header Training for Low Rank [task:4] ==
Fix_Classifier: Done (training=False, fix_classifier=True)
Debug-1: loss=0.454, acc=0.856
sow: rank=704, freezed_rank=928
 a:        ['-5.95e+00', '+4.96e+00'] .... ['-1.80e+308', '-1.80e+308'] .... ['-1.80e+308', '-1.80e+308']
 a_backup: ['-5.95e+00', '+4.96e+00'] .... ['+4.97e+00', '+4.37e+00'] .... ['+1.29e+00', '-2.74e+00']
SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.008767991699104466
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0

Parameter Group 1
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.0263039750973134
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0
)
 Group[0]  param_num=161, free_param_num=2
 Group[1]  param_num=3, free_param_num=0
sow: rank=704, freezed_rank=928
 a:        ['-5.95e+00', '+4.96e+00'] .... ['-1.80e+308', '-1.80e+308'] .... ['-1.80e+308', '-1.80e+308']
 a_backup: ['-5.95e+00', '+4.96e+00'] .... ['+4.97e+00', '+4.37e+00'] .... ['+1.29e+00', '-2.74e+00']
| Epoch   1, lr=8.8e-03 time=  2.1s/  2.5s | Train: loss=0.904, TAw acc= 71.4% | Valid: time=  0.5s loss=0.494, TAw acc= 85.0% |
| Epoch   2, lr=8.8e-03 time=  2.2s/  2.2s | Train: loss=0.899, TAw acc= 70.5% | Valid: time=  0.4s loss=0.483, TAw acc= 85.6% |
| Epoch   3, lr=8.8e-03 time=  1.6s/  1.9s | Train: loss=0.905, TAw acc= 70.1% | Valid: time=  0.4s loss=0.483, TAw acc= 84.8% |
| Epoch   4, lr=8.8e-03 time=  1.7s/  2.8s | Train: loss=0.882, TAw acc= 70.3% | Valid: time=  0.5s loss=0.489, TAw acc= 84.8% |
| Epoch   5, lr=8.8e-03 time=  1.7s/  2.0s | Train: loss=0.864, TAw acc= 71.8% | Valid: time=  0.4s loss=0.484, TAw acc= 84.2% |
| Epoch   6, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.874, TAw acc= 71.4% | Valid: time=  0.5s loss=0.494, TAw acc= 84.8% |
| Epoch   7, lr=8.8e-03 time=  2.6s/  2.0s | Train: loss=0.870, TAw acc= 71.8% | Valid: time=  0.4s loss=0.460, TAw acc= 87.2% |
| Epoch   8, lr=8.8e-03 time=  1.8s/  1.9s | Train: loss=0.872, TAw acc= 70.2% | Valid: time=  0.4s loss=0.464, TAw acc= 87.2% |
| Epoch   9, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.865, TAw acc= 71.2% | Valid: time=  0.5s loss=0.475, TAw acc= 86.0% |
| Epoch  10, lr=8.8e-03 time=  1.7s/  3.0s | Train: loss=0.856, TAw acc= 71.4% | Valid: time=  0.5s loss=0.483, TAw acc= 86.0% |
| Epoch  11, lr=8.8e-03 time=  1.7s/  2.4s | Train: loss=0.856, TAw acc= 71.3% | Valid: time=  0.5s loss=0.471, TAw acc= 86.4% |
| Epoch  12, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=0.855, TAw acc= 71.5% | Valid: time=  0.5s loss=0.471, TAw acc= 86.4% |
| Epoch  13, lr=8.8e-03 time=  2.1s/  2.1s | Train: loss=0.856, TAw acc= 72.4% | Valid: time=  0.4s loss=0.477, TAw acc= 85.6% |
| Epoch  14, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.854, TAw acc= 71.5% | Valid: time=  0.4s loss=0.489, TAw acc= 85.2% |
| Epoch  15, lr=8.8e-03 time=  2.3s/  1.9s | Train: loss=0.855, TAw acc= 72.0% | Valid: time=  0.4s loss=0.486, TAw acc= 85.6% |
| Epoch  16, lr=8.8e-03 time=  1.8s/  2.1s | Train: loss=0.856, TAw acc= 71.8% | Valid: time=  0.4s loss=0.465, TAw acc= 86.6% |
| Epoch  17, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.837, TAw acc= 72.9% | Valid: time=  0.4s loss=0.473, TAw acc= 85.8% |
| Epoch  18, lr=8.8e-03 time=  2.9s/  1.9s | Train: loss=0.826, TAw acc= 72.4% | Valid: time=  0.4s loss=0.473, TAw acc= 85.4% |
| Epoch  19, lr=8.8e-03 time=  1.7s/  2.1s | Train: loss=0.830, TAw acc= 72.9% | Valid: time=  0.5s loss=0.479, TAw acc= 85.8% |
| Epoch  20, lr=8.8e-03 time=  1.7s/  2.1s | Train: loss=0.837, TAw acc= 71.9% | Valid: time=  0.5s loss=0.476, TAw acc= 86.2% |
| Epoch  21, lr=8.8e-03 time=  1.9s/  1.9s | Train: loss=0.816, TAw acc= 73.3% | Valid: time=  0.4s loss=0.482, TAw acc= 86.0% |
| Epoch  22, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.815, TAw acc= 72.8% | Valid: time=  0.4s loss=0.489, TAw acc= 85.2% |
| Epoch  23, lr=8.8e-03 time=  1.7s/  2.3s | Train: loss=0.833, TAw acc= 72.5% | Valid: time=  0.9s loss=0.486, TAw acc= 85.6% |
| Epoch  24, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.812, TAw acc= 73.4% | Valid: time=  0.6s loss=0.489, TAw acc= 85.8% |
| Epoch  25, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=0.827, TAw acc= 72.3% | Valid: time=  0.5s loss=0.488, TAw acc= 85.4% |
| Epoch  26, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=0.819, TAw acc= 72.8% | Valid: time=  0.5s loss=0.482, TAw acc= 85.6% |
| Epoch  27, lr=8.8e-03 time=  2.1s/  1.9s | Train: loss=0.825, TAw acc= 73.0% | Valid: time=  0.4s loss=0.479, TAw acc= 86.2% |
| Epoch  28, lr=8.8e-03 time=  2.0s/  2.1s | Train: loss=0.840, TAw acc= 72.5% | Valid: time=  0.4s loss=0.468, TAw acc= 86.0% |
| Epoch  29, lr=8.8e-03 time=  1.8s/  1.9s | Train: loss=0.797, TAw acc= 73.7% | Valid: time=  0.4s loss=0.474, TAw acc= 85.6% |
| Epoch  30, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.812, TAw acc= 72.5% | Valid: time=  0.4s loss=0.471, TAw acc= 86.2% | lr=2.9e-03
| Epoch  31, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=0.933, TAw acc= 69.6% | Valid: time=  0.5s loss=0.487, TAw acc= 85.6% |
| Epoch  32, lr=2.9e-03 time=  3.0s/  1.9s | Train: loss=0.910, TAw acc= 70.4% | Valid: time=  0.5s loss=0.487, TAw acc= 85.0% |
| Epoch  33, lr=2.9e-03 time=  2.2s/  2.4s | Train: loss=0.920, TAw acc= 70.0% | Valid: time=  0.5s loss=0.485, TAw acc= 85.4% |
| Epoch  34, lr=2.9e-03 time=  2.2s/  2.4s | Train: loss=0.905, TAw acc= 70.7% | Valid: time=  0.5s loss=0.479, TAw acc= 85.6% |
| Epoch  35, lr=2.9e-03 time=  1.7s/  2.1s | Train: loss=0.907, TAw acc= 70.2% | Valid: time=  0.4s loss=0.475, TAw acc= 85.4% |
| Epoch  36, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=0.901, TAw acc= 71.1% | Valid: time=  0.5s loss=0.479, TAw acc= 85.2% |
| Epoch  37, lr=2.9e-03 time=  3.0s/  1.9s | Train: loss=0.890, TAw acc= 71.1% | Valid: time=  0.4s loss=0.482, TAw acc= 85.0% |
| Epoch  38, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=0.893, TAw acc= 70.7% | Valid: time=  0.4s loss=0.477, TAw acc= 85.4% |
| Epoch  39, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=0.874, TAw acc= 71.3% | Valid: time=  0.4s loss=0.480, TAw acc= 85.2% |
| Epoch  40, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=0.902, TAw acc= 70.9% | Valid: time=  0.4s loss=0.476, TAw acc= 85.2% |
| Epoch  41, lr=2.9e-03 time=  1.7s/  3.1s | Train: loss=0.893, TAw acc= 71.0% | Valid: time=  0.5s loss=0.479, TAw acc= 85.0% |
| Epoch  42, lr=2.9e-03 time=  1.7s/  2.3s | Train: loss=0.911, TAw acc= 69.6% | Valid: time=  0.5s loss=0.479, TAw acc= 85.0% |
| Epoch  43, lr=2.9e-03 time=  2.1s/  2.4s | Train: loss=0.871, TAw acc= 72.6% | Valid: time=  0.5s loss=0.477, TAw acc= 85.6% |
| Epoch  44, lr=2.9e-03 time=  2.1s/  2.4s | Train: loss=0.879, TAw acc= 71.7% | Valid: time=  0.5s loss=0.476, TAw acc= 85.4% |
| Epoch  45, lr=2.9e-03 time=  2.1s/  2.4s | Train: loss=0.885, TAw acc= 71.0% | Valid: time=  0.5s loss=0.479, TAw acc= 85.2% |
| Epoch  46, lr=2.9e-03 time=  2.1s/  2.1s | Train: loss=0.874, TAw acc= 71.3% | Valid: time=  0.5s loss=0.482, TAw acc= 85.2% |
| Epoch  47, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=0.883, TAw acc= 71.9% | Valid: time=  0.4s loss=0.474, TAw acc= 85.2% |
| Epoch  48, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=0.887, TAw acc= 71.3% | Valid: time=  0.9s loss=0.478, TAw acc= 85.2% |
| Epoch  49, lr=2.9e-03 time=  1.7s/  2.0s | Train: loss=0.880, TAw acc= 71.0% | Valid: time=  0.4s loss=0.476, TAw acc= 85.6% |
| Epoch  50, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=0.874, TAw acc= 71.1% | Valid: time=  0.9s loss=0.483, TAw acc= 85.2% |
| Epoch  51, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=0.878, TAw acc= 71.1% | Valid: time=  0.4s loss=0.476, TAw acc= 85.4% |
| Epoch  52, lr=2.9e-03 time=  2.0s/  2.1s | Train: loss=0.869, TAw acc= 71.6% | Valid: time=  0.4s loss=0.475, TAw acc= 85.8% |
| Epoch  53, lr=2.9e-03 time=  1.8s/  2.1s | Train: loss=0.875, TAw acc= 71.6% | Valid: time=  0.4s loss=0.479, TAw acc= 85.6% |
| Epoch  54, lr=2.9e-03 time=  1.8s/  1.9s | Train: loss=0.875, TAw acc= 71.8% | Valid: time=  0.4s loss=0.485, TAw acc= 85.4% |
| Epoch  55, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=0.858, TAw acc= 72.0% | Valid: time=  0.4s loss=0.482, TAw acc= 85.6% |
| Epoch  56, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=0.859, TAw acc= 72.0% | Valid: time=  0.4s loss=0.481, TAw acc= 86.0% |
| Epoch  57, lr=2.9e-03 time=  1.7s/  2.0s | Train: loss=0.877, TAw acc= 71.4% | Valid: time=  0.4s loss=0.480, TAw acc= 85.2% |
| Epoch  58, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=0.852, TAw acc= 72.2% | Valid: time=  0.4s loss=0.476, TAw acc= 85.8% |
| Epoch  59, lr=2.9e-03 time=  2.3s/  2.0s | Train: loss=0.861, TAw acc= 71.6% | Valid: time=  0.4s loss=0.476, TAw acc= 85.6% |
| Epoch  60, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=0.861, TAw acc= 71.9% | Valid: time=  0.4s loss=0.482, TAw acc= 85.4% | lr=9.7e-04
Debug-2: loss=0.454, acc=0.856
sow: rank=704, freezed_rank=928
 a:        ['-5.95e+00', '+4.96e+00'] .... ['-1.80e+308', '-1.80e+308'] .... ['-1.80e+308', '-1.80e+308']
 a_backup: ['-5.95e+00', '+4.96e+00'] .... ['+4.97e+00', '+4.37e+00'] .... ['+1.29e+00', '-2.74e+00']
Free_Classifier: Done (training=True, fix_classifier=False)
Debug-3: loss=0.453 acc=0.856
sow: rank=1024, freezed_rank=928
 a:        ['-5.95e+00', '+4.96e+00'] .... ['+4.97e+00', '+4.37e+00'] .... ['+1.29e+00', '-2.74e+00']
 a_backup: ['-5.95e+00', '+4.96e+00'] .... ['+4.97e+00', '+4.37e+00'] .... ['+1.29e+00', '-2.74e+00']
Header Training: Finished.
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.592 | TAw acc= 80.3%, forg=  0.0%| TAg acc= 29.2%, forg= 51.1% <<<
>>> Test on task  1 : loss=0.953 | TAw acc= 67.2%, forg=  0.0%| TAg acc= 37.1%, forg= 16.4% <<<
>>> Test on task  2 : loss=0.628 | TAw acc= 79.9%, forg=  0.0%| TAg acc= 47.8%, forg=  9.6% <<<
>>> Test on task  3 : loss=0.612 | TAw acc= 79.6%, forg=  0.0%| TAg acc= 41.8%, forg=  1.9% <<<
>>> Test on task  4 : loss=0.516 | TAw acc= 83.5%, forg=  0.0%| TAg acc= 34.7%, forg=  0.0% <<<
Save at ../RESULT_AAAI2025/CR10/0/cifar100_sow
************************************************************************************************************
Task  5
************************************************************************************************************
LLL_Net(
  (model): ResNet50_32SOW(
    in_channels=3, in_H=32, in_W=32, training=False, fix_features=True, fix_classifier=False
    (features): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
      (3): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (4): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (3): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (5): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (3): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (4): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (5): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (6): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (7): AdaptiveAvgPool2d(output_size=(1, 1))
    )
    (classifier): Sequential(
      (0): Dropout(p=0.417598542370663, inplace=False)
      (1): SOW_V3(in_features=1024, out_features=1024, num_tasks=10, dtype=torch.float64, device=cuda:0)
      (2): ReLU()
    )
    (fc): Sequential()
  )
  (heads): ModuleList(
    (0-5): 6 x Linear(in_features=1024, out_features=10, bias=True)
  )
)
SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.0263039750973134
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0

Parameter Group 1
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.0263039750973134
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0
)
 Group[0]  param_num=161, free_param_num=2
 Group[1]  param_num=3, free_param_num=3
sow: rank=1024, freezed_rank=928
 a:        ['-5.95e+00', '+4.96e+00'] .... ['+4.97e+00', '+4.37e+00'] .... ['+1.29e+00', '-2.74e+00']
 a_backup: ['-5.95e+00', '+4.96e+00'] .... ['+4.97e+00', '+4.37e+00'] .... ['+1.29e+00', '-2.74e+00']
| Epoch   1, lr=2.6e-02 time=  2.7s/  3.2s | Train: loss=1.241, TAw acc= 54.2% | Valid: time=  0.5s loss=1.038, TAw acc= 59.8% | *
| Epoch   2, lr=2.6e-02 time=  2.8s/  2.5s | Train: loss=1.121, TAw acc= 59.7% | Valid: time=  0.5s loss=0.923, TAw acc= 68.0% | *
| Epoch   3, lr=2.6e-02 time=  3.6s/  2.4s | Train: loss=1.117, TAw acc= 60.0% | Valid: time=  0.5s loss=0.900, TAw acc= 67.8% | *
| Epoch   4, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.102, TAw acc= 61.2% | Valid: time=  0.5s loss=0.903, TAw acc= 67.2% |
| Epoch   5, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=1.127, TAw acc= 60.7% | Valid: time=  0.4s loss=0.880, TAw acc= 68.0% | *
| Epoch   6, lr=2.6e-02 time=  2.8s/  2.8s | Train: loss=1.083, TAw acc= 61.5% | Valid: time=  0.4s loss=0.911, TAw acc= 65.2% |
| Epoch   7, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=1.055, TAw acc= 63.5% | Valid: time=  0.4s loss=0.912, TAw acc= 67.2% |
| Epoch   8, lr=2.6e-02 time=  2.7s/  2.0s | Train: loss=1.067, TAw acc= 61.6% | Valid: time=  0.5s loss=0.916, TAw acc= 65.4% |
| Epoch   9, lr=2.6e-02 time=  4.0s/  2.0s | Train: loss=1.052, TAw acc= 61.9% | Valid: time=  0.5s loss=0.881, TAw acc= 70.4% |
| Epoch  10, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.063, TAw acc= 63.3% | Valid: time=  0.5s loss=0.884, TAw acc= 67.2% |
| Epoch  11, lr=2.6e-02 time=  3.6s/  2.3s | Train: loss=1.071, TAw acc= 62.6% | Valid: time=  0.5s loss=0.924, TAw acc= 65.0% |
| Epoch  12, lr=2.6e-02 time=  3.0s/  1.9s | Train: loss=1.049, TAw acc= 62.7% | Valid: time=  0.8s loss=0.857, TAw acc= 69.8% | *
| Epoch  13, lr=2.6e-02 time=  2.8s/  2.0s | Train: loss=1.057, TAw acc= 61.6% | Valid: time=  0.4s loss=0.885, TAw acc= 67.8% |
| Epoch  14, lr=2.6e-02 time=  3.3s/  2.1s | Train: loss=1.030, TAw acc= 62.9% | Valid: time=  0.5s loss=0.857, TAw acc= 69.4% | *
| Epoch  15, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=1.035, TAw acc= 63.0% | Valid: time=  0.4s loss=0.858, TAw acc= 68.2% |
| Epoch  16, lr=2.6e-02 time=  3.7s/  2.0s | Train: loss=1.073, TAw acc= 61.8% | Valid: time=  0.5s loss=0.837, TAw acc= 70.4% | *
| Epoch  17, lr=2.6e-02 time=  3.3s/  2.5s | Train: loss=1.081, TAw acc= 60.9% | Valid: time=  0.5s loss=0.898, TAw acc= 67.0% |
| Epoch  18, lr=2.6e-02 time=  3.9s/  2.4s | Train: loss=1.035, TAw acc= 63.8% | Valid: time=  0.5s loss=0.910, TAw acc= 67.4% |
| Epoch  19, lr=2.6e-02 time=  3.6s/  2.3s | Train: loss=1.039, TAw acc= 63.7% | Valid: time=  0.4s loss=0.847, TAw acc= 69.2% |
| Epoch  20, lr=2.6e-02 time=  2.7s/  1.9s | Train: loss=1.051, TAw acc= 63.0% | Valid: time=  0.4s loss=0.841, TAw acc= 67.6% |
| Epoch  21, lr=2.6e-02 time=  2.7s/  1.9s | Train: loss=1.036, TAw acc= 63.3% | Valid: time=  0.4s loss=0.855, TAw acc= 68.0% |
| Epoch  22, lr=2.6e-02 time=  2.8s/  2.3s | Train: loss=1.044, TAw acc= 63.2% | Valid: time=  0.9s loss=0.836, TAw acc= 69.6% | *
| Epoch  23, lr=2.6e-02 time=  2.8s/  2.3s | Train: loss=1.035, TAw acc= 63.6% | Valid: time=  0.5s loss=0.858, TAw acc= 69.6% |
| Epoch  24, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.018, TAw acc= 64.0% | Valid: time=  0.5s loss=0.869, TAw acc= 69.0% |
| Epoch  25, lr=2.6e-02 time=  3.5s/  1.9s | Train: loss=1.029, TAw acc= 63.8% | Valid: time=  0.4s loss=0.875, TAw acc= 68.4% |
| Epoch  26, lr=2.6e-02 time=  2.7s/  2.0s | Train: loss=1.033, TAw acc= 62.9% | Valid: time=  0.4s loss=0.862, TAw acc= 70.0% |
| Epoch  27, lr=2.6e-02 time=  2.9s/  2.7s | Train: loss=1.044, TAw acc= 62.5% | Valid: time=  0.5s loss=0.852, TAw acc= 69.6% |
| Epoch  28, lr=2.6e-02 time=  3.2s/  2.2s | Train: loss=1.043, TAw acc= 62.9% | Valid: time=  0.5s loss=0.832, TAw acc= 69.2% | *
| Epoch  29, lr=2.6e-02 time=  3.0s/  2.0s | Train: loss=1.026, TAw acc= 62.3% | Valid: time=  0.4s loss=0.834, TAw acc= 71.6% |
| Epoch  30, lr=2.6e-02 time=  2.8s/  2.0s | Train: loss=1.025, TAw acc= 62.7% | Valid: time=  0.8s loss=0.854, TAw acc= 68.4% |
| Epoch  31, lr=2.6e-02 time=  3.4s/  2.2s | Train: loss=0.998, TAw acc= 64.2% | Valid: time=  0.5s loss=0.841, TAw acc= 69.8% |
| Epoch  32, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.042, TAw acc= 62.6% | Valid: time=  0.5s loss=0.839, TAw acc= 71.0% |
| Epoch  33, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.024, TAw acc= 64.1% | Valid: time=  0.5s loss=0.831, TAw acc= 71.2% | *
| Epoch  34, lr=2.6e-02 time=  3.3s/  1.9s | Train: loss=1.040, TAw acc= 63.5% | Valid: time=  0.5s loss=0.849, TAw acc= 69.8% |
| Epoch  35, lr=2.6e-02 time=  3.5s/  2.1s | Train: loss=1.031, TAw acc= 62.5% | Valid: time=  0.5s loss=0.897, TAw acc= 66.6% |
| Epoch  36, lr=2.6e-02 time=  3.1s/  2.1s | Train: loss=1.028, TAw acc= 63.2% | Valid: time=  0.5s loss=0.887, TAw acc= 66.6% |
| Epoch  37, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=1.009, TAw acc= 64.9% | Valid: time=  0.4s loss=0.844, TAw acc= 69.2% |
| Epoch  38, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=1.016, TAw acc= 63.8% | Valid: time=  0.5s loss=0.858, TAw acc= 68.0% |
| Epoch  39, lr=2.6e-02 time=  4.0s/  2.0s | Train: loss=1.009, TAw acc= 64.9% | Valid: time=  0.5s loss=0.818, TAw acc= 69.8% | *
| Epoch  40, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.985, TAw acc= 64.7% | Valid: time=  0.5s loss=0.852, TAw acc= 69.6% |
| Epoch  41, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.031, TAw acc= 63.3% | Valid: time=  0.5s loss=0.837, TAw acc= 69.4% |
| Epoch  42, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.018, TAw acc= 63.2% | Valid: time=  0.5s loss=0.843, TAw acc= 69.2% |
| Epoch  43, lr=2.6e-02 time=  3.3s/  2.0s | Train: loss=1.019, TAw acc= 63.5% | Valid: time=  0.4s loss=0.839, TAw acc= 68.8% |
| Epoch  44, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=1.027, TAw acc= 64.0% | Valid: time=  0.4s loss=0.822, TAw acc= 69.0% |
| Epoch  45, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=1.039, TAw acc= 62.8% | Valid: time=  0.9s loss=0.856, TAw acc= 68.4% |
| Epoch  46, lr=2.6e-02 time=  3.2s/  2.2s | Train: loss=1.009, TAw acc= 63.9% | Valid: time=  0.5s loss=0.832, TAw acc= 68.8% |
| Epoch  47, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.031, TAw acc= 63.6% | Valid: time=  0.5s loss=0.818, TAw acc= 70.0% | *
| Epoch  48, lr=2.6e-02 time=  3.2s/  1.9s | Train: loss=1.002, TAw acc= 64.1% | Valid: time=  0.4s loss=0.871, TAw acc= 68.0% |
| Epoch  49, lr=2.6e-02 time=  2.7s/  1.9s | Train: loss=1.015, TAw acc= 64.8% | Valid: time=  0.5s loss=0.869, TAw acc= 68.8% |
| Epoch  50, lr=2.6e-02 time=  2.8s/  2.0s | Train: loss=0.999, TAw acc= 64.2% | Valid: time=  0.5s loss=0.809, TAw acc= 71.0% | *
| Epoch  51, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=1.031, TAw acc= 62.8% | Valid: time=  0.5s loss=0.859, TAw acc= 67.4% |
| Epoch  52, lr=2.6e-02 time=  2.8s/  2.0s | Train: loss=1.013, TAw acc= 64.0% | Valid: time=  0.5s loss=0.829, TAw acc= 70.2% |
| Epoch  53, lr=2.6e-02 time=  2.8s/  2.0s | Train: loss=1.045, TAw acc= 61.5% | Valid: time=  0.5s loss=0.814, TAw acc= 70.6% |
| Epoch  54, lr=2.6e-02 time=  2.8s/  2.0s | Train: loss=1.036, TAw acc= 63.3% | Valid: time=  0.5s loss=0.828, TAw acc= 69.2% |
| Epoch  55, lr=2.6e-02 time=  2.8s/  2.0s | Train: loss=1.005, TAw acc= 64.8% | Valid: time=  0.5s loss=0.865, TAw acc= 67.4% |
| Epoch  56, lr=2.6e-02 time=  2.8s/  2.0s | Train: loss=1.035, TAw acc= 62.6% | Valid: time=  0.5s loss=0.852, TAw acc= 69.6% |
| Epoch  57, lr=2.6e-02 time=  4.1s/  2.0s | Train: loss=1.040, TAw acc= 62.7% | Valid: time=  0.6s loss=0.837, TAw acc= 69.6% |
| Epoch  58, lr=2.6e-02 time=  3.7s/  2.5s | Train: loss=1.012, TAw acc= 64.3% | Valid: time=  0.5s loss=0.834, TAw acc= 69.2% |
| Epoch  59, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.028, TAw acc= 63.4% | Valid: time=  0.5s loss=0.840, TAw acc= 67.6% |
| Epoch  60, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.005, TAw acc= 64.4% | Valid: time=  0.5s loss=0.831, TAw acc= 69.2% |
== Rank Reduction [task:5] ==
Debug-0:
  best_loss=0.809,   best_acc=0.710
loss_margin=0.006, acc_margin=0.004
sow: rank=1024, freezed_rank=928
 a:        ['-6.21e+00', '+4.96e+00'] .... ['+4.97e+00', '+4.38e+00'] .... ['+1.29e+00', '-2.74e+00']
 a_backup: ['-6.21e+00', '+4.96e+00'] .... ['+4.97e+00', '+4.38e+00'] .... ['+1.29e+00', '-2.74e+00']
 r=256, loss=1.866, acc=0.348 (rank=256)
 r=288, loss=1.682, acc=0.432 (rank=288)
 r=320, loss=1.483, acc=0.478 (rank=320)
 r=352, loss=1.343, acc=0.542 (rank=352)
 r=384, loss=1.224, acc=0.554 (rank=384)
 r=416, loss=1.149, acc=0.592 (rank=416)
 r=448, loss=0.996, acc=0.638 (rank=448)
 r=480, loss=0.911, acc=0.684 (rank=480)
 r=512, loss=0.830, acc=0.716 (rank=512)
 r=544, loss=0.837, acc=0.716 (rank=544)
 r=576, loss=0.856, acc=0.684 (rank=576)
 r=608, loss=0.851, acc=0.696 (rank=608)
 r=640, loss=0.834, acc=0.700 (rank=640)
 r=672, loss=0.838, acc=0.704 (rank=672)
 r=704, loss=0.827, acc=0.704 (rank=704)
 r=736, loss=0.816, acc=0.714 (rank=736)
 r=768, loss=0.807, acc=0.734 (rank=768)
 best_r=768, loss=0.807, acc=0.734
== Header Training for Low Rank [task:5] ==
Fix_Classifier: Done (training=False, fix_classifier=True)
Debug-1: loss=0.807, acc=0.734
sow: rank=768, freezed_rank=928
 a:        ['-6.21e+00', '+4.96e+00'] .... ['-1.80e+308', '-1.80e+308'] .... ['-1.80e+308', '-1.80e+308']
 a_backup: ['-6.21e+00', '+4.96e+00'] .... ['+4.97e+00', '+4.38e+00'] .... ['+1.29e+00', '-2.74e+00']
SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.008767991699104466
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0

Parameter Group 1
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.0263039750973134
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0
)
 Group[0]  param_num=161, free_param_num=2
 Group[1]  param_num=3, free_param_num=0
sow: rank=768, freezed_rank=928
 a:        ['-6.21e+00', '+4.96e+00'] .... ['-1.80e+308', '-1.80e+308'] .... ['-1.80e+308', '-1.80e+308']
 a_backup: ['-6.21e+00', '+4.96e+00'] .... ['+4.97e+00', '+4.38e+00'] .... ['+1.29e+00', '-2.74e+00']
| Epoch   1, lr=8.8e-03 time=  1.7s/  3.2s | Train: loss=0.994, TAw acc= 65.3% | Valid: time=  0.5s loss=0.864, TAw acc= 69.2% |
| Epoch   2, lr=8.8e-03 time=  1.7s/  2.3s | Train: loss=1.008, TAw acc= 64.6% | Valid: time=  0.5s loss=0.862, TAw acc= 69.2% |
| Epoch   3, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=0.964, TAw acc= 66.0% | Valid: time=  0.5s loss=0.846, TAw acc= 70.4% |
| Epoch   4, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=1.006, TAw acc= 64.5% | Valid: time=  0.5s loss=0.861, TAw acc= 69.4% |
| Epoch   5, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=0.987, TAw acc= 65.1% | Valid: time=  0.5s loss=0.855, TAw acc= 69.8% |
| Epoch   6, lr=8.8e-03 time=  2.2s/  2.0s | Train: loss=0.993, TAw acc= 64.8% | Valid: time=  0.5s loss=0.848, TAw acc= 70.6% |
| Epoch   7, lr=8.8e-03 time=  2.1s/  2.0s | Train: loss=0.992, TAw acc= 64.8% | Valid: time=  0.5s loss=0.847, TAw acc= 70.2% |
| Epoch   8, lr=8.8e-03 time=  1.7s/  2.6s | Train: loss=0.983, TAw acc= 64.7% | Valid: time=  0.4s loss=0.858, TAw acc= 70.4% |
| Epoch   9, lr=8.8e-03 time=  1.7s/  2.0s | Train: loss=0.968, TAw acc= 65.5% | Valid: time=  0.5s loss=0.847, TAw acc= 71.0% |
| Epoch  10, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.990, TAw acc= 65.8% | Valid: time=  0.4s loss=0.850, TAw acc= 69.8% |
| Epoch  11, lr=8.8e-03 time=  1.7s/  2.8s | Train: loss=0.997, TAw acc= 64.7% | Valid: time=  0.5s loss=0.851, TAw acc= 69.8% |
| Epoch  12, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.980, TAw acc= 65.6% | Valid: time=  0.7s loss=0.847, TAw acc= 70.4% |
| Epoch  13, lr=8.8e-03 time=  1.8s/  2.1s | Train: loss=0.977, TAw acc= 65.7% | Valid: time=  0.4s loss=0.854, TAw acc= 70.2% |
| Epoch  14, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.958, TAw acc= 65.9% | Valid: time=  0.5s loss=0.845, TAw acc= 70.2% |
| Epoch  15, lr=8.8e-03 time=  1.7s/  2.9s | Train: loss=0.966, TAw acc= 66.9% | Valid: time=  0.5s loss=0.848, TAw acc= 69.2% |
| Epoch  16, lr=8.8e-03 time=  1.8s/  1.9s | Train: loss=0.948, TAw acc= 67.0% | Valid: time=  0.5s loss=0.856, TAw acc= 69.8% |
| Epoch  17, lr=8.8e-03 time=  2.2s/  2.0s | Train: loss=0.979, TAw acc= 64.5% | Valid: time=  0.4s loss=0.838, TAw acc= 70.0% |
| Epoch  18, lr=8.8e-03 time=  1.7s/  2.3s | Train: loss=0.965, TAw acc= 65.9% | Valid: time=  0.5s loss=0.841, TAw acc= 69.6% |
| Epoch  19, lr=8.8e-03 time=  1.7s/  2.4s | Train: loss=0.956, TAw acc= 66.4% | Valid: time=  0.5s loss=0.831, TAw acc= 70.0% |
| Epoch  20, lr=8.8e-03 time=  2.0s/  2.0s | Train: loss=0.948, TAw acc= 66.9% | Valid: time=  0.4s loss=0.848, TAw acc= 70.0% |
| Epoch  21, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.977, TAw acc= 66.3% | Valid: time=  0.4s loss=0.859, TAw acc= 69.2% |
| Epoch  22, lr=8.8e-03 time=  1.8s/  1.9s | Train: loss=0.971, TAw acc= 66.7% | Valid: time=  0.4s loss=0.843, TAw acc= 69.8% |
| Epoch  23, lr=8.8e-03 time=  2.8s/  2.0s | Train: loss=0.950, TAw acc= 66.8% | Valid: time=  0.5s loss=0.861, TAw acc= 69.4% |
| Epoch  24, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=0.955, TAw acc= 66.9% | Valid: time=  0.5s loss=0.835, TAw acc= 70.4% |
| Epoch  25, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=0.960, TAw acc= 66.5% | Valid: time=  0.5s loss=0.829, TAw acc= 70.8% |
| Epoch  26, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=0.948, TAw acc= 66.4% | Valid: time=  0.5s loss=0.840, TAw acc= 70.0% |
| Epoch  27, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=0.941, TAw acc= 66.6% | Valid: time=  0.5s loss=0.823, TAw acc= 70.8% |
| Epoch  28, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=0.955, TAw acc= 66.5% | Valid: time=  0.5s loss=0.844, TAw acc= 69.4% |
| Epoch  29, lr=8.8e-03 time=  2.0s/  2.0s | Train: loss=0.960, TAw acc= 66.3% | Valid: time=  0.4s loss=0.841, TAw acc= 69.6% |
| Epoch  30, lr=8.8e-03 time=  1.7s/  2.3s | Train: loss=0.972, TAw acc= 66.5% | Valid: time=  0.5s loss=0.842, TAw acc= 69.6% | lr=2.9e-03
| Epoch  31, lr=2.9e-03 time=  2.0s/  1.9s | Train: loss=0.990, TAw acc= 65.4% | Valid: time=  0.5s loss=0.847, TAw acc= 70.2% |
| Epoch  32, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=1.015, TAw acc= 64.4% | Valid: time=  0.5s loss=0.857, TAw acc= 70.0% |
| Epoch  33, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=0.994, TAw acc= 65.4% | Valid: time=  0.5s loss=0.863, TAw acc= 68.6% |
| Epoch  34, lr=2.9e-03 time=  1.7s/  2.3s | Train: loss=1.003, TAw acc= 65.2% | Valid: time=  0.9s loss=0.855, TAw acc= 69.0% |
| Epoch  35, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=1.006, TAw acc= 64.2% | Valid: time=  0.5s loss=0.856, TAw acc= 70.0% |
| Epoch  36, lr=2.9e-03 time=  2.2s/  2.4s | Train: loss=0.983, TAw acc= 65.3% | Valid: time=  0.5s loss=0.852, TAw acc= 70.4% |
| Epoch  37, lr=2.9e-03 time=  2.2s/  2.4s | Train: loss=0.998, TAw acc= 64.2% | Valid: time=  0.5s loss=0.857, TAw acc= 70.0% |
| Epoch  38, lr=2.9e-03 time=  2.2s/  2.5s | Train: loss=1.004, TAw acc= 64.1% | Valid: time=  0.5s loss=0.855, TAw acc= 69.4% |
| Epoch  39, lr=2.9e-03 time=  2.2s/  2.4s | Train: loss=0.988, TAw acc= 65.3% | Valid: time=  0.5s loss=0.850, TAw acc= 69.8% |
| Epoch  40, lr=2.9e-03 time=  2.2s/  2.5s | Train: loss=0.981, TAw acc= 65.7% | Valid: time=  0.6s loss=0.848, TAw acc= 70.4% |
| Epoch  41, lr=2.9e-03 time=  2.2s/  2.5s | Train: loss=0.985, TAw acc= 65.4% | Valid: time=  0.5s loss=0.850, TAw acc= 70.0% |
| Epoch  42, lr=2.9e-03 time=  2.2s/  2.3s | Train: loss=0.994, TAw acc= 64.7% | Valid: time=  0.4s loss=0.847, TAw acc= 70.0% |
| Epoch  43, lr=2.9e-03 time=  1.9s/  1.9s | Train: loss=0.970, TAw acc= 66.2% | Valid: time=  0.4s loss=0.845, TAw acc= 70.0% |
| Epoch  44, lr=2.9e-03 time=  1.7s/  2.0s | Train: loss=0.970, TAw acc= 66.2% | Valid: time=  0.4s loss=0.845, TAw acc= 70.2% |
| Epoch  45, lr=2.9e-03 time=  1.7s/  3.1s | Train: loss=0.986, TAw acc= 65.1% | Valid: time=  0.5s loss=0.844, TAw acc= 70.2% |
| Epoch  46, lr=2.9e-03 time=  1.7s/  2.4s | Train: loss=0.981, TAw acc= 65.4% | Valid: time=  0.5s loss=0.842, TAw acc= 70.6% |
| Epoch  47, lr=2.9e-03 time=  2.2s/  2.5s | Train: loss=0.992, TAw acc= 64.8% | Valid: time=  0.5s loss=0.847, TAw acc= 70.2% |
| Epoch  48, lr=2.9e-03 time=  2.2s/  2.5s | Train: loss=0.981, TAw acc= 66.0% | Valid: time=  0.5s loss=0.845, TAw acc= 70.0% |
| Epoch  49, lr=2.9e-03 time=  2.2s/  2.4s | Train: loss=0.985, TAw acc= 65.3% | Valid: time=  0.4s loss=0.849, TAw acc= 70.2% |
| Epoch  50, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=0.990, TAw acc= 64.8% | Valid: time=  0.4s loss=0.850, TAw acc= 70.2% |
| Epoch  51, lr=2.9e-03 time=  2.3s/  1.9s | Train: loss=0.981, TAw acc= 65.8% | Valid: time=  0.4s loss=0.847, TAw acc= 70.8% |
| Epoch  52, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=0.970, TAw acc= 66.2% | Valid: time=  0.4s loss=0.850, TAw acc= 70.2% |
| Epoch  53, lr=2.9e-03 time=  2.4s/  1.9s | Train: loss=0.984, TAw acc= 65.3% | Valid: time=  0.4s loss=0.845, TAw acc= 70.8% |
| Epoch  54, lr=2.9e-03 time=  1.8s/  1.9s | Train: loss=0.981, TAw acc= 66.4% | Valid: time=  0.4s loss=0.851, TAw acc= 70.6% |
| Epoch  55, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=0.980, TAw acc= 65.5% | Valid: time=  0.4s loss=0.841, TAw acc= 71.0% |
| Epoch  56, lr=2.9e-03 time=  1.8s/  1.9s | Train: loss=0.988, TAw acc= 65.2% | Valid: time=  0.5s loss=0.845, TAw acc= 70.0% |
| Epoch  57, lr=2.9e-03 time=  3.0s/  1.9s | Train: loss=0.973, TAw acc= 65.9% | Valid: time=  0.5s loss=0.850, TAw acc= 70.2% |
| Epoch  58, lr=2.9e-03 time=  2.1s/  2.4s | Train: loss=0.980, TAw acc= 65.5% | Valid: time=  0.5s loss=0.851, TAw acc= 70.0% |
| Epoch  59, lr=2.9e-03 time=  2.2s/  2.4s | Train: loss=0.975, TAw acc= 65.4% | Valid: time=  0.5s loss=0.843, TAw acc= 70.6% |
| Epoch  60, lr=2.9e-03 time=  2.2s/  2.4s | Train: loss=0.981, TAw acc= 65.8% | Valid: time=  0.5s loss=0.843, TAw acc= 70.8% | lr=9.7e-04
Debug-2: loss=0.807, acc=0.734
sow: rank=768, freezed_rank=928
 a:        ['-6.21e+00', '+4.96e+00'] .... ['-1.80e+308', '-1.80e+308'] .... ['-1.80e+308', '-1.80e+308']
 a_backup: ['-6.21e+00', '+4.96e+00'] .... ['+4.97e+00', '+4.38e+00'] .... ['+1.29e+00', '-2.74e+00']
Free_Classifier: Done (training=True, fix_classifier=False)
Debug-3: loss=0.809 acc=0.710
sow: rank=1024, freezed_rank=928
 a:        ['-6.21e+00', '+4.96e+00'] .... ['+4.97e+00', '+4.38e+00'] .... ['+1.29e+00', '-2.74e+00']
 a_backup: ['-6.21e+00', '+4.96e+00'] .... ['+4.97e+00', '+4.38e+00'] .... ['+1.29e+00', '-2.74e+00']
Header Training: Finished.
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.592 | TAw acc= 80.3%, forg=  0.0%| TAg acc= 26.1%, forg= 54.2% <<<
>>> Test on task  1 : loss=0.953 | TAw acc= 67.2%, forg=  0.0%| TAg acc= 35.4%, forg= 18.1% <<<
>>> Test on task  2 : loss=0.628 | TAw acc= 79.9%, forg=  0.0%| TAg acc= 42.7%, forg= 14.7% <<<
>>> Test on task  3 : loss=0.612 | TAw acc= 79.6%, forg=  0.0%| TAg acc= 35.8%, forg=  7.9% <<<
>>> Test on task  4 : loss=0.516 | TAw acc= 83.5%, forg=  0.0%| TAg acc= 29.3%, forg=  5.4% <<<
>>> Test on task  5 : loss=0.808 | TAw acc= 69.6%, forg=  0.0%| TAg acc= 37.2%, forg=  0.0% <<<
Save at ../RESULT_AAAI2025/CR10/0/cifar100_sow
************************************************************************************************************
Task  6
************************************************************************************************************
LLL_Net(
  (model): ResNet50_32SOW(
    in_channels=3, in_H=32, in_W=32, training=False, fix_features=True, fix_classifier=False
    (features): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
      (3): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (4): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (3): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (5): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (3): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (4): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (5): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (6): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (7): AdaptiveAvgPool2d(output_size=(1, 1))
    )
    (classifier): Sequential(
      (0): Dropout(p=0.417598542370663, inplace=False)
      (1): SOW_V3(in_features=1024, out_features=1024, num_tasks=10, dtype=torch.float64, device=cuda:0)
      (2): ReLU()
    )
    (fc): Sequential()
  )
  (heads): ModuleList(
    (0-6): 7 x Linear(in_features=1024, out_features=10, bias=True)
  )
)
SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.0263039750973134
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0

Parameter Group 1
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.0263039750973134
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0
)
 Group[0]  param_num=161, free_param_num=2
 Group[1]  param_num=3, free_param_num=3
sow: rank=1024, freezed_rank=928
 a:        ['-6.21e+00', '+4.96e+00'] .... ['+4.97e+00', '+4.38e+00'] .... ['+1.29e+00', '-2.74e+00']
 a_backup: ['-6.21e+00', '+4.96e+00'] .... ['+4.97e+00', '+4.38e+00'] .... ['+1.29e+00', '-2.74e+00']
| Epoch   1, lr=2.6e-02 time=  2.8s/  2.6s | Train: loss=1.181, TAw acc= 60.8% | Valid: time=  0.5s loss=0.790, TAw acc= 72.8% | *
| Epoch   2, lr=2.6e-02 time=  3.0s/  2.0s | Train: loss=1.110, TAw acc= 63.2% | Valid: time=  0.5s loss=0.712, TAw acc= 74.0% | *
| Epoch   3, lr=2.6e-02 time=  2.8s/  2.0s | Train: loss=1.077, TAw acc= 63.8% | Valid: time=  0.4s loss=0.678, TAw acc= 77.6% | *
| Epoch   4, lr=2.6e-02 time=  4.1s/  2.0s | Train: loss=1.093, TAw acc= 64.0% | Valid: time=  0.5s loss=0.685, TAw acc= 76.8% |
| Epoch   5, lr=2.6e-02 time=  3.4s/  2.5s | Train: loss=1.072, TAw acc= 64.5% | Valid: time=  0.5s loss=0.677, TAw acc= 76.8% | *
| Epoch   6, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.029, TAw acc= 66.2% | Valid: time=  0.5s loss=0.683, TAw acc= 76.4% |
| Epoch   7, lr=2.6e-02 time=  3.6s/  2.0s | Train: loss=1.039, TAw acc= 65.2% | Valid: time=  0.4s loss=0.649, TAw acc= 79.0% | *
| Epoch   8, lr=2.6e-02 time=  2.8s/  2.0s | Train: loss=1.056, TAw acc= 65.4% | Valid: time=  0.5s loss=0.670, TAw acc= 77.4% |
| Epoch   9, lr=2.6e-02 time=  4.1s/  2.0s | Train: loss=1.023, TAw acc= 66.1% | Valid: time=  0.5s loss=0.647, TAw acc= 77.2% | *
| Epoch  10, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.013, TAw acc= 65.9% | Valid: time=  0.5s loss=0.691, TAw acc= 77.6% |
| Epoch  11, lr=2.6e-02 time=  3.5s/  1.9s | Train: loss=1.018, TAw acc= 66.8% | Valid: time=  0.4s loss=0.662, TAw acc= 76.4% |
| Epoch  12, lr=2.6e-02 time=  2.8s/  2.0s | Train: loss=1.006, TAw acc= 66.1% | Valid: time=  0.5s loss=0.674, TAw acc= 76.2% |
| Epoch  13, lr=2.6e-02 time=  2.8s/  3.2s | Train: loss=0.978, TAw acc= 67.9% | Valid: time=  0.5s loss=0.644, TAw acc= 78.2% | *
| Epoch  14, lr=2.6e-02 time=  2.8s/  2.5s | Train: loss=1.006, TAw acc= 67.8% | Valid: time=  0.5s loss=0.695, TAw acc= 75.8% |
| Epoch  15, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.997, TAw acc= 67.0% | Valid: time=  0.5s loss=0.679, TAw acc= 77.0% |
| Epoch  16, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.992, TAw acc= 67.2% | Valid: time=  0.5s loss=0.664, TAw acc= 75.6% |
| Epoch  17, lr=2.6e-02 time=  3.0s/  1.9s | Train: loss=0.986, TAw acc= 66.9% | Valid: time=  0.4s loss=0.698, TAw acc= 76.0% |
| Epoch  18, lr=2.6e-02 time=  3.1s/  2.0s | Train: loss=0.987, TAw acc= 67.4% | Valid: time=  1.0s loss=0.677, TAw acc= 77.0% |
| Epoch  19, lr=2.6e-02 time=  3.2s/  2.3s | Train: loss=1.024, TAw acc= 66.6% | Valid: time=  0.5s loss=0.628, TAw acc= 78.8% | *
| Epoch  20, lr=2.6e-02 time=  3.6s/  2.4s | Train: loss=0.975, TAw acc= 67.2% | Valid: time=  0.5s loss=0.636, TAw acc= 77.2% |
| Epoch  21, lr=2.6e-02 time=  3.2s/  1.9s | Train: loss=0.980, TAw acc= 68.4% | Valid: time=  0.4s loss=0.617, TAw acc= 78.6% | *
| Epoch  22, lr=2.6e-02 time=  2.7s/  2.0s | Train: loss=0.991, TAw acc= 66.7% | Valid: time=  0.5s loss=0.631, TAw acc= 79.4% |
| Epoch  23, lr=2.6e-02 time=  4.1s/  2.0s | Train: loss=0.994, TAw acc= 67.2% | Valid: time=  0.5s loss=0.687, TAw acc= 77.2% |
| Epoch  24, lr=2.6e-02 time=  3.6s/  2.6s | Train: loss=0.991, TAw acc= 67.2% | Valid: time=  0.5s loss=0.672, TAw acc= 76.0% |
| Epoch  25, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.977, TAw acc= 67.3% | Valid: time=  0.5s loss=0.646, TAw acc= 77.6% |
| Epoch  26, lr=2.6e-02 time=  3.1s/  1.9s | Train: loss=0.992, TAw acc= 66.6% | Valid: time=  0.4s loss=0.630, TAw acc= 76.4% |
| Epoch  27, lr=2.6e-02 time=  2.7s/  1.9s | Train: loss=0.980, TAw acc= 66.6% | Valid: time=  0.5s loss=0.655, TAw acc= 78.2% |
| Epoch  28, lr=2.6e-02 time=  2.8s/  3.2s | Train: loss=1.003, TAw acc= 66.4% | Valid: time=  0.5s loss=0.605, TAw acc= 78.2% | *
| Epoch  29, lr=2.6e-02 time=  2.8s/  2.5s | Train: loss=0.950, TAw acc= 68.7% | Valid: time=  0.5s loss=0.632, TAw acc= 78.8% |
| Epoch  30, lr=2.6e-02 time=  3.7s/  2.4s | Train: loss=1.006, TAw acc= 66.5% | Valid: time=  0.5s loss=0.631, TAw acc= 78.0% |
| Epoch  31, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.953, TAw acc= 68.7% | Valid: time=  0.5s loss=0.604, TAw acc= 79.4% | *
| Epoch  32, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.963, TAw acc= 68.2% | Valid: time=  0.5s loss=0.613, TAw acc= 80.0% |
| Epoch  33, lr=2.6e-02 time=  3.2s/  2.3s | Train: loss=0.985, TAw acc= 68.0% | Valid: time=  0.4s loss=0.621, TAw acc= 78.4% |
| Epoch  34, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=0.988, TAw acc= 66.2% | Valid: time=  0.4s loss=0.641, TAw acc= 78.6% |
| Epoch  35, lr=2.6e-02 time=  3.9s/  2.0s | Train: loss=0.963, TAw acc= 67.8% | Valid: time=  0.4s loss=0.613, TAw acc= 80.2% |
| Epoch  36, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=0.942, TAw acc= 68.9% | Valid: time=  0.4s loss=0.634, TAw acc= 78.6% |
| Epoch  37, lr=2.6e-02 time=  4.0s/  1.9s | Train: loss=0.967, TAw acc= 68.0% | Valid: time=  0.4s loss=0.663, TAw acc= 78.0% |
| Epoch  38, lr=2.6e-02 time=  3.0s/  2.1s | Train: loss=0.969, TAw acc= 68.1% | Valid: time=  0.5s loss=0.687, TAw acc= 76.8% |
| Epoch  39, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=0.957, TAw acc= 68.3% | Valid: time=  0.4s loss=0.639, TAw acc= 79.2% |
| Epoch  40, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=0.954, TAw acc= 68.5% | Valid: time=  0.5s loss=0.649, TAw acc= 77.6% |
| Epoch  41, lr=2.6e-02 time=  4.1s/  2.0s | Train: loss=0.971, TAw acc= 67.5% | Valid: time=  0.5s loss=0.632, TAw acc= 80.6% |
| Epoch  42, lr=2.6e-02 time=  3.7s/  2.5s | Train: loss=0.972, TAw acc= 67.9% | Valid: time=  0.4s loss=0.651, TAw acc= 78.2% |
| Epoch  43, lr=2.6e-02 time=  2.7s/  1.9s | Train: loss=0.954, TAw acc= 67.8% | Valid: time=  0.4s loss=0.617, TAw acc= 78.8% |
| Epoch  44, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=0.938, TAw acc= 68.7% | Valid: time=  0.5s loss=0.606, TAw acc= 80.4% |
| Epoch  45, lr=2.6e-02 time=  4.1s/  2.0s | Train: loss=0.957, TAw acc= 68.7% | Valid: time=  0.5s loss=0.640, TAw acc= 79.2% |
| Epoch  46, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.979, TAw acc= 67.9% | Valid: time=  0.5s loss=0.610, TAw acc= 81.2% |
| Epoch  47, lr=2.6e-02 time=  3.6s/  2.4s | Train: loss=0.976, TAw acc= 67.4% | Valid: time=  0.4s loss=0.633, TAw acc= 78.8% |
| Epoch  48, lr=2.6e-02 time=  3.1s/  1.9s | Train: loss=0.967, TAw acc= 68.0% | Valid: time=  0.4s loss=0.629, TAw acc= 78.6% |
| Epoch  49, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=0.952, TAw acc= 68.8% | Valid: time=  0.4s loss=0.613, TAw acc= 80.6% |
| Epoch  50, lr=2.6e-02 time=  3.0s/  1.9s | Train: loss=0.933, TAw acc= 69.2% | Valid: time=  0.4s loss=0.609, TAw acc= 80.6% |
| Epoch  51, lr=2.6e-02 time=  2.7s/  1.9s | Train: loss=0.948, TAw acc= 68.5% | Valid: time=  0.4s loss=0.627, TAw acc= 80.8% |
| Epoch  52, lr=2.6e-02 time=  2.8s/  2.0s | Train: loss=0.987, TAw acc= 67.3% | Valid: time=  0.4s loss=0.600, TAw acc= 80.2% | *
| Epoch  53, lr=2.6e-02 time=  4.1s/  2.0s | Train: loss=0.961, TAw acc= 67.9% | Valid: time=  0.5s loss=0.601, TAw acc= 80.8% |
| Epoch  54, lr=2.6e-02 time=  3.5s/  2.5s | Train: loss=0.918, TAw acc= 69.1% | Valid: time=  0.5s loss=0.670, TAw acc= 77.8% |
| Epoch  55, lr=2.6e-02 time=  3.0s/  2.2s | Train: loss=0.952, TAw acc= 67.0% | Valid: time=  0.5s loss=0.631, TAw acc= 79.8% |
| Epoch  56, lr=2.6e-02 time=  3.1s/  2.0s | Train: loss=0.959, TAw acc= 67.5% | Valid: time=  0.4s loss=0.613, TAw acc= 79.8% |
| Epoch  57, lr=2.6e-02 time=  3.2s/  2.2s | Train: loss=0.955, TAw acc= 69.1% | Valid: time=  0.5s loss=0.621, TAw acc= 79.6% |
| Epoch  58, lr=2.6e-02 time=  2.8s/  2.5s | Train: loss=0.960, TAw acc= 67.2% | Valid: time=  0.5s loss=0.636, TAw acc= 79.4% |
| Epoch  59, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=0.973, TAw acc= 67.3% | Valid: time=  0.5s loss=0.633, TAw acc= 79.8% |
| Epoch  60, lr=2.6e-02 time=  3.6s/  2.0s | Train: loss=0.950, TAw acc= 68.7% | Valid: time=  0.5s loss=0.633, TAw acc= 78.6% |
== Rank Reduction [task:6] ==
Debug-0:
  best_loss=0.600,   best_acc=0.802
loss_margin=0.006, acc_margin=0.004
sow: rank=1024, freezed_rank=928
 a:        ['-6.27e+00', '+4.95e+00'] .... ['+4.97e+00', '+4.38e+00'] .... ['+1.29e+00', '-2.74e+00']
 a_backup: ['-6.27e+00', '+4.95e+00'] .... ['+4.97e+00', '+4.38e+00'] .... ['+1.29e+00', '-2.74e+00']
 r=256, loss=2.077, acc=0.230 (rank=256)
 r=288, loss=2.002, acc=0.262 (rank=288)
 r=320, loss=1.646, acc=0.424 (rank=320)
 r=352, loss=1.329, acc=0.584 (rank=352)
 r=384, loss=1.166, acc=0.638 (rank=384)
 r=416, loss=0.964, acc=0.704 (rank=416)
 r=448, loss=0.776, acc=0.770 (rank=448)
 r=480, loss=0.710, acc=0.776 (rank=480)
 r=512, loss=0.652, acc=0.778 (rank=512)
 r=544, loss=0.647, acc=0.776 (rank=544)
 r=576, loss=0.657, acc=0.768 (rank=576)
 r=608, loss=0.664, acc=0.770 (rank=608)
 r=640, loss=0.649, acc=0.786 (rank=640)
 r=672, loss=0.635, acc=0.786 (rank=672)
 r=704, loss=0.616, acc=0.792 (rank=704)
 r=736, loss=0.601, acc=0.796 (rank=736)
 r=768, loss=0.590, acc=0.806 (rank=768)
 best_r=768, loss=0.590, acc=0.806
== Header Training for Low Rank [task:6] ==
Fix_Classifier: Done (training=False, fix_classifier=True)
Debug-1: loss=0.590, acc=0.806
sow: rank=768, freezed_rank=928
 a:        ['-6.27e+00', '+4.95e+00'] .... ['-1.80e+308', '-1.80e+308'] .... ['-1.80e+308', '-1.80e+308']
 a_backup: ['-6.27e+00', '+4.95e+00'] .... ['+4.97e+00', '+4.38e+00'] .... ['+1.29e+00', '-2.74e+00']
SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.008767991699104466
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0

Parameter Group 1
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.0263039750973134
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0
)
 Group[0]  param_num=161, free_param_num=2
 Group[1]  param_num=3, free_param_num=0
sow: rank=768, freezed_rank=928
 a:        ['-6.27e+00', '+4.95e+00'] .... ['-1.80e+308', '-1.80e+308'] .... ['-1.80e+308', '-1.80e+308']
 a_backup: ['-6.27e+00', '+4.95e+00'] .... ['+4.97e+00', '+4.38e+00'] .... ['+1.29e+00', '-2.74e+00']
| Epoch   1, lr=8.8e-03 time=  1.8s/  1.9s | Train: loss=0.921, TAw acc= 70.0% | Valid: time=  0.4s loss=0.701, TAw acc= 76.8% |
| Epoch   2, lr=8.8e-03 time=  1.7s/  2.2s | Train: loss=0.930, TAw acc= 68.4% | Valid: time=  0.9s loss=0.674, TAw acc= 77.4% |
| Epoch   3, lr=8.8e-03 time=  1.7s/  2.0s | Train: loss=0.926, TAw acc= 69.6% | Valid: time=  0.5s loss=0.668, TAw acc= 76.8% |
| Epoch   4, lr=8.8e-03 time=  2.3s/  2.5s | Train: loss=0.920, TAw acc= 69.7% | Valid: time=  0.5s loss=0.659, TAw acc= 77.6% |
| Epoch   5, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=0.935, TAw acc= 69.9% | Valid: time=  0.5s loss=0.682, TAw acc= 76.8% |
| Epoch   6, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=0.924, TAw acc= 69.8% | Valid: time=  0.5s loss=0.664, TAw acc= 77.2% |
| Epoch   7, lr=8.8e-03 time=  1.7s/  2.4s | Train: loss=0.906, TAw acc= 70.3% | Valid: time=  0.5s loss=0.670, TAw acc= 77.4% |
| Epoch   8, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.920, TAw acc= 69.7% | Valid: time=  0.4s loss=0.667, TAw acc= 77.0% |
| Epoch   9, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.922, TAw acc= 69.8% | Valid: time=  0.4s loss=0.677, TAw acc= 76.6% |
| Epoch  10, lr=8.8e-03 time=  2.6s/  2.0s | Train: loss=0.922, TAw acc= 70.1% | Valid: time=  0.5s loss=0.676, TAw acc= 77.4% |
| Epoch  11, lr=8.8e-03 time=  1.8s/  2.5s | Train: loss=0.914, TAw acc= 70.4% | Valid: time=  0.5s loss=0.695, TAw acc= 76.2% |
| Epoch  12, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=0.908, TAw acc= 69.7% | Valid: time=  0.5s loss=0.666, TAw acc= 77.2% |
| Epoch  13, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=0.895, TAw acc= 70.7% | Valid: time=  0.4s loss=0.677, TAw acc= 77.0% |
| Epoch  14, lr=8.8e-03 time=  1.9s/  2.0s | Train: loss=0.902, TAw acc= 70.6% | Valid: time=  0.4s loss=0.684, TAw acc= 76.8% |
| Epoch  15, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.894, TAw acc= 70.9% | Valid: time=  0.4s loss=0.686, TAw acc= 76.8% |
| Epoch  16, lr=8.8e-03 time=  1.7s/  3.2s | Train: loss=0.893, TAw acc= 71.2% | Valid: time=  0.5s loss=0.686, TAw acc= 77.4% |
| Epoch  17, lr=8.8e-03 time=  1.7s/  2.2s | Train: loss=0.909, TAw acc= 70.5% | Valid: time=  0.5s loss=0.685, TAw acc= 77.6% |
| Epoch  18, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=0.905, TAw acc= 70.9% | Valid: time=  0.5s loss=0.685, TAw acc= 77.0% |
| Epoch  19, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=0.877, TAw acc= 71.0% | Valid: time=  0.5s loss=0.679, TAw acc= 77.2% |
| Epoch  20, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=0.893, TAw acc= 71.2% | Valid: time=  0.5s loss=0.692, TAw acc= 76.8% |
| Epoch  21, lr=8.8e-03 time=  2.2s/  1.9s | Train: loss=0.893, TAw acc= 70.0% | Valid: time=  0.4s loss=0.686, TAw acc= 77.2% |
| Epoch  22, lr=8.8e-03 time=  2.2s/  1.9s | Train: loss=0.891, TAw acc= 70.8% | Valid: time=  0.4s loss=0.691, TAw acc= 77.0% |
| Epoch  23, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.900, TAw acc= 70.7% | Valid: time=  0.7s loss=0.688, TAw acc= 77.4% |
| Epoch  24, lr=8.8e-03 time=  1.7s/  2.1s | Train: loss=0.891, TAw acc= 70.0% | Valid: time=  0.4s loss=0.689, TAw acc= 77.4% |
| Epoch  25, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.879, TAw acc= 71.2% | Valid: time=  0.4s loss=0.673, TAw acc= 77.8% |
| Epoch  26, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.882, TAw acc= 70.4% | Valid: time=  0.4s loss=0.687, TAw acc= 77.0% |
| Epoch  27, lr=8.8e-03 time=  1.7s/  3.2s | Train: loss=0.897, TAw acc= 70.9% | Valid: time=  0.5s loss=0.685, TAw acc= 77.4% |
| Epoch  28, lr=8.8e-03 time=  1.7s/  2.5s | Train: loss=0.875, TAw acc= 71.3% | Valid: time=  0.5s loss=0.681, TAw acc= 77.8% |
| Epoch  29, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=0.865, TAw acc= 71.5% | Valid: time=  0.5s loss=0.683, TAw acc= 77.0% |
| Epoch  30, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=0.880, TAw acc= 70.7% | Valid: time=  0.5s loss=0.700, TAw acc= 76.2% | lr=2.9e-03
| Epoch  31, lr=2.9e-03 time=  2.2s/  2.3s | Train: loss=0.929, TAw acc= 69.6% | Valid: time=  0.4s loss=0.649, TAw acc= 78.4% |
| Epoch  32, lr=2.9e-03 time=  1.7s/  2.1s | Train: loss=0.943, TAw acc= 69.4% | Valid: time=  0.4s loss=0.671, TAw acc= 77.6% |
| Epoch  33, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=0.931, TAw acc= 69.7% | Valid: time=  0.4s loss=0.681, TAw acc= 77.2% |
| Epoch  34, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=0.912, TAw acc= 70.7% | Valid: time=  0.5s loss=0.678, TAw acc= 77.6% |
| Epoch  35, lr=2.9e-03 time=  3.0s/  1.9s | Train: loss=0.949, TAw acc= 68.6% | Valid: time=  0.4s loss=0.683, TAw acc= 77.4% |
| Epoch  36, lr=2.9e-03 time=  2.1s/  2.4s | Train: loss=0.932, TAw acc= 70.4% | Valid: time=  0.5s loss=0.680, TAw acc= 77.0% |
| Epoch  37, lr=2.9e-03 time=  2.2s/  2.5s | Train: loss=0.923, TAw acc= 70.7% | Valid: time=  0.5s loss=0.675, TAw acc= 77.6% |
| Epoch  38, lr=2.9e-03 time=  1.8s/  1.9s | Train: loss=0.924, TAw acc= 70.2% | Valid: time=  0.4s loss=0.677, TAw acc= 77.6% |
| Epoch  39, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=0.935, TAw acc= 69.2% | Valid: time=  0.5s loss=0.682, TAw acc= 77.0% |
| Epoch  40, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=0.937, TAw acc= 69.4% | Valid: time=  0.4s loss=0.680, TAw acc= 76.8% |
| Epoch  41, lr=2.9e-03 time=  1.7s/  2.9s | Train: loss=0.908, TAw acc= 70.6% | Valid: time=  0.4s loss=0.675, TAw acc= 77.2% |
| Epoch  42, lr=2.9e-03 time=  1.7s/  2.4s | Train: loss=0.921, TAw acc= 69.6% | Valid: time=  0.5s loss=0.674, TAw acc= 77.8% |
| Epoch  43, lr=2.9e-03 time=  2.2s/  2.5s | Train: loss=0.912, TAw acc= 70.5% | Valid: time=  0.5s loss=0.672, TAw acc= 77.4% |
| Epoch  44, lr=2.9e-03 time=  2.2s/  2.4s | Train: loss=0.898, TAw acc= 70.7% | Valid: time=  0.5s loss=0.674, TAw acc= 77.2% |
| Epoch  45, lr=2.9e-03 time=  2.1s/  1.9s | Train: loss=0.920, TAw acc= 69.4% | Valid: time=  0.4s loss=0.676, TAw acc= 77.2% |
| Epoch  46, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=0.908, TAw acc= 70.3% | Valid: time=  0.4s loss=0.673, TAw acc= 77.8% |
| Epoch  47, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=0.895, TAw acc= 70.9% | Valid: time=  0.5s loss=0.676, TAw acc= 77.2% |
| Epoch  48, lr=2.9e-03 time=  3.0s/  1.9s | Train: loss=0.920, TAw acc= 69.8% | Valid: time=  0.4s loss=0.668, TAw acc= 77.6% |
| Epoch  49, lr=2.9e-03 time=  1.9s/  2.5s | Train: loss=0.913, TAw acc= 70.1% | Valid: time=  0.5s loss=0.679, TAw acc= 76.8% |
| Epoch  50, lr=2.9e-03 time=  2.2s/  2.5s | Train: loss=0.910, TAw acc= 70.9% | Valid: time=  0.5s loss=0.678, TAw acc= 77.2% |
| Epoch  51, lr=2.9e-03 time=  2.2s/  2.3s | Train: loss=0.904, TAw acc= 70.8% | Valid: time=  0.4s loss=0.674, TAw acc= 77.4% |
| Epoch  52, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=0.930, TAw acc= 69.7% | Valid: time=  0.5s loss=0.674, TAw acc= 77.2% |
| Epoch  53, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=0.917, TAw acc= 69.8% | Valid: time=  0.9s loss=0.686, TAw acc= 76.4% |
| Epoch  54, lr=2.9e-03 time=  1.9s/  1.9s | Train: loss=0.900, TAw acc= 70.2% | Valid: time=  0.5s loss=0.677, TAw acc= 77.0% |
| Epoch  55, lr=2.9e-03 time=  2.2s/  2.5s | Train: loss=0.918, TAw acc= 69.8% | Valid: time=  0.6s loss=0.677, TAw acc= 76.8% |
| Epoch  56, lr=2.9e-03 time=  2.2s/  2.4s | Train: loss=0.890, TAw acc= 71.4% | Valid: time=  0.5s loss=0.675, TAw acc= 77.0% |
| Epoch  57, lr=2.9e-03 time=  1.9s/  1.9s | Train: loss=0.895, TAw acc= 71.3% | Valid: time=  0.4s loss=0.680, TAw acc= 77.2% |
| Epoch  58, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=0.915, TAw acc= 70.2% | Valid: time=  0.5s loss=0.674, TAw acc= 77.4% |
| Epoch  59, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=0.902, TAw acc= 70.9% | Valid: time=  0.9s loss=0.674, TAw acc= 77.2% |
| Epoch  60, lr=2.9e-03 time=  2.0s/  1.9s | Train: loss=0.918, TAw acc= 70.7% | Valid: time=  0.5s loss=0.680, TAw acc= 76.8% | lr=9.7e-04
Debug-2: loss=0.590, acc=0.806
sow: rank=768, freezed_rank=928
 a:        ['-6.27e+00', '+4.95e+00'] .... ['-1.80e+308', '-1.80e+308'] .... ['-1.80e+308', '-1.80e+308']
 a_backup: ['-6.27e+00', '+4.95e+00'] .... ['+4.97e+00', '+4.38e+00'] .... ['+1.29e+00', '-2.74e+00']
Free_Classifier: Done (training=True, fix_classifier=False)
Debug-3: loss=0.600 acc=0.802
sow: rank=1024, freezed_rank=928
 a:        ['-6.27e+00', '+4.95e+00'] .... ['+4.97e+00', '+4.38e+00'] .... ['+1.29e+00', '-2.74e+00']
 a_backup: ['-6.27e+00', '+4.95e+00'] .... ['+4.97e+00', '+4.38e+00'] .... ['+1.29e+00', '-2.74e+00']
Header Training: Finished.
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.592 | TAw acc= 80.3%, forg=  0.0%| TAg acc= 22.4%, forg= 57.9% <<<
>>> Test on task  1 : loss=0.953 | TAw acc= 67.2%, forg=  0.0%| TAg acc= 33.2%, forg= 20.3% <<<
>>> Test on task  2 : loss=0.628 | TAw acc= 79.9%, forg=  0.0%| TAg acc= 39.6%, forg= 17.8% <<<
>>> Test on task  3 : loss=0.612 | TAw acc= 79.6%, forg=  0.0%| TAg acc= 31.8%, forg= 11.9% <<<
>>> Test on task  4 : loss=0.516 | TAw acc= 83.5%, forg=  0.0%| TAg acc= 27.1%, forg=  7.6% <<<
>>> Test on task  5 : loss=0.808 | TAw acc= 69.6%, forg=  0.0%| TAg acc= 30.1%, forg=  7.1% <<<
>>> Test on task  6 : loss=0.592 | TAw acc= 78.6%, forg=  0.0%| TAg acc= 45.2%, forg=  0.0% <<<
Save at ../RESULT_AAAI2025/CR10/0/cifar100_sow
************************************************************************************************************
Task  7
************************************************************************************************************
LLL_Net(
  (model): ResNet50_32SOW(
    in_channels=3, in_H=32, in_W=32, training=False, fix_features=True, fix_classifier=False
    (features): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
      (3): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (4): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (3): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (5): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (3): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (4): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (5): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (6): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (7): AdaptiveAvgPool2d(output_size=(1, 1))
    )
    (classifier): Sequential(
      (0): Dropout(p=0.417598542370663, inplace=False)
      (1): SOW_V3(in_features=1024, out_features=1024, num_tasks=10, dtype=torch.float64, device=cuda:0)
      (2): ReLU()
    )
    (fc): Sequential()
  )
  (heads): ModuleList(
    (0-7): 8 x Linear(in_features=1024, out_features=10, bias=True)
  )
)
SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.0263039750973134
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0

Parameter Group 1
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.0263039750973134
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0
)
 Group[0]  param_num=161, free_param_num=2
 Group[1]  param_num=3, free_param_num=3
sow: rank=1024, freezed_rank=928
 a:        ['-6.27e+00', '+4.95e+00'] .... ['+4.97e+00', '+4.38e+00'] .... ['+1.29e+00', '-2.74e+00']
 a_backup: ['-6.27e+00', '+4.95e+00'] .... ['+4.97e+00', '+4.38e+00'] .... ['+1.29e+00', '-2.74e+00']
| Epoch   1, lr=2.6e-02 time=  3.7s/  2.0s | Train: loss=1.148, TAw acc= 60.2% | Valid: time=  0.5s loss=0.851, TAw acc= 71.2% | *
| Epoch   2, lr=2.6e-02 time=  2.8s/  2.0s | Train: loss=1.066, TAw acc= 62.6% | Valid: time=  0.5s loss=0.795, TAw acc= 73.2% | *
| Epoch   3, lr=2.6e-02 time=  4.1s/  1.9s | Train: loss=1.065, TAw acc= 64.5% | Valid: time=  0.4s loss=0.772, TAw acc= 72.8% | *
| Epoch   4, lr=2.6e-02 time=  3.4s/  2.5s | Train: loss=1.040, TAw acc= 64.2% | Valid: time=  0.6s loss=0.772, TAw acc= 72.8% | *
| Epoch   5, lr=2.6e-02 time=  3.7s/  2.4s | Train: loss=1.046, TAw acc= 63.6% | Valid: time=  0.4s loss=0.767, TAw acc= 74.2% | *
| Epoch   6, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=0.993, TAw acc= 66.4% | Valid: time=  0.4s loss=0.745, TAw acc= 75.2% | *
| Epoch   7, lr=2.6e-02 time=  4.1s/  2.0s | Train: loss=0.997, TAw acc= 65.6% | Valid: time=  0.5s loss=0.770, TAw acc= 73.4% |
| Epoch   8, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.026, TAw acc= 64.5% | Valid: time=  0.5s loss=0.771, TAw acc= 74.4% |
| Epoch   9, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=0.995, TAw acc= 66.5% | Valid: time=  0.5s loss=0.747, TAw acc= 75.0% |
| Epoch  10, lr=2.6e-02 time=  2.8s/  2.0s | Train: loss=1.012, TAw acc= 65.3% | Valid: time=  0.4s loss=0.729, TAw acc= 75.2% | *
| Epoch  11, lr=2.6e-02 time=  4.1s/  2.0s | Train: loss=0.988, TAw acc= 67.1% | Valid: time=  0.5s loss=0.737, TAw acc= 74.8% |
| Epoch  12, lr=2.6e-02 time=  3.1s/  2.0s | Train: loss=0.980, TAw acc= 67.0% | Valid: time=  0.4s loss=0.732, TAw acc= 73.8% |
| Epoch  13, lr=2.6e-02 time=  3.2s/  2.0s | Train: loss=1.027, TAw acc= 65.4% | Valid: time=  0.4s loss=0.728, TAw acc= 72.6% | *
| Epoch  14, lr=2.6e-02 time=  2.8s/  2.0s | Train: loss=0.975, TAw acc= 66.7% | Valid: time=  0.4s loss=0.699, TAw acc= 74.8% | *
| Epoch  15, lr=2.6e-02 time=  4.1s/  1.9s | Train: loss=0.976, TAw acc= 66.8% | Valid: time=  0.4s loss=0.727, TAw acc= 76.2% |
| Epoch  16, lr=2.6e-02 time=  3.4s/  2.5s | Train: loss=1.001, TAw acc= 64.8% | Valid: time=  0.5s loss=0.738, TAw acc= 74.8% |
| Epoch  17, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.979, TAw acc= 66.4% | Valid: time=  0.5s loss=0.703, TAw acc= 75.2% |
| Epoch  18, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.965, TAw acc= 67.1% | Valid: time=  0.5s loss=0.738, TAw acc= 74.2% |
| Epoch  19, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.955, TAw acc= 66.8% | Valid: time=  0.5s loss=0.726, TAw acc= 75.0% |
| Epoch  20, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.969, TAw acc= 66.1% | Valid: time=  0.6s loss=0.711, TAw acc= 75.0% |
| Epoch  21, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.963, TAw acc= 66.3% | Valid: time=  0.6s loss=0.740, TAw acc= 74.4% |
| Epoch  22, lr=2.6e-02 time=  3.7s/  2.5s | Train: loss=0.970, TAw acc= 67.1% | Valid: time=  0.5s loss=0.724, TAw acc= 74.6% |
| Epoch  23, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.947, TAw acc= 67.3% | Valid: time=  0.6s loss=0.714, TAw acc= 75.6% |
| Epoch  24, lr=2.6e-02 time=  3.7s/  2.5s | Train: loss=0.952, TAw acc= 67.8% | Valid: time=  0.5s loss=0.739, TAw acc= 73.8% |
| Epoch  25, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.969, TAw acc= 66.6% | Valid: time=  0.5s loss=0.717, TAw acc= 74.0% |
| Epoch  26, lr=2.6e-02 time=  3.7s/  2.5s | Train: loss=0.986, TAw acc= 66.2% | Valid: time=  0.5s loss=0.736, TAw acc= 73.8% |
| Epoch  27, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.958, TAw acc= 67.2% | Valid: time=  0.5s loss=0.755, TAw acc= 73.4% |
| Epoch  28, lr=2.6e-02 time=  3.7s/  2.5s | Train: loss=0.945, TAw acc= 67.5% | Valid: time=  0.5s loss=0.753, TAw acc= 73.6% |
| Epoch  29, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.945, TAw acc= 67.6% | Valid: time=  0.5s loss=0.741, TAw acc= 73.4% |
| Epoch  30, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.937, TAw acc= 67.7% | Valid: time=  0.5s loss=0.715, TAw acc= 74.6% |
| Epoch  31, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.961, TAw acc= 67.4% | Valid: time=  0.5s loss=0.699, TAw acc= 75.0% |
| Epoch  32, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.959, TAw acc= 67.2% | Valid: time=  0.5s loss=0.717, TAw acc= 74.8% |
| Epoch  33, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.955, TAw acc= 67.2% | Valid: time=  0.5s loss=0.718, TAw acc= 74.4% |
| Epoch  34, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.951, TAw acc= 67.5% | Valid: time=  0.5s loss=0.712, TAw acc= 72.6% |
| Epoch  35, lr=2.6e-02 time=  3.7s/  2.5s | Train: loss=0.949, TAw acc= 67.9% | Valid: time=  0.5s loss=0.699, TAw acc= 74.2% |
| Epoch  36, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.969, TAw acc= 67.2% | Valid: time=  0.5s loss=0.732, TAw acc= 73.6% |
| Epoch  37, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.951, TAw acc= 67.4% | Valid: time=  0.5s loss=0.697, TAw acc= 75.0% | *
| Epoch  38, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.969, TAw acc= 67.3% | Valid: time=  0.5s loss=0.697, TAw acc= 75.4% |
| Epoch  39, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.974, TAw acc= 66.6% | Valid: time=  0.5s loss=0.705, TAw acc= 74.2% |
| Epoch  40, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.947, TAw acc= 67.9% | Valid: time=  0.5s loss=0.716, TAw acc= 73.4% |
| Epoch  41, lr=2.6e-02 time=  3.9s/  2.5s | Train: loss=0.946, TAw acc= 67.6% | Valid: time=  0.5s loss=0.737, TAw acc= 75.8% |
| Epoch  42, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.960, TAw acc= 67.9% | Valid: time=  0.5s loss=0.680, TAw acc= 76.4% | *
| Epoch  43, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.984, TAw acc= 66.2% | Valid: time=  0.5s loss=0.696, TAw acc= 75.0% |
| Epoch  44, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.951, TAw acc= 67.4% | Valid: time=  0.5s loss=0.689, TAw acc= 76.4% |
| Epoch  45, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.983, TAw acc= 66.4% | Valid: time=  0.5s loss=0.724, TAw acc= 75.0% |
| Epoch  46, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.909, TAw acc= 68.8% | Valid: time=  0.5s loss=0.721, TAw acc= 74.2% |
| Epoch  47, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.946, TAw acc= 67.4% | Valid: time=  0.5s loss=0.711, TAw acc= 75.4% |
| Epoch  48, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.965, TAw acc= 67.8% | Valid: time=  0.5s loss=0.705, TAw acc= 75.4% |
| Epoch  49, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.958, TAw acc= 67.2% | Valid: time=  0.5s loss=0.722, TAw acc= 75.2% |
| Epoch  50, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.918, TAw acc= 69.5% | Valid: time=  0.5s loss=0.714, TAw acc= 76.0% |
| Epoch  51, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.957, TAw acc= 67.4% | Valid: time=  0.5s loss=0.693, TAw acc= 75.6% |
| Epoch  52, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.927, TAw acc= 68.1% | Valid: time=  0.5s loss=0.718, TAw acc= 75.0% |
| Epoch  53, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.953, TAw acc= 68.2% | Valid: time=  0.5s loss=0.700, TAw acc= 74.8% |
| Epoch  54, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.915, TAw acc= 69.1% | Valid: time=  0.5s loss=0.705, TAw acc= 75.6% |
| Epoch  55, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.931, TAw acc= 67.9% | Valid: time=  0.6s loss=0.715, TAw acc= 74.4% |
| Epoch  56, lr=2.6e-02 time=  3.7s/  2.5s | Train: loss=0.949, TAw acc= 67.6% | Valid: time=  0.5s loss=0.724, TAw acc= 75.6% |
| Epoch  57, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.907, TAw acc= 68.6% | Valid: time=  0.5s loss=0.708, TAw acc= 76.2% |
| Epoch  58, lr=2.6e-02 time=  3.7s/  2.5s | Train: loss=0.932, TAw acc= 67.5% | Valid: time=  0.5s loss=0.708, TAw acc= 76.0% |
| Epoch  59, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.947, TAw acc= 68.1% | Valid: time=  0.5s loss=0.732, TAw acc= 74.2% |
| Epoch  60, lr=2.6e-02 time=  3.7s/  2.5s | Train: loss=0.919, TAw acc= 68.8% | Valid: time=  0.5s loss=0.692, TAw acc= 75.8% |
== Rank Reduction [task:7] ==
Debug-0:
  best_loss=0.680,   best_acc=0.764
loss_margin=0.006, acc_margin=0.004
sow: rank=1024, freezed_rank=928
 a:        ['-6.20e+00', '+4.95e+00'] .... ['+4.97e+00', '+4.38e+00'] .... ['+1.29e+00', '-2.74e+00']
 a_backup: ['-6.20e+00', '+4.95e+00'] .... ['+4.97e+00', '+4.38e+00'] .... ['+1.29e+00', '-2.74e+00']
 r=256, loss=1.920, acc=0.304 (rank=256)
 r=288, loss=1.737, acc=0.402 (rank=288)
 r=320, loss=1.471, acc=0.480 (rank=320)
 r=352, loss=1.345, acc=0.476 (rank=352)
 r=384, loss=1.198, acc=0.546 (rank=384)
 r=416, loss=1.112, acc=0.580 (rank=416)
 r=448, loss=0.896, acc=0.670 (rank=448)
 r=480, loss=0.814, acc=0.698 (rank=480)
 r=512, loss=0.695, acc=0.762 (rank=512)
 r=544, loss=0.697, acc=0.768 (rank=544)
 r=576, loss=0.699, acc=0.762 (rank=576)
 r=608, loss=0.688, acc=0.744 (rank=608)
 r=640, loss=0.701, acc=0.740 (rank=640)
 r=672, loss=0.700, acc=0.734 (rank=672)
 r=704, loss=0.701, acc=0.756 (rank=704)
 r=736, loss=0.691, acc=0.766 (rank=736)
 r=768, loss=0.683, acc=0.762 (rank=768)
 best_r=768, loss=0.683, acc=0.762
== Header Training for Low Rank [task:7] ==
Fix_Classifier: Done (training=False, fix_classifier=True)
Debug-1: loss=0.683, acc=0.762
sow: rank=768, freezed_rank=928
 a:        ['-6.20e+00', '+4.95e+00'] .... ['-1.80e+308', '-1.80e+308'] .... ['-1.80e+308', '-1.80e+308']
 a_backup: ['-6.20e+00', '+4.95e+00'] .... ['+4.97e+00', '+4.38e+00'] .... ['+1.29e+00', '-2.74e+00']
SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.008767991699104466
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0

Parameter Group 1
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.0263039750973134
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0
)
 Group[0]  param_num=161, free_param_num=2
 Group[1]  param_num=3, free_param_num=0
sow: rank=768, freezed_rank=928
 a:        ['-6.20e+00', '+4.95e+00'] .... ['-1.80e+308', '-1.80e+308'] .... ['-1.80e+308', '-1.80e+308']
 a_backup: ['-6.20e+00', '+4.95e+00'] .... ['+4.97e+00', '+4.38e+00'] .... ['+1.29e+00', '-2.74e+00']
| Epoch   1, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=0.936, TAw acc= 68.8% | Valid: time=  0.5s loss=0.702, TAw acc= 74.8% |
| Epoch   2, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=0.916, TAw acc= 70.1% | Valid: time=  0.5s loss=0.696, TAw acc= 75.2% |
| Epoch   3, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=0.927, TAw acc= 68.5% | Valid: time=  0.6s loss=0.694, TAw acc= 74.0% |
| Epoch   4, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=0.927, TAw acc= 68.6% | Valid: time=  0.5s loss=0.707, TAw acc= 75.4% |
| Epoch   5, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=0.920, TAw acc= 68.8% | Valid: time=  0.6s loss=0.709, TAw acc= 75.6% |
| Epoch   6, lr=8.8e-03 time=  2.3s/  2.5s | Train: loss=0.920, TAw acc= 69.1% | Valid: time=  0.5s loss=0.708, TAw acc= 74.2% |
| Epoch   7, lr=8.8e-03 time=  2.3s/  2.4s | Train: loss=0.930, TAw acc= 68.3% | Valid: time=  0.5s loss=0.707, TAw acc= 74.8% |
| Epoch   8, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=0.914, TAw acc= 69.6% | Valid: time=  0.5s loss=0.707, TAw acc= 75.6% |
| Epoch   9, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=0.931, TAw acc= 68.0% | Valid: time=  0.5s loss=0.707, TAw acc= 75.6% |
| Epoch  10, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=0.916, TAw acc= 68.8% | Valid: time=  0.5s loss=0.706, TAw acc= 74.4% |
| Epoch  11, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=0.924, TAw acc= 68.2% | Valid: time=  0.5s loss=0.716, TAw acc= 75.6% |
| Epoch  12, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=0.924, TAw acc= 68.6% | Valid: time=  0.5s loss=0.705, TAw acc= 75.0% |
| Epoch  13, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=0.921, TAw acc= 68.9% | Valid: time=  0.5s loss=0.713, TAw acc= 75.6% |
| Epoch  14, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=0.892, TAw acc= 70.5% | Valid: time=  0.5s loss=0.708, TAw acc= 75.8% |
| Epoch  15, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=0.910, TAw acc= 69.5% | Valid: time=  0.5s loss=0.721, TAw acc= 75.6% |
| Epoch  16, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=0.913, TAw acc= 68.1% | Valid: time=  0.5s loss=0.708, TAw acc= 76.0% |
| Epoch  17, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=0.896, TAw acc= 69.7% | Valid: time=  0.5s loss=0.713, TAw acc= 76.4% |
| Epoch  18, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=0.900, TAw acc= 69.0% | Valid: time=  0.5s loss=0.719, TAw acc= 76.2% |
| Epoch  19, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=0.879, TAw acc= 70.2% | Valid: time=  0.5s loss=0.700, TAw acc= 75.4% |
| Epoch  20, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=0.893, TAw acc= 69.6% | Valid: time=  0.5s loss=0.723, TAw acc= 75.8% |
| Epoch  21, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=0.894, TAw acc= 69.1% | Valid: time=  0.5s loss=0.712, TAw acc= 76.4% |
| Epoch  22, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=0.889, TAw acc= 69.3% | Valid: time=  0.5s loss=0.710, TAw acc= 75.6% |
| Epoch  23, lr=8.8e-03 time=  2.3s/  2.5s | Train: loss=0.893, TAw acc= 69.9% | Valid: time=  0.5s loss=0.715, TAw acc= 75.4% |
| Epoch  24, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=0.877, TAw acc= 70.5% | Valid: time=  0.5s loss=0.718, TAw acc= 74.8% |
| Epoch  25, lr=8.8e-03 time=  2.2s/  2.2s | Train: loss=0.910, TAw acc= 69.7% | Valid: time=  0.4s loss=0.709, TAw acc= 76.6% |
| Epoch  26, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.881, TAw acc= 70.0% | Valid: time=  0.4s loss=0.712, TAw acc= 76.0% |
| Epoch  27, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.874, TAw acc= 70.5% | Valid: time=  0.4s loss=0.715, TAw acc= 75.8% |
| Epoch  28, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.875, TAw acc= 70.3% | Valid: time=  0.5s loss=0.715, TAw acc= 75.2% |
| Epoch  29, lr=8.8e-03 time=  3.0s/  1.9s | Train: loss=0.880, TAw acc= 70.5% | Valid: time=  0.4s loss=0.710, TAw acc= 76.0% |
| Epoch  30, lr=8.8e-03 time=  2.0s/  2.4s | Train: loss=0.884, TAw acc= 69.2% | Valid: time=  0.5s loss=0.709, TAw acc= 76.0% | lr=2.9e-03
| Epoch  31, lr=2.9e-03 time=  2.2s/  2.5s | Train: loss=0.931, TAw acc= 68.8% | Valid: time=  0.5s loss=0.687, TAw acc= 75.4% |
| Epoch  32, lr=2.9e-03 time=  2.2s/  2.5s | Train: loss=0.947, TAw acc= 67.8% | Valid: time=  0.5s loss=0.697, TAw acc= 74.8% |
| Epoch  33, lr=2.9e-03 time=  2.2s/  2.5s | Train: loss=0.940, TAw acc= 68.8% | Valid: time=  0.5s loss=0.696, TAw acc= 75.6% |
| Epoch  34, lr=2.9e-03 time=  1.9s/  1.9s | Train: loss=0.940, TAw acc= 68.0% | Valid: time=  0.4s loss=0.695, TAw acc= 74.8% |
| Epoch  35, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=0.928, TAw acc= 68.1% | Valid: time=  0.4s loss=0.696, TAw acc= 76.2% |
| Epoch  36, lr=2.9e-03 time=  3.0s/  1.9s | Train: loss=0.916, TAw acc= 69.2% | Valid: time=  0.5s loss=0.700, TAw acc= 76.0% |
| Epoch  37, lr=2.9e-03 time=  2.0s/  2.5s | Train: loss=0.929, TAw acc= 69.0% | Valid: time=  0.5s loss=0.699, TAw acc= 75.6% |
| Epoch  38, lr=2.9e-03 time=  2.2s/  2.5s | Train: loss=0.939, TAw acc= 67.7% | Valid: time=  0.5s loss=0.701, TAw acc= 75.8% |
| Epoch  39, lr=2.9e-03 time=  2.2s/  2.1s | Train: loss=0.909, TAw acc= 69.8% | Valid: time=  0.5s loss=0.700, TAw acc= 75.8% |
| Epoch  40, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=0.909, TAw acc= 69.9% | Valid: time=  0.4s loss=0.701, TAw acc= 75.6% |
| Epoch  41, lr=2.9e-03 time=  3.0s/  1.9s | Train: loss=0.931, TAw acc= 69.0% | Valid: time=  0.5s loss=0.700, TAw acc= 76.0% |
| Epoch  42, lr=2.9e-03 time=  2.0s/  2.5s | Train: loss=0.928, TAw acc= 68.4% | Valid: time=  0.5s loss=0.704, TAw acc= 75.8% |
| Epoch  43, lr=2.9e-03 time=  2.2s/  2.4s | Train: loss=0.928, TAw acc= 68.5% | Valid: time=  0.5s loss=0.702, TAw acc= 76.0% |
| Epoch  44, lr=2.9e-03 time=  2.2s/  2.5s | Train: loss=0.921, TAw acc= 69.4% | Valid: time=  0.5s loss=0.701, TAw acc= 74.4% |
| Epoch  45, lr=2.9e-03 time=  1.8s/  1.9s | Train: loss=0.913, TAw acc= 69.5% | Valid: time=  0.4s loss=0.701, TAw acc= 75.4% |
| Epoch  46, lr=2.9e-03 time=  1.7s/  2.4s | Train: loss=0.920, TAw acc= 69.2% | Valid: time=  0.8s loss=0.703, TAw acc= 75.6% |
| Epoch  47, lr=2.9e-03 time=  1.7s/  2.0s | Train: loss=0.921, TAw acc= 69.2% | Valid: time=  0.4s loss=0.701, TAw acc= 75.6% |
| Epoch  48, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=0.909, TAw acc= 70.1% | Valid: time=  0.7s loss=0.701, TAw acc= 75.0% |
| Epoch  49, lr=2.9e-03 time=  1.8s/  2.1s | Train: loss=0.906, TAw acc= 70.1% | Valid: time=  0.4s loss=0.703, TAw acc= 74.8% |
| Epoch  50, lr=2.9e-03 time=  1.7s/  2.2s | Train: loss=0.911, TAw acc= 69.0% | Valid: time=  0.5s loss=0.706, TAw acc= 75.0% |
| Epoch  51, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=0.905, TAw acc= 69.3% | Valid: time=  0.5s loss=0.706, TAw acc= 75.8% |
| Epoch  52, lr=2.9e-03 time=  1.7s/  3.0s | Train: loss=0.911, TAw acc= 69.6% | Valid: time=  0.5s loss=0.708, TAw acc= 74.8% |
| Epoch  53, lr=2.9e-03 time=  1.7s/  2.5s | Train: loss=0.915, TAw acc= 69.6% | Valid: time=  0.5s loss=0.707, TAw acc= 75.4% |
| Epoch  54, lr=2.9e-03 time=  2.2s/  2.4s | Train: loss=0.923, TAw acc= 68.7% | Valid: time=  0.5s loss=0.707, TAw acc= 74.8% |
| Epoch  55, lr=2.9e-03 time=  2.1s/  2.1s | Train: loss=0.916, TAw acc= 69.7% | Valid: time=  0.4s loss=0.711, TAw acc= 74.8% |
| Epoch  56, lr=2.9e-03 time=  1.9s/  1.9s | Train: loss=0.912, TAw acc= 68.7% | Valid: time=  0.4s loss=0.712, TAw acc= 75.0% |
| Epoch  57, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=0.925, TAw acc= 68.6% | Valid: time=  0.5s loss=0.709, TAw acc= 74.8% |
| Epoch  58, lr=2.9e-03 time=  1.7s/  3.1s | Train: loss=0.901, TAw acc= 69.7% | Valid: time=  0.4s loss=0.708, TAw acc= 74.4% |
| Epoch  59, lr=2.9e-03 time=  1.7s/  2.3s | Train: loss=0.907, TAw acc= 69.3% | Valid: time=  0.5s loss=0.708, TAw acc= 74.6% |
| Epoch  60, lr=2.9e-03 time=  2.2s/  2.4s | Train: loss=0.928, TAw acc= 69.0% | Valid: time=  0.5s loss=0.710, TAw acc= 74.4% | lr=9.7e-04
Debug-2: loss=0.683, acc=0.762
sow: rank=768, freezed_rank=928
 a:        ['-6.20e+00', '+4.95e+00'] .... ['-1.80e+308', '-1.80e+308'] .... ['-1.80e+308', '-1.80e+308']
 a_backup: ['-6.20e+00', '+4.95e+00'] .... ['+4.97e+00', '+4.38e+00'] .... ['+1.29e+00', '-2.74e+00']
Free_Classifier: Done (training=True, fix_classifier=False)
Debug-3: loss=0.680 acc=0.764
sow: rank=1024, freezed_rank=928
 a:        ['-6.20e+00', '+4.95e+00'] .... ['+4.97e+00', '+4.38e+00'] .... ['+1.29e+00', '-2.74e+00']
 a_backup: ['-6.20e+00', '+4.95e+00'] .... ['+4.97e+00', '+4.38e+00'] .... ['+1.29e+00', '-2.74e+00']
Header Training: Finished.
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.592 | TAw acc= 80.3%, forg=  0.0%| TAg acc= 19.7%, forg= 60.6% <<<
>>> Test on task  1 : loss=0.953 | TAw acc= 67.2%, forg=  0.0%| TAg acc= 31.0%, forg= 22.5% <<<
>>> Test on task  2 : loss=0.628 | TAw acc= 79.9%, forg=  0.0%| TAg acc= 37.3%, forg= 20.1% <<<
>>> Test on task  3 : loss=0.612 | TAw acc= 79.6%, forg=  0.0%| TAg acc= 30.2%, forg= 13.5% <<<
>>> Test on task  4 : loss=0.516 | TAw acc= 83.5%, forg=  0.0%| TAg acc= 25.2%, forg=  9.5% <<<
>>> Test on task  5 : loss=0.808 | TAw acc= 69.6%, forg=  0.0%| TAg acc= 27.5%, forg=  9.7% <<<
>>> Test on task  6 : loss=0.592 | TAw acc= 78.6%, forg=  0.0%| TAg acc= 42.8%, forg=  2.4% <<<
>>> Test on task  7 : loss=0.666 | TAw acc= 77.9%, forg=  0.0%| TAg acc= 29.7%, forg=  0.0% <<<
Save at ../RESULT_AAAI2025/CR10/0/cifar100_sow
************************************************************************************************************
Task  8
************************************************************************************************************
LLL_Net(
  (model): ResNet50_32SOW(
    in_channels=3, in_H=32, in_W=32, training=False, fix_features=True, fix_classifier=False
    (features): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
      (3): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (4): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (3): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (5): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (3): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (4): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (5): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (6): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (7): AdaptiveAvgPool2d(output_size=(1, 1))
    )
    (classifier): Sequential(
      (0): Dropout(p=0.417598542370663, inplace=False)
      (1): SOW_V3(in_features=1024, out_features=1024, num_tasks=10, dtype=torch.float64, device=cuda:0)
      (2): ReLU()
    )
    (fc): Sequential()
  )
  (heads): ModuleList(
    (0-8): 9 x Linear(in_features=1024, out_features=10, bias=True)
  )
)
SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.0263039750973134
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0

Parameter Group 1
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.0263039750973134
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0
)
 Group[0]  param_num=161, free_param_num=2
 Group[1]  param_num=3, free_param_num=3
sow: rank=1024, freezed_rank=928
 a:        ['-6.20e+00', '+4.95e+00'] .... ['+4.97e+00', '+4.38e+00'] .... ['+1.29e+00', '-2.74e+00']
 a_backup: ['-6.20e+00', '+4.95e+00'] .... ['+4.97e+00', '+4.38e+00'] .... ['+1.29e+00', '-2.74e+00']
| Epoch   1, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=1.277, TAw acc= 56.5% | Valid: time=  0.5s loss=0.943, TAw acc= 68.6% | *
| Epoch   2, lr=2.6e-02 time=  4.1s/  2.0s | Train: loss=1.218, TAw acc= 58.2% | Valid: time=  0.6s loss=0.823, TAw acc= 72.0% | *
| Epoch   3, lr=2.6e-02 time=  3.7s/  2.5s | Train: loss=1.172, TAw acc= 60.4% | Valid: time=  0.5s loss=0.894, TAw acc= 67.8% |
| Epoch   4, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.166, TAw acc= 60.1% | Valid: time=  0.5s loss=0.820, TAw acc= 72.0% | *
| Epoch   5, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.161, TAw acc= 60.5% | Valid: time=  0.6s loss=0.775, TAw acc= 74.4% | *
| Epoch   6, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.134, TAw acc= 61.1% | Valid: time=  0.5s loss=0.844, TAw acc= 70.8% |
| Epoch   7, lr=2.6e-02 time=  3.6s/  2.4s | Train: loss=1.128, TAw acc= 62.0% | Valid: time=  0.5s loss=0.779, TAw acc= 73.8% |
| Epoch   8, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.122, TAw acc= 60.4% | Valid: time=  0.6s loss=0.796, TAw acc= 72.2% |
| Epoch   9, lr=2.6e-02 time=  3.7s/  2.5s | Train: loss=1.101, TAw acc= 62.7% | Valid: time=  0.5s loss=0.807, TAw acc= 71.8% |
| Epoch  10, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.098, TAw acc= 62.7% | Valid: time=  0.5s loss=0.772, TAw acc= 74.2% | *
| Epoch  11, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.096, TAw acc= 63.1% | Valid: time=  0.5s loss=0.777, TAw acc= 73.0% |
| Epoch  12, lr=2.6e-02 time=  3.7s/  2.5s | Train: loss=1.135, TAw acc= 61.0% | Valid: time=  0.5s loss=0.778, TAw acc= 73.8% |
| Epoch  13, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.078, TAw acc= 64.1% | Valid: time=  0.5s loss=0.802, TAw acc= 73.2% |
| Epoch  14, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.092, TAw acc= 63.4% | Valid: time=  0.5s loss=0.768, TAw acc= 73.6% | *
| Epoch  15, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.114, TAw acc= 61.9% | Valid: time=  0.5s loss=0.860, TAw acc= 70.4% |
| Epoch  16, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.094, TAw acc= 63.1% | Valid: time=  0.5s loss=0.778, TAw acc= 73.4% |
| Epoch  17, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.053, TAw acc= 64.2% | Valid: time=  0.5s loss=0.845, TAw acc= 71.0% |
| Epoch  18, lr=2.6e-02 time=  3.6s/  2.6s | Train: loss=1.096, TAw acc= 62.1% | Valid: time=  0.5s loss=0.776, TAw acc= 74.0% |
| Epoch  19, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.089, TAw acc= 62.9% | Valid: time=  0.5s loss=0.824, TAw acc= 72.8% |
| Epoch  20, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.105, TAw acc= 62.5% | Valid: time=  0.5s loss=0.879, TAw acc= 71.2% |
| Epoch  21, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.082, TAw acc= 64.4% | Valid: time=  0.5s loss=0.801, TAw acc= 71.8% |
| Epoch  22, lr=2.6e-02 time=  3.6s/  2.6s | Train: loss=1.090, TAw acc= 63.0% | Valid: time=  0.5s loss=0.827, TAw acc= 70.2% |
| Epoch  23, lr=2.6e-02 time=  3.6s/  2.6s | Train: loss=1.085, TAw acc= 63.2% | Valid: time=  0.5s loss=0.782, TAw acc= 73.0% |
| Epoch  24, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.062, TAw acc= 64.6% | Valid: time=  0.5s loss=0.803, TAw acc= 72.6% |
| Epoch  25, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.083, TAw acc= 62.9% | Valid: time=  0.5s loss=0.794, TAw acc= 71.6% |
| Epoch  26, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.053, TAw acc= 64.2% | Valid: time=  0.5s loss=0.840, TAw acc= 71.0% |
| Epoch  27, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.091, TAw acc= 62.4% | Valid: time=  0.5s loss=0.793, TAw acc= 72.8% |
| Epoch  28, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.090, TAw acc= 62.2% | Valid: time=  0.5s loss=0.821, TAw acc= 73.0% |
| Epoch  29, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.084, TAw acc= 63.6% | Valid: time=  0.5s loss=0.801, TAw acc= 73.0% |
| Epoch  30, lr=2.6e-02 time=  3.7s/  2.5s | Train: loss=1.078, TAw acc= 62.8% | Valid: time=  0.5s loss=0.750, TAw acc= 73.0% | *
| Epoch  31, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.079, TAw acc= 63.3% | Valid: time=  0.6s loss=0.763, TAw acc= 74.6% |
| Epoch  32, lr=2.6e-02 time=  3.7s/  2.5s | Train: loss=1.069, TAw acc= 64.1% | Valid: time=  0.5s loss=0.769, TAw acc= 73.6% |
| Epoch  33, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.060, TAw acc= 65.5% | Valid: time=  0.5s loss=0.811, TAw acc= 73.4% |
| Epoch  34, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.031, TAw acc= 65.6% | Valid: time=  0.5s loss=0.829, TAw acc= 73.8% |
| Epoch  35, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.077, TAw acc= 63.6% | Valid: time=  0.5s loss=0.783, TAw acc= 72.6% |
| Epoch  36, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.077, TAw acc= 64.5% | Valid: time=  0.5s loss=0.753, TAw acc= 72.4% |
| Epoch  37, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.069, TAw acc= 63.4% | Valid: time=  0.6s loss=0.800, TAw acc= 73.6% |
| Epoch  38, lr=2.6e-02 time=  3.7s/  2.5s | Train: loss=1.064, TAw acc= 63.6% | Valid: time=  0.5s loss=0.790, TAw acc= 72.8% |
| Epoch  39, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.045, TAw acc= 64.0% | Valid: time=  0.5s loss=0.787, TAw acc= 73.0% |
| Epoch  40, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.059, TAw acc= 64.0% | Valid: time=  0.6s loss=0.793, TAw acc= 71.0% |
| Epoch  41, lr=2.6e-02 time=  3.7s/  2.5s | Train: loss=1.058, TAw acc= 64.9% | Valid: time=  0.6s loss=0.825, TAw acc= 70.8% |
| Epoch  42, lr=2.6e-02 time=  3.7s/  2.5s | Train: loss=1.073, TAw acc= 64.6% | Valid: time=  0.5s loss=0.787, TAw acc= 72.6% |
| Epoch  43, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.040, TAw acc= 64.8% | Valid: time=  0.5s loss=0.803, TAw acc= 72.6% |
| Epoch  44, lr=2.6e-02 time=  3.7s/  2.5s | Train: loss=1.070, TAw acc= 63.2% | Valid: time=  0.5s loss=0.859, TAw acc= 70.2% |
| Epoch  45, lr=2.6e-02 time=  3.7s/  2.5s | Train: loss=1.020, TAw acc= 65.5% | Valid: time=  0.5s loss=0.786, TAw acc= 73.4% |
| Epoch  46, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.048, TAw acc= 65.1% | Valid: time=  0.5s loss=0.802, TAw acc= 73.8% |
| Epoch  47, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.066, TAw acc= 64.4% | Valid: time=  0.5s loss=0.816, TAw acc= 72.6% |
| Epoch  48, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.042, TAw acc= 64.9% | Valid: time=  0.5s loss=0.810, TAw acc= 72.8% |
| Epoch  49, lr=2.6e-02 time=  3.7s/  2.5s | Train: loss=1.069, TAw acc= 64.0% | Valid: time=  0.5s loss=0.779, TAw acc= 72.4% |
| Epoch  50, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.064, TAw acc= 63.7% | Valid: time=  0.5s loss=0.767, TAw acc= 72.6% |
| Epoch  51, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.045, TAw acc= 63.9% | Valid: time=  0.5s loss=0.783, TAw acc= 73.2% |
| Epoch  52, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.067, TAw acc= 64.7% | Valid: time=  0.5s loss=0.744, TAw acc= 73.0% | *
| Epoch  53, lr=2.6e-02 time=  3.7s/  2.5s | Train: loss=1.043, TAw acc= 64.7% | Valid: time=  0.5s loss=0.815, TAw acc= 72.0% |
| Epoch  54, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.049, TAw acc= 64.3% | Valid: time=  0.5s loss=0.811, TAw acc= 72.4% |
| Epoch  55, lr=2.6e-02 time=  3.7s/  2.5s | Train: loss=1.057, TAw acc= 64.4% | Valid: time=  0.5s loss=0.790, TAw acc= 73.4% |
| Epoch  56, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.037, TAw acc= 64.9% | Valid: time=  0.5s loss=0.802, TAw acc= 73.2% |
| Epoch  57, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.032, TAw acc= 64.6% | Valid: time=  0.5s loss=0.776, TAw acc= 71.8% |
| Epoch  58, lr=2.6e-02 time=  3.7s/  2.5s | Train: loss=1.029, TAw acc= 66.0% | Valid: time=  0.5s loss=0.808, TAw acc= 73.0% |
| Epoch  59, lr=2.6e-02 time=  3.7s/  2.5s | Train: loss=1.022, TAw acc= 65.9% | Valid: time=  0.5s loss=0.784, TAw acc= 72.8% |
| Epoch  60, lr=2.6e-02 time=  3.7s/  2.6s | Train: loss=1.013, TAw acc= 65.4% | Valid: time=  0.6s loss=0.798, TAw acc= 72.6% |
== Rank Reduction [task:8] ==
Debug-0:
  best_loss=0.744,   best_acc=0.730
loss_margin=0.006, acc_margin=0.004
sow: rank=1024, freezed_rank=928
 a:        ['-6.36e+00', '+4.95e+00'] .... ['+4.98e+00', '+4.39e+00'] .... ['+1.29e+00', '-2.74e+00']
 a_backup: ['-6.36e+00', '+4.95e+00'] .... ['+4.98e+00', '+4.39e+00'] .... ['+1.29e+00', '-2.74e+00']
 r=256, loss=1.898, acc=0.382 (rank=256)
 r=288, loss=1.800, acc=0.372 (rank=288)
 r=320, loss=1.678, acc=0.422 (rank=320)
 r=352, loss=1.444, acc=0.538 (rank=352)
 r=384, loss=1.367, acc=0.556 (rank=384)
 r=416, loss=1.219, acc=0.574 (rank=416)
 r=448, loss=1.040, acc=0.640 (rank=448)
 r=480, loss=0.855, acc=0.708 (rank=480)
 r=512, loss=0.742, acc=0.742 (rank=512)
 best_r=512, loss=0.742, acc=0.742
== Header Training for Low Rank [task:8] ==
Fix_Classifier: Done (training=False, fix_classifier=True)
Debug-1: loss=0.742, acc=0.742
sow: rank=512, freezed_rank=928
 a:        ['-6.36e+00', '+4.95e+00'] .... ['-1.80e+308', '-1.80e+308'] .... ['-1.80e+308', '-1.80e+308']
 a_backup: ['-6.36e+00', '+4.95e+00'] .... ['+4.98e+00', '+4.39e+00'] .... ['+1.29e+00', '-2.74e+00']
SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.008767991699104466
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0

Parameter Group 1
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.0263039750973134
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0
)
 Group[0]  param_num=161, free_param_num=2
 Group[1]  param_num=3, free_param_num=0
sow: rank=512, freezed_rank=928
 a:        ['-6.36e+00', '+4.95e+00'] .... ['-1.80e+308', '-1.80e+308'] .... ['-1.80e+308', '-1.80e+308']
 a_backup: ['-6.36e+00', '+4.95e+00'] .... ['+4.98e+00', '+4.39e+00'] .... ['+1.29e+00', '-2.74e+00']
| Epoch   1, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=1.106, TAw acc= 64.7% | Valid: time=  0.5s loss=0.799, TAw acc= 71.6% |
| Epoch   2, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=1.101, TAw acc= 65.8% | Valid: time=  0.5s loss=0.793, TAw acc= 72.0% |
| Epoch   3, lr=8.8e-03 time=  2.1s/  2.4s | Train: loss=1.121, TAw acc= 63.3% | Valid: time=  0.5s loss=0.789, TAw acc= 72.6% |
| Epoch   4, lr=8.8e-03 time=  2.1s/  2.4s | Train: loss=1.100, TAw acc= 64.8% | Valid: time=  0.5s loss=0.801, TAw acc= 71.8% |
| Epoch   5, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=1.079, TAw acc= 65.8% | Valid: time=  0.5s loss=0.805, TAw acc= 72.0% |
| Epoch   6, lr=8.8e-03 time=  2.1s/  2.4s | Train: loss=1.073, TAw acc= 64.6% | Valid: time=  0.5s loss=0.799, TAw acc= 72.0% |
| Epoch   7, lr=8.8e-03 time=  2.1s/  2.4s | Train: loss=1.074, TAw acc= 65.1% | Valid: time=  0.5s loss=0.791, TAw acc= 72.6% |
| Epoch   8, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=1.049, TAw acc= 66.2% | Valid: time=  0.5s loss=0.790, TAw acc= 72.2% |
| Epoch   9, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=1.066, TAw acc= 66.0% | Valid: time=  0.5s loss=0.787, TAw acc= 72.4% |
| Epoch  10, lr=8.8e-03 time=  1.9s/  1.9s | Train: loss=1.076, TAw acc= 64.9% | Valid: time=  0.5s loss=0.792, TAw acc= 72.6% |
| Epoch  11, lr=8.8e-03 time=  2.1s/  1.9s | Train: loss=1.062, TAw acc= 66.3% | Valid: time=  0.5s loss=0.791, TAw acc= 72.0% |
| Epoch  12, lr=8.8e-03 time=  1.6s/  1.8s | Train: loss=1.044, TAw acc= 66.3% | Valid: time=  0.4s loss=0.782, TAw acc= 72.4% |
| Epoch  13, lr=8.8e-03 time=  1.7s/  1.8s | Train: loss=1.043, TAw acc= 66.0% | Valid: time=  0.4s loss=0.799, TAw acc= 72.0% |
| Epoch  14, lr=8.8e-03 time=  1.8s/  2.7s | Train: loss=1.067, TAw acc= 64.2% | Valid: time=  0.4s loss=0.792, TAw acc= 72.2% |
| Epoch  15, lr=8.8e-03 time=  1.7s/  2.4s | Train: loss=1.048, TAw acc= 66.4% | Valid: time=  0.5s loss=0.785, TAw acc= 72.6% |
| Epoch  16, lr=8.8e-03 time=  2.1s/  2.4s | Train: loss=1.054, TAw acc= 66.2% | Valid: time=  0.5s loss=0.780, TAw acc= 72.8% |
| Epoch  17, lr=8.8e-03 time=  2.1s/  2.3s | Train: loss=1.041, TAw acc= 66.4% | Valid: time=  0.5s loss=0.781, TAw acc= 72.6% |
| Epoch  18, lr=8.8e-03 time=  1.7s/  2.5s | Train: loss=1.046, TAw acc= 65.8% | Valid: time=  0.5s loss=0.787, TAw acc= 72.2% |
| Epoch  19, lr=8.8e-03 time=  1.7s/  2.1s | Train: loss=1.038, TAw acc= 66.8% | Valid: time=  0.4s loss=0.801, TAw acc= 72.0% |
| Epoch  20, lr=8.8e-03 time=  1.6s/  1.9s | Train: loss=1.069, TAw acc= 64.9% | Valid: time=  0.4s loss=0.793, TAw acc= 72.0% |
| Epoch  21, lr=8.8e-03 time=  1.6s/  1.9s | Train: loss=1.024, TAw acc= 66.0% | Valid: time=  0.5s loss=0.796, TAw acc= 72.6% |
| Epoch  22, lr=8.8e-03 time=  2.4s/  2.1s | Train: loss=1.020, TAw acc= 66.4% | Valid: time=  0.4s loss=0.792, TAw acc= 72.0% |
| Epoch  23, lr=8.8e-03 time=  1.7s/  2.5s | Train: loss=1.027, TAw acc= 66.2% | Valid: time=  0.5s loss=0.785, TAw acc= 71.8% |
| Epoch  24, lr=8.8e-03 time=  2.1s/  2.3s | Train: loss=1.024, TAw acc= 66.8% | Valid: time=  0.5s loss=0.786, TAw acc= 72.4% |
| Epoch  25, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=1.022, TAw acc= 66.1% | Valid: time=  0.5s loss=0.794, TAw acc= 72.2% |
| Epoch  26, lr=8.8e-03 time=  1.6s/  1.8s | Train: loss=1.037, TAw acc= 66.0% | Valid: time=  0.4s loss=0.794, TAw acc= 72.6% |
| Epoch  27, lr=8.8e-03 time=  1.6s/  1.9s | Train: loss=1.030, TAw acc= 65.9% | Valid: time=  0.4s loss=0.789, TAw acc= 72.2% |
| Epoch  28, lr=8.8e-03 time=  1.7s/  2.9s | Train: loss=1.024, TAw acc= 66.0% | Valid: time=  0.5s loss=0.787, TAw acc= 72.4% |
| Epoch  29, lr=8.8e-03 time=  1.7s/  2.4s | Train: loss=1.012, TAw acc= 66.9% | Valid: time=  0.5s loss=0.785, TAw acc= 73.0% |
| Epoch  30, lr=8.8e-03 time=  2.1s/  2.4s | Train: loss=1.032, TAw acc= 66.4% | Valid: time=  0.5s loss=0.782, TAw acc= 72.0% | lr=2.9e-03
| Epoch  31, lr=2.9e-03 time=  2.1s/  2.4s | Train: loss=1.099, TAw acc= 65.0% | Valid: time=  0.5s loss=0.768, TAw acc= 72.6% |
| Epoch  32, lr=2.9e-03 time=  1.9s/  1.9s | Train: loss=1.116, TAw acc= 64.6% | Valid: time=  0.4s loss=0.788, TAw acc= 71.8% |
| Epoch  33, lr=2.9e-03 time=  1.6s/  1.8s | Train: loss=1.118, TAw acc= 64.2% | Valid: time=  0.4s loss=0.792, TAw acc= 71.2% |
| Epoch  34, lr=2.9e-03 time=  1.6s/  1.9s | Train: loss=1.096, TAw acc= 65.4% | Valid: time=  0.4s loss=0.797, TAw acc= 71.2% |
| Epoch  35, lr=2.9e-03 time=  1.6s/  3.1s | Train: loss=1.123, TAw acc= 64.4% | Valid: time=  0.4s loss=0.798, TAw acc= 71.4% |
| Epoch  36, lr=2.9e-03 time=  1.7s/  2.2s | Train: loss=1.113, TAw acc= 64.8% | Valid: time=  0.5s loss=0.794, TAw acc= 72.4% |
| Epoch  37, lr=2.9e-03 time=  2.1s/  2.4s | Train: loss=1.092, TAw acc= 64.8% | Valid: time=  0.5s loss=0.797, TAw acc= 71.0% |
| Epoch  38, lr=2.9e-03 time=  2.1s/  2.0s | Train: loss=1.105, TAw acc= 64.8% | Valid: time=  0.4s loss=0.801, TAw acc= 72.0% |
| Epoch  39, lr=2.9e-03 time=  1.6s/  1.9s | Train: loss=1.101, TAw acc= 64.9% | Valid: time=  0.4s loss=0.798, TAw acc= 71.4% |
| Epoch  40, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=1.101, TAw acc= 64.0% | Valid: time=  0.4s loss=0.794, TAw acc= 72.0% |
| Epoch  41, lr=2.9e-03 time=  2.9s/  1.9s | Train: loss=1.076, TAw acc= 65.4% | Valid: time=  0.5s loss=0.800, TAw acc= 71.8% |
| Epoch  42, lr=2.9e-03 time=  1.9s/  2.4s | Train: loss=1.097, TAw acc= 65.0% | Valid: time=  0.5s loss=0.797, TAw acc= 72.6% |
| Epoch  43, lr=2.9e-03 time=  2.1s/  2.3s | Train: loss=1.079, TAw acc= 65.5% | Valid: time=  0.4s loss=0.794, TAw acc= 72.0% |
| Epoch  44, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=1.087, TAw acc= 65.8% | Valid: time=  0.4s loss=0.798, TAw acc= 72.0% |
| Epoch  45, lr=2.9e-03 time=  1.6s/  1.9s | Train: loss=1.081, TAw acc= 65.1% | Valid: time=  0.4s loss=0.799, TAw acc= 71.6% |
| Epoch  46, lr=2.9e-03 time=  2.7s/  1.9s | Train: loss=1.076, TAw acc= 65.0% | Valid: time=  0.4s loss=0.800, TAw acc= 71.8% |
| Epoch  47, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=1.080, TAw acc= 65.1% | Valid: time=  0.4s loss=0.800, TAw acc= 71.4% |
| Epoch  48, lr=2.9e-03 time=  1.8s/  1.8s | Train: loss=1.071, TAw acc= 65.6% | Valid: time=  0.7s loss=0.797, TAw acc= 72.0% |
| Epoch  49, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=1.074, TAw acc= 64.9% | Valid: time=  0.4s loss=0.800, TAw acc= 71.8% |
| Epoch  50, lr=2.9e-03 time=  1.6s/  1.9s | Train: loss=1.071, TAw acc= 65.7% | Valid: time=  0.4s loss=0.801, TAw acc= 71.6% |
| Epoch  51, lr=2.9e-03 time=  1.6s/  1.9s | Train: loss=1.075, TAw acc= 65.2% | Valid: time=  0.5s loss=0.796, TAw acc= 71.2% |
| Epoch  52, lr=2.9e-03 time=  2.9s/  1.9s | Train: loss=1.083, TAw acc= 65.2% | Valid: time=  0.5s loss=0.796, TAw acc= 71.6% |
| Epoch  53, lr=2.9e-03 time=  2.1s/  2.4s | Train: loss=1.083, TAw acc= 64.9% | Valid: time=  0.5s loss=0.797, TAw acc= 71.8% |
| Epoch  54, lr=2.9e-03 time=  2.1s/  2.4s | Train: loss=1.081, TAw acc= 64.4% | Valid: time=  0.5s loss=0.795, TAw acc= 71.4% |
| Epoch  55, lr=2.9e-03 time=  2.1s/  2.4s | Train: loss=1.085, TAw acc= 64.4% | Valid: time=  0.5s loss=0.792, TAw acc= 72.0% |
| Epoch  56, lr=2.9e-03 time=  2.1s/  2.0s | Train: loss=1.062, TAw acc= 65.8% | Valid: time=  0.4s loss=0.793, TAw acc= 71.8% |
| Epoch  57, lr=2.9e-03 time=  1.6s/  1.8s | Train: loss=1.029, TAw acc= 66.6% | Valid: time=  0.5s loss=0.793, TAw acc= 72.0% |
| Epoch  58, lr=2.9e-03 time=  1.6s/  1.9s | Train: loss=1.079, TAw acc= 65.1% | Valid: time=  0.4s loss=0.796, TAw acc= 71.8% |
| Epoch  59, lr=2.9e-03 time=  1.7s/  2.3s | Train: loss=1.054, TAw acc= 66.2% | Valid: time=  0.9s loss=0.796, TAw acc= 72.0% |
| Epoch  60, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=1.075, TAw acc= 64.3% | Valid: time=  0.6s loss=0.794, TAw acc= 72.0% | lr=9.7e-04
Debug-2: loss=0.742, acc=0.742
sow: rank=512, freezed_rank=928
 a:        ['-6.36e+00', '+4.95e+00'] .... ['-1.80e+308', '-1.80e+308'] .... ['-1.80e+308', '-1.80e+308']
 a_backup: ['-6.36e+00', '+4.95e+00'] .... ['+4.98e+00', '+4.39e+00'] .... ['+1.29e+00', '-2.74e+00']
Free_Classifier: Done (training=True, fix_classifier=False)
Debug-3: loss=0.744 acc=0.730
sow: rank=1024, freezed_rank=928
 a:        ['-6.36e+00', '+4.95e+00'] .... ['+4.98e+00', '+4.39e+00'] .... ['+1.29e+00', '-2.74e+00']
 a_backup: ['-6.36e+00', '+4.95e+00'] .... ['+4.98e+00', '+4.39e+00'] .... ['+1.29e+00', '-2.74e+00']
Header Training: Finished.
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.592 | TAw acc= 80.3%, forg=  0.0%| TAg acc= 13.7%, forg= 66.6% <<<
>>> Test on task  1 : loss=0.953 | TAw acc= 67.2%, forg=  0.0%| TAg acc= 28.9%, forg= 24.6% <<<
>>> Test on task  2 : loss=0.628 | TAw acc= 79.9%, forg=  0.0%| TAg acc= 35.8%, forg= 21.6% <<<
>>> Test on task  3 : loss=0.612 | TAw acc= 79.6%, forg=  0.0%| TAg acc= 28.1%, forg= 15.6% <<<
>>> Test on task  4 : loss=0.516 | TAw acc= 83.5%, forg=  0.0%| TAg acc= 17.7%, forg= 17.0% <<<
>>> Test on task  5 : loss=0.808 | TAw acc= 69.6%, forg=  0.0%| TAg acc= 25.7%, forg= 11.5% <<<
>>> Test on task  6 : loss=0.592 | TAw acc= 78.6%, forg=  0.0%| TAg acc= 39.0%, forg=  6.2% <<<
>>> Test on task  7 : loss=0.666 | TAw acc= 77.9%, forg=  0.0%| TAg acc= 25.7%, forg=  4.0% <<<
>>> Test on task  8 : loss=0.717 | TAw acc= 75.6%, forg=  0.0%| TAg acc= 39.3%, forg=  0.0% <<<
Save at ../RESULT_AAAI2025/CR10/0/cifar100_sow
************************************************************************************************************
Task  9
************************************************************************************************************
LLL_Net(
  (model): ResNet50_32SOW(
    in_channels=3, in_H=32, in_W=32, training=False, fix_features=True, fix_classifier=False
    (features): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
      (3): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (4): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (3): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (5): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (3): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (4): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (5): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (6): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (7): AdaptiveAvgPool2d(output_size=(1, 1))
    )
    (classifier): Sequential(
      (0): Dropout(p=0.417598542370663, inplace=False)
      (1): SOW_V3(in_features=1024, out_features=1024, num_tasks=10, dtype=torch.float64, device=cuda:0)
      (2): ReLU()
    )
    (fc): Sequential()
  )
  (heads): ModuleList(
    (0-9): 10 x Linear(in_features=1024, out_features=10, bias=True)
  )
)
SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.0263039750973134
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0

Parameter Group 1
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.0263039750973134
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0
)
 Group[0]  param_num=161, free_param_num=2
 Group[1]  param_num=3, free_param_num=3
sow: rank=1024, freezed_rank=928
 a:        ['-6.36e+00', '+4.95e+00'] .... ['+4.98e+00', '+4.39e+00'] .... ['+1.29e+00', '-2.74e+00']
 a_backup: ['-6.36e+00', '+4.95e+00'] .... ['+4.98e+00', '+4.39e+00'] .... ['+1.29e+00', '-2.74e+00']
| Epoch   1, lr=2.6e-02 time=  3.0s/  2.5s | Train: loss=1.240, TAw acc= 57.8% | Valid: time=  0.4s loss=1.026, TAw acc= 64.0% | *
| Epoch   2, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=1.179, TAw acc= 58.9% | Valid: time=  0.5s loss=0.966, TAw acc= 68.8% | *
| Epoch   3, lr=2.6e-02 time=  2.8s/  2.0s | Train: loss=1.151, TAw acc= 61.0% | Valid: time=  0.5s loss=0.912, TAw acc= 68.8% | *
| Epoch   4, lr=2.6e-02 time=  4.1s/  1.9s | Train: loss=1.132, TAw acc= 61.7% | Valid: time=  0.5s loss=0.912, TAw acc= 69.2% |
| Epoch   5, lr=2.6e-02 time=  3.5s/  2.5s | Train: loss=1.141, TAw acc= 61.6% | Valid: time=  0.5s loss=0.884, TAw acc= 70.4% | *
| Epoch   6, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.087, TAw acc= 62.5% | Valid: time=  0.5s loss=0.903, TAw acc= 71.4% |
| Epoch   7, lr=2.6e-02 time=  3.4s/  2.0s | Train: loss=1.109, TAw acc= 62.6% | Valid: time=  0.4s loss=0.891, TAw acc= 71.6% |
| Epoch   8, lr=2.6e-02 time=  2.8s/  2.0s | Train: loss=1.121, TAw acc= 62.0% | Valid: time=  0.4s loss=0.883, TAw acc= 68.0% | *
| Epoch   9, lr=2.6e-02 time=  4.1s/  1.9s | Train: loss=1.109, TAw acc= 61.8% | Valid: time=  0.5s loss=0.905, TAw acc= 69.6% |
| Epoch  10, lr=2.6e-02 time=  3.5s/  2.5s | Train: loss=1.098, TAw acc= 62.0% | Valid: time=  0.5s loss=0.937, TAw acc= 67.2% |
| Epoch  11, lr=2.6e-02 time=  3.7s/  2.5s | Train: loss=1.078, TAw acc= 63.9% | Valid: time=  0.5s loss=0.866, TAw acc= 70.2% | *
| Epoch  12, lr=2.6e-02 time=  3.6s/  2.1s | Train: loss=1.061, TAw acc= 62.5% | Valid: time=  0.5s loss=0.890, TAw acc= 69.6% |
| Epoch  13, lr=2.6e-02 time=  2.8s/  3.2s | Train: loss=1.096, TAw acc= 62.5% | Valid: time=  0.5s loss=0.856, TAw acc= 69.4% | *
| Epoch  14, lr=2.6e-02 time=  2.8s/  2.5s | Train: loss=1.074, TAw acc= 63.6% | Valid: time=  0.5s loss=0.867, TAw acc= 69.6% |
| Epoch  15, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.054, TAw acc= 63.8% | Valid: time=  0.5s loss=0.843, TAw acc= 70.8% | *
| Epoch  16, lr=2.6e-02 time=  3.6s/  2.0s | Train: loss=1.052, TAw acc= 63.6% | Valid: time=  0.4s loss=0.838, TAw acc= 70.2% | *
| Epoch  17, lr=2.6e-02 time=  2.8s/  2.0s | Train: loss=1.089, TAw acc= 62.4% | Valid: time=  1.0s loss=0.868, TAw acc= 69.2% |
| Epoch  18, lr=2.6e-02 time=  2.9s/  2.0s | Train: loss=1.054, TAw acc= 63.9% | Valid: time=  0.5s loss=0.880, TAw acc= 68.8% |
| Epoch  19, lr=2.6e-02 time=  2.8s/  2.0s | Train: loss=1.073, TAw acc= 63.1% | Valid: time=  0.9s loss=0.850, TAw acc= 69.0% |
| Epoch  20, lr=2.6e-02 time=  3.2s/  2.3s | Train: loss=1.050, TAw acc= 63.9% | Valid: time=  0.5s loss=0.865, TAw acc= 70.0% |
| Epoch  21, lr=2.6e-02 time=  3.7s/  2.5s | Train: loss=1.055, TAw acc= 64.3% | Valid: time=  0.5s loss=0.844, TAw acc= 69.6% |
| Epoch  22, lr=2.6e-02 time=  3.6s/  1.9s | Train: loss=1.056, TAw acc= 64.0% | Valid: time=  0.5s loss=0.880, TAw acc= 69.8% |
| Epoch  23, lr=2.6e-02 time=  3.4s/  2.1s | Train: loss=1.086, TAw acc= 63.0% | Valid: time=  0.5s loss=0.861, TAw acc= 68.8% |
| Epoch  24, lr=2.6e-02 time=  2.8s/  2.0s | Train: loss=1.062, TAw acc= 63.4% | Valid: time=  0.5s loss=0.918, TAw acc= 67.6% |
| Epoch  25, lr=2.6e-02 time=  4.0s/  2.0s | Train: loss=1.055, TAw acc= 64.3% | Valid: time=  0.5s loss=0.854, TAw acc= 69.4% |
| Epoch  26, lr=2.6e-02 time=  3.4s/  2.5s | Train: loss=1.079, TAw acc= 64.6% | Valid: time=  0.5s loss=0.860, TAw acc= 70.0% |
| Epoch  27, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.062, TAw acc= 63.4% | Valid: time=  0.5s loss=0.853, TAw acc= 70.0% |
| Epoch  28, lr=2.6e-02 time=  2.9s/  1.9s | Train: loss=1.036, TAw acc= 64.4% | Valid: time=  0.5s loss=0.844, TAw acc= 71.0% |
| Epoch  29, lr=2.6e-02 time=  2.8s/  2.0s | Train: loss=1.032, TAw acc= 63.7% | Valid: time=  0.5s loss=0.864, TAw acc= 69.8% |
| Epoch  30, lr=2.6e-02 time=  4.1s/  2.0s | Train: loss=1.051, TAw acc= 64.9% | Valid: time=  0.5s loss=0.888, TAw acc= 68.2% |
| Epoch  31, lr=2.6e-02 time=  3.5s/  2.5s | Train: loss=1.038, TAw acc= 64.9% | Valid: time=  0.5s loss=0.870, TAw acc= 69.2% |
| Epoch  32, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.064, TAw acc= 64.0% | Valid: time=  0.5s loss=0.890, TAw acc= 69.0% |
| Epoch  33, lr=2.6e-02 time=  3.6s/  2.3s | Train: loss=1.047, TAw acc= 64.3% | Valid: time=  0.5s loss=0.836, TAw acc= 69.4% | *
| Epoch  34, lr=2.6e-02 time=  3.0s/  1.9s | Train: loss=1.035, TAw acc= 64.7% | Valid: time=  0.4s loss=0.863, TAw acc= 68.2% |
| Epoch  35, lr=2.6e-02 time=  2.8s/  2.0s | Train: loss=1.043, TAw acc= 64.4% | Valid: time=  0.5s loss=0.852, TAw acc= 69.8% |
| Epoch  36, lr=2.6e-02 time=  4.0s/  2.0s | Train: loss=1.066, TAw acc= 64.0% | Valid: time=  0.4s loss=0.871, TAw acc= 69.8% |
| Epoch  37, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.058, TAw acc= 63.5% | Valid: time=  0.5s loss=0.860, TAw acc= 68.6% |
| Epoch  38, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.061, TAw acc= 64.6% | Valid: time=  0.4s loss=0.830, TAw acc= 70.6% | *
| Epoch  39, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=1.065, TAw acc= 63.9% | Valid: time=  0.4s loss=0.812, TAw acc= 70.2% | *
| Epoch  40, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=1.045, TAw acc= 65.2% | Valid: time=  0.4s loss=0.801, TAw acc= 70.2% | *
| Epoch  41, lr=2.6e-02 time=  7.4s/  2.5s | Train: loss=1.023, TAw acc= 65.5% | Valid: time=  0.5s loss=0.833, TAw acc= 71.0% |
| Epoch  42, lr=2.6e-02 time=  3.8s/  2.6s | Train: loss=1.023, TAw acc= 63.7% | Valid: time=  0.6s loss=0.889, TAw acc= 70.2% |
| Epoch  43, lr=2.6e-02 time=  2.8s/  2.0s | Train: loss=1.032, TAw acc= 64.8% | Valid: time=  0.4s loss=0.854, TAw acc= 70.0% |
| Epoch  44, lr=2.6e-02 time=  2.8s/  3.1s | Train: loss=1.050, TAw acc= 64.6% | Valid: time=  0.5s loss=0.833, TAw acc= 69.4% |
| Epoch  45, lr=2.6e-02 time=  3.1s/  2.5s | Train: loss=1.037, TAw acc= 64.6% | Valid: time=  0.5s loss=0.851, TAw acc= 70.2% |
| Epoch  46, lr=2.6e-02 time=  3.7s/  2.5s | Train: loss=1.038, TAw acc= 64.8% | Valid: time=  0.5s loss=0.829, TAw acc= 70.4% |
| Epoch  47, lr=2.6e-02 time=  2.8s/  2.1s | Train: loss=1.043, TAw acc= 65.1% | Valid: time=  0.4s loss=0.826, TAw acc= 70.8% |
| Epoch  48, lr=2.6e-02 time=  2.9s/  2.0s | Train: loss=1.002, TAw acc= 65.8% | Valid: time=  0.5s loss=0.839, TAw acc= 70.4% |
| Epoch  49, lr=2.6e-02 time=  2.8s/  3.2s | Train: loss=1.020, TAw acc= 65.3% | Valid: time=  0.5s loss=0.869, TAw acc= 68.6% |
| Epoch  50, lr=2.6e-02 time=  3.0s/  2.5s | Train: loss=1.037, TAw acc= 64.7% | Valid: time=  0.5s loss=0.831, TAw acc= 69.6% |
| Epoch  51, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.029, TAw acc= 65.4% | Valid: time=  0.5s loss=0.852, TAw acc= 70.2% |
| Epoch  52, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.033, TAw acc= 65.2% | Valid: time=  0.5s loss=0.819, TAw acc= 70.8% |
| Epoch  53, lr=2.6e-02 time=  3.4s/  1.9s | Train: loss=1.040, TAw acc= 64.0% | Valid: time=  0.4s loss=0.847, TAw acc= 71.4% |
| Epoch  54, lr=2.6e-02 time=  3.6s/  2.0s | Train: loss=1.014, TAw acc= 65.6% | Valid: time=  0.4s loss=0.849, TAw acc= 70.4% |
| Epoch  55, lr=2.6e-02 time=  3.1s/  2.2s | Train: loss=1.037, TAw acc= 65.0% | Valid: time=  0.5s loss=0.793, TAw acc= 73.0% | *
| Epoch  56, lr=2.6e-02 time=  3.5s/  2.0s | Train: loss=1.048, TAw acc= 64.2% | Valid: time=  0.4s loss=0.882, TAw acc= 69.0% |
| Epoch  57, lr=2.6e-02 time=  2.8s/  2.3s | Train: loss=1.022, TAw acc= 64.7% | Valid: time=  0.5s loss=0.855, TAw acc= 70.2% |
| Epoch  58, lr=2.6e-02 time=  2.9s/  2.0s | Train: loss=1.036, TAw acc= 64.5% | Valid: time=  0.9s loss=0.820, TAw acc= 72.0% |
| Epoch  59, lr=2.6e-02 time=  2.8s/  2.0s | Train: loss=1.005, TAw acc= 65.4% | Valid: time=  0.5s loss=0.844, TAw acc= 70.8% |
| Epoch  60, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=1.063, TAw acc= 64.0% | Valid: time=  0.5s loss=0.822, TAw acc= 71.0% |
== Rank Reduction [task:9] ==
Debug-0:
  best_loss=0.793,   best_acc=0.730
loss_margin=0.006, acc_margin=0.004
sow: rank=1024, freezed_rank=928
 a:        ['-6.39e+00', '+4.95e+00'] .... ['+4.98e+00', '+4.39e+00'] .... ['+1.29e+00', '-2.74e+00']
 a_backup: ['-6.39e+00', '+4.95e+00'] .... ['+4.98e+00', '+4.39e+00'] .... ['+1.29e+00', '-2.74e+00']
 r=256, loss=1.898, acc=0.326 (rank=256)
 r=288, loss=1.725, acc=0.376 (rank=288)
 r=320, loss=1.597, acc=0.416 (rank=320)
 r=352, loss=1.469, acc=0.446 (rank=352)
 r=384, loss=1.270, acc=0.570 (rank=384)
 r=416, loss=1.175, acc=0.606 (rank=416)
 r=448, loss=0.997, acc=0.676 (rank=448)
 r=480, loss=0.869, acc=0.710 (rank=480)
 r=512, loss=0.797, acc=0.752 (rank=512)
 best_r=512, loss=0.797, acc=0.752
== Header Training for Low Rank [task:9] ==
Fix_Classifier: Done (training=False, fix_classifier=True)
Debug-1: loss=0.797, acc=0.752
sow: rank=512, freezed_rank=928
 a:        ['-6.39e+00', '+4.95e+00'] .... ['-1.80e+308', '-1.80e+308'] .... ['-1.80e+308', '-1.80e+308']
 a_backup: ['-6.39e+00', '+4.95e+00'] .... ['+4.98e+00', '+4.39e+00'] .... ['+1.29e+00', '-2.74e+00']
SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.008767991699104466
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0

Parameter Group 1
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.0263039750973134
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0
)
 Group[0]  param_num=161, free_param_num=2
 Group[1]  param_num=3, free_param_num=0
sow: rank=512, freezed_rank=928
 a:        ['-6.39e+00', '+4.95e+00'] .... ['-1.80e+308', '-1.80e+308'] .... ['-1.80e+308', '-1.80e+308']
 a_backup: ['-6.39e+00', '+4.95e+00'] .... ['+4.98e+00', '+4.39e+00'] .... ['+1.29e+00', '-2.74e+00']
| Epoch   1, lr=8.8e-03 time=  1.6s/  1.9s | Train: loss=1.107, TAw acc= 63.2% | Valid: time=  0.5s loss=0.848, TAw acc= 71.4% |
| Epoch   2, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=1.064, TAw acc= 64.6% | Valid: time=  0.5s loss=0.854, TAw acc= 71.8% |
| Epoch   3, lr=8.8e-03 time=  2.1s/  2.1s | Train: loss=1.082, TAw acc= 63.8% | Valid: time=  0.4s loss=0.848, TAw acc= 72.2% |
| Epoch   4, lr=8.8e-03 time=  1.6s/  1.9s | Train: loss=1.070, TAw acc= 64.4% | Valid: time=  0.8s loss=0.853, TAw acc= 71.4% |
| Epoch   5, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=1.063, TAw acc= 64.5% | Valid: time=  0.4s loss=0.856, TAw acc= 71.6% |
| Epoch   6, lr=8.8e-03 time=  1.9s/  2.2s | Train: loss=1.064, TAw acc= 65.0% | Valid: time=  0.4s loss=0.844, TAw acc= 71.6% |
| Epoch   7, lr=8.8e-03 time=  1.7s/  2.1s | Train: loss=1.065, TAw acc= 64.0% | Valid: time=  0.5s loss=0.841, TAw acc= 72.0% |
| Epoch   8, lr=8.8e-03 time=  1.9s/  1.9s | Train: loss=1.071, TAw acc= 64.8% | Valid: time=  0.5s loss=0.847, TAw acc= 71.2% |
| Epoch   9, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=1.060, TAw acc= 64.4% | Valid: time=  0.5s loss=0.845, TAw acc= 71.6% |
| Epoch  10, lr=8.8e-03 time=  1.6s/  3.0s | Train: loss=1.052, TAw acc= 65.1% | Valid: time=  0.4s loss=0.837, TAw acc= 71.0% |
| Epoch  11, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=1.066, TAw acc= 64.8% | Valid: time=  0.4s loss=0.835, TAw acc= 71.6% |
| Epoch  12, lr=8.8e-03 time=  1.6s/  1.9s | Train: loss=1.050, TAw acc= 65.4% | Valid: time=  0.4s loss=0.827, TAw acc= 71.4% |
| Epoch  13, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=1.046, TAw acc= 64.9% | Valid: time=  0.5s loss=0.832, TAw acc= 72.4% |
| Epoch  14, lr=8.8e-03 time=  2.9s/  1.9s | Train: loss=1.038, TAw acc= 65.2% | Valid: time=  0.4s loss=0.820, TAw acc= 71.6% |
| Epoch  15, lr=8.8e-03 time=  1.7s/  2.1s | Train: loss=1.032, TAw acc= 65.7% | Valid: time=  0.4s loss=0.842, TAw acc= 72.8% |
| Epoch  16, lr=8.8e-03 time=  1.6s/  1.9s | Train: loss=1.049, TAw acc= 64.5% | Valid: time=  0.4s loss=0.828, TAw acc= 71.2% |
| Epoch  17, lr=8.8e-03 time=  2.9s/  1.9s | Train: loss=1.033, TAw acc= 64.8% | Valid: time=  0.4s loss=0.828, TAw acc= 72.6% |
| Epoch  18, lr=8.8e-03 time=  2.0s/  2.4s | Train: loss=1.033, TAw acc= 65.7% | Valid: time=  0.5s loss=0.826, TAw acc= 73.0% |
| Epoch  19, lr=8.8e-03 time=  2.1s/  2.4s | Train: loss=1.035, TAw acc= 65.4% | Valid: time=  0.5s loss=0.840, TAw acc= 72.2% |
| Epoch  20, lr=8.8e-03 time=  2.1s/  2.4s | Train: loss=1.013, TAw acc= 65.9% | Valid: time=  0.4s loss=0.826, TAw acc= 71.8% |
| Epoch  21, lr=8.8e-03 time=  1.8s/  1.9s | Train: loss=1.041, TAw acc= 64.9% | Valid: time=  0.4s loss=0.832, TAw acc= 72.4% |
| Epoch  22, lr=8.8e-03 time=  2.4s/  1.8s | Train: loss=1.029, TAw acc= 65.4% | Valid: time=  0.5s loss=0.838, TAw acc= 72.0% |
| Epoch  23, lr=8.8e-03 time=  1.6s/  1.8s | Train: loss=1.018, TAw acc= 65.7% | Valid: time=  0.4s loss=0.828, TAw acc= 72.8% |
| Epoch  24, lr=8.8e-03 time=  1.6s/  1.8s | Train: loss=1.018, TAw acc= 66.0% | Valid: time=  0.5s loss=0.817, TAw acc= 72.0% |
| Epoch  25, lr=8.8e-03 time=  1.6s/  1.9s | Train: loss=1.034, TAw acc= 64.7% | Valid: time=  0.9s loss=0.815, TAw acc= 72.6% |
| Epoch  26, lr=8.8e-03 time=  1.8s/  1.9s | Train: loss=1.014, TAw acc= 65.6% | Valid: time=  0.5s loss=0.825, TAw acc= 72.2% |
| Epoch  27, lr=8.8e-03 time=  2.1s/  2.4s | Train: loss=1.011, TAw acc= 66.0% | Valid: time=  0.5s loss=0.818, TAw acc= 72.6% |
| Epoch  28, lr=8.8e-03 time=  2.1s/  2.4s | Train: loss=1.011, TAw acc= 65.9% | Valid: time=  0.5s loss=0.811, TAw acc= 72.2% |
| Epoch  29, lr=8.8e-03 time=  2.1s/  1.9s | Train: loss=1.010, TAw acc= 65.6% | Valid: time=  0.4s loss=0.828, TAw acc= 73.4% |
| Epoch  30, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=1.036, TAw acc= 64.5% | Valid: time=  0.5s loss=0.824, TAw acc= 72.8% | lr=2.9e-03
| Epoch  31, lr=2.9e-03 time=  1.7s/  1.8s | Train: loss=1.114, TAw acc= 63.2% | Valid: time=  0.4s loss=0.830, TAw acc= 72.4% |
| Epoch  32, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=1.090, TAw acc= 64.9% | Valid: time=  0.4s loss=0.848, TAw acc= 71.6% |
| Epoch  33, lr=2.9e-03 time=  2.9s/  1.9s | Train: loss=1.090, TAw acc= 64.2% | Valid: time=  0.5s loss=0.855, TAw acc= 71.6% |
| Epoch  34, lr=2.9e-03 time=  2.0s/  2.4s | Train: loss=1.098, TAw acc= 64.4% | Valid: time=  0.5s loss=0.852, TAw acc= 71.8% |
| Epoch  35, lr=2.9e-03 time=  2.1s/  2.1s | Train: loss=1.071, TAw acc= 64.6% | Valid: time=  0.4s loss=0.855, TAw acc= 71.6% |
| Epoch  36, lr=2.9e-03 time=  1.8s/  1.8s | Train: loss=1.089, TAw acc= 64.3% | Valid: time=  0.4s loss=0.855, TAw acc= 71.6% |
| Epoch  37, lr=2.9e-03 time=  1.6s/  1.9s | Train: loss=1.088, TAw acc= 64.7% | Valid: time=  0.5s loss=0.858, TAw acc= 71.8% |
| Epoch  38, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=1.085, TAw acc= 64.1% | Valid: time=  0.8s loss=0.852, TAw acc= 72.2% |
| Epoch  39, lr=2.9e-03 time=  2.3s/  1.8s | Train: loss=1.076, TAw acc= 65.1% | Valid: time=  0.5s loss=0.848, TAw acc= 72.2% |
| Epoch  40, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=1.074, TAw acc= 63.8% | Valid: time=  0.4s loss=0.849, TAw acc= 71.8% |
| Epoch  41, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=1.073, TAw acc= 65.6% | Valid: time=  0.5s loss=0.847, TAw acc= 71.6% |
| Epoch  42, lr=2.9e-03 time=  3.0s/  1.9s | Train: loss=1.080, TAw acc= 64.2% | Valid: time=  0.4s loss=0.846, TAw acc= 71.4% |
| Epoch  43, lr=2.9e-03 time=  2.1s/  2.4s | Train: loss=1.061, TAw acc= 64.5% | Valid: time=  0.5s loss=0.844, TAw acc= 71.4% |
| Epoch  44, lr=2.9e-03 time=  2.1s/  2.4s | Train: loss=1.058, TAw acc= 64.8% | Valid: time=  0.5s loss=0.844, TAw acc= 72.0% |
| Epoch  45, lr=2.9e-03 time=  2.1s/  2.3s | Train: loss=1.075, TAw acc= 64.9% | Valid: time=  0.5s loss=0.841, TAw acc= 72.2% |
| Epoch  46, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=1.084, TAw acc= 64.1% | Valid: time=  0.4s loss=0.839, TAw acc= 72.0% |
| Epoch  47, lr=2.9e-03 time=  1.9s/  1.9s | Train: loss=1.072, TAw acc= 64.6% | Valid: time=  0.4s loss=0.841, TAw acc= 72.2% |
| Epoch  48, lr=2.9e-03 time=  1.6s/  1.9s | Train: loss=1.076, TAw acc= 64.9% | Valid: time=  0.4s loss=0.839, TAw acc= 72.0% |
| Epoch  49, lr=2.9e-03 time=  1.7s/  2.8s | Train: loss=1.066, TAw acc= 64.8% | Valid: time=  0.5s loss=0.838, TAw acc= 72.2% |
| Epoch  50, lr=2.9e-03 time=  1.7s/  1.8s | Train: loss=1.064, TAw acc= 64.3% | Valid: time=  0.4s loss=0.837, TAw acc= 72.4% |
| Epoch  51, lr=2.9e-03 time=  1.6s/  1.8s | Train: loss=1.054, TAw acc= 64.2% | Valid: time=  0.4s loss=0.837, TAw acc= 72.4% |
| Epoch  52, lr=2.9e-03 time=  1.6s/  1.9s | Train: loss=1.059, TAw acc= 64.4% | Valid: time=  0.4s loss=0.841, TAw acc= 72.6% |
| Epoch  53, lr=2.9e-03 time=  1.6s/  3.1s | Train: loss=1.065, TAw acc= 64.7% | Valid: time=  0.5s loss=0.836, TAw acc= 72.2% |
| Epoch  54, lr=2.9e-03 time=  1.7s/  2.3s | Train: loss=1.058, TAw acc= 64.4% | Valid: time=  0.5s loss=0.843, TAw acc= 72.0% |
| Epoch  55, lr=2.9e-03 time=  2.1s/  2.4s | Train: loss=1.050, TAw acc= 64.6% | Valid: time=  0.5s loss=0.836, TAw acc= 72.0% |
| Epoch  56, lr=2.9e-03 time=  2.2s/  2.4s | Train: loss=1.063, TAw acc= 64.0% | Valid: time=  0.5s loss=0.832, TAw acc= 72.2% |
| Epoch  57, lr=2.9e-03 time=  2.1s/  2.4s | Train: loss=1.055, TAw acc= 64.6% | Valid: time=  0.5s loss=0.833, TAw acc= 72.0% |
| Epoch  58, lr=2.9e-03 time=  2.1s/  2.4s | Train: loss=1.053, TAw acc= 64.3% | Valid: time=  0.5s loss=0.834, TAw acc= 72.2% |
| Epoch  59, lr=2.9e-03 time=  2.1s/  2.3s | Train: loss=1.062, TAw acc= 64.6% | Valid: time=  0.5s loss=0.830, TAw acc= 71.8% |
| Epoch  60, lr=2.9e-03 time=  2.1s/  2.3s | Train: loss=1.043, TAw acc= 64.9% | Valid: time=  0.4s loss=0.833, TAw acc= 71.8% | lr=9.7e-04
Debug-2: loss=0.797, acc=0.752
sow: rank=512, freezed_rank=928
 a:        ['-6.39e+00', '+4.95e+00'] .... ['-1.80e+308', '-1.80e+308'] .... ['-1.80e+308', '-1.80e+308']
 a_backup: ['-6.39e+00', '+4.95e+00'] .... ['+4.98e+00', '+4.39e+00'] .... ['+1.29e+00', '-2.74e+00']
Free_Classifier: Done (training=True, fix_classifier=False)
Debug-3: loss=0.793 acc=0.730
sow: rank=1024, freezed_rank=928
 a:        ['-6.39e+00', '+4.95e+00'] .... ['+4.98e+00', '+4.39e+00'] .... ['+1.29e+00', '-2.74e+00']
 a_backup: ['-6.39e+00', '+4.95e+00'] .... ['+4.98e+00', '+4.39e+00'] .... ['+1.29e+00', '-2.74e+00']
Header Training: Finished.
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.592 | TAw acc= 80.3%, forg=  0.0%| TAg acc= 11.1%, forg= 69.2% <<<
>>> Test on task  1 : loss=0.953 | TAw acc= 67.2%, forg=  0.0%| TAg acc= 27.8%, forg= 25.7% <<<
>>> Test on task  2 : loss=0.628 | TAw acc= 79.9%, forg=  0.0%| TAg acc= 34.9%, forg= 22.5% <<<
>>> Test on task  3 : loss=0.612 | TAw acc= 79.6%, forg=  0.0%| TAg acc= 27.3%, forg= 16.4% <<<
>>> Test on task  4 : loss=0.516 | TAw acc= 83.5%, forg=  0.0%| TAg acc= 14.4%, forg= 20.3% <<<
>>> Test on task  5 : loss=0.808 | TAw acc= 69.6%, forg=  0.0%| TAg acc= 23.5%, forg= 13.7% <<<
>>> Test on task  6 : loss=0.592 | TAw acc= 78.6%, forg=  0.0%| TAg acc= 36.3%, forg=  8.9% <<<
>>> Test on task  7 : loss=0.666 | TAw acc= 77.9%, forg=  0.0%| TAg acc= 24.0%, forg=  5.7% <<<
>>> Test on task  8 : loss=0.717 | TAw acc= 75.6%, forg=  0.0%| TAg acc= 38.3%, forg=  1.0% <<<
>>> Test on task  9 : loss=0.829 | TAw acc= 72.8%, forg=  0.0%| TAg acc= 32.8%, forg=  0.0% <<<
Save at ../RESULT_AAAI2025/CR10/0/cifar100_sow
************************************************************************************************************
TAw Acc
	 80.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 80.3% 
	 80.3%  67.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 73.8% 
	 80.3%  67.2%  79.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 75.8% 
	 80.3%  67.2%  79.9%  79.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 76.8% 
	 80.3%  67.2%  79.9%  79.6%  83.5%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 78.1% 
	 80.3%  67.2%  79.9%  79.6%  83.5%  69.6%   0.0%   0.0%   0.0%   0.0% 	Avg.: 76.7% 
	 80.3%  67.2%  79.9%  79.6%  83.5%  69.6%  78.6%   0.0%   0.0%   0.0% 	Avg.: 77.0% 
	 80.3%  67.2%  79.9%  79.6%  83.5%  69.6%  78.6%  77.9%   0.0%   0.0% 	Avg.: 77.1% 
	 80.3%  67.2%  79.9%  79.6%  83.5%  69.6%  78.6%  77.9%  75.6%   0.0% 	Avg.: 76.9% 
	 80.3%  67.2%  79.9%  79.6%  83.5%  69.6%  78.6%  77.9%  75.6%  72.8% 	Avg.: 76.5% 
************************************************************************************************************
TAg Acc
	 80.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 80.3% 
	 57.4%  53.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 55.5% 
	 45.3%  45.8%  57.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 49.5% 
	 33.7%  38.3%  49.1%  43.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 41.2% 
	 29.2%  37.1%  47.8%  41.8%  34.7%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 38.1% 
	 26.1%  35.4%  42.7%  35.8%  29.3%  37.2%   0.0%   0.0%   0.0%   0.0% 	Avg.: 34.4% 
	 22.4%  33.2%  39.6%  31.8%  27.1%  30.1%  45.2%   0.0%   0.0%   0.0% 	Avg.: 32.8% 
	 19.7%  31.0%  37.3%  30.2%  25.2%  27.5%  42.8%  29.7%   0.0%   0.0% 	Avg.: 30.4% 
	 13.7%  28.9%  35.8%  28.1%  17.7%  25.7%  39.0%  25.7%  39.3%   0.0% 	Avg.: 28.2% 
	 11.1%  27.8%  34.9%  27.3%  14.4%  23.5%  36.3%  24.0%  38.3%  32.8% 	Avg.: 27.0% 
************************************************************************************************************
TAw Forg
	  0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 
	  0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.0% 
	  0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.0% 
	  0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.0% 
	  0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.0% 
	  0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.0% 
	  0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.0% 
	  0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.0% 
	  0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.0% 
	  0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.0% 
************************************************************************************************************
TAg Forg
	  0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 
	 22.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 22.9% 
	 35.0%   7.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 21.4% 
	 46.6%  15.2%   8.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 23.4% 
	 51.1%  16.4%   9.6%   1.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 19.8% 
	 54.2%  18.1%  14.7%   7.9%   5.4%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 20.1% 
	 57.9%  20.3%  17.8%  11.9%   7.6%   7.1%   0.0%   0.0%   0.0%   0.0% 	Avg.: 20.4% 
	 60.6%  22.5%  20.1%  13.5%   9.5%   9.7%   2.4%   0.0%   0.0%   0.0% 	Avg.: 19.8% 
	 66.6%  24.6%  21.6%  15.6%  17.0%  11.5%   6.2%   4.0%   0.0%   0.0% 	Avg.: 20.9% 
	 69.2%  25.7%  22.5%  16.4%  20.3%  13.7%   8.9%   5.7%   1.0%   0.0% 	Avg.: 20.4% 
************************************************************************************************************
>> Task 0, SOW-0: Rank = 512 (928)
>> Task 1, SOW-0: Rank = 672 (928)
>> Task 2, SOW-0: Rank = 768 (928)
>> Task 3, SOW-0: Rank = 928 (928)
>> Task 4, SOW-0: Rank = 704 (928)
>> Task 5, SOW-0: Rank = 768 (928)
>> Task 6, SOW-0: Rank = 768 (928)
>> Task 7, SOW-0: Rank = 768 (928)
>> Task 8, SOW-0: Rank = 512 (928)
>> Task 9, SOW-0: Rank = 512 (928)
************************************************************************************************************
[Elapsed time = 1.8 h]
Done!
