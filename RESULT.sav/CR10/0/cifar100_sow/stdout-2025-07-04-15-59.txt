============================================================================================================
Arguments =
	approach: sow
	batch_size: 64
	clipping: 1.0
	datasets: ['cifar100']
	eval_on_train: True
	exp_name: None
	fix_bn: True
	gpu: 0
	gridsearch_tasks: -1
	keep_existing_head: False
	last_layer_analysis: False
	log: ['disk']
	lr: 0.0263039750973134
	lr_factor: 3.0
	lr_first: None
	lr_min: 1e-05
	lr_patience: 30
	momentum: 0.9
	multi_softmax: True
	nc_first_task: None
	nepochs: 60
	network: resnet50_32sow
	no_cudnn_deterministic: False
	num_tasks: 10
	num_workers: 4
	pin_memory: True
	pretrained: False
	results_path: ../RESULT_AAAI2025/CR10/0
	save_models: True
	seed: 0
	stop_at_task: 0
	validation: 0.1
	warmup_lr_factor: 1.0
	warmup_nepochs: 0
	weight_decay: 0.0
============================================================================================================
	device: cuda:0
============================================================================================================
Network arguments =
	dropout: 0.417598542370663
	fix_features: True
	load_features: True
	pretrained_path: ../Conv-Model/ResNet50-TinyImageNet.pt
============================================================================================================
../src/networks/INIT/1024.dict: loading..
ResNet50_32SOW,__init__: features is loaded (../Conv-Model/ResNet50-TinyImageNet.pt)
Fix_Features: Done
Approach arguments =
	acc_margin: 0.004
	all_outputs: False
	loss_margin: 0.006
	sow_lr: 0.05
	sow_mo: 0.9
============================================================================================================
Exemplars dataset arguments =
	exemplar_selection: random
	num_exemplars: 0
	num_exemplars_per_class: 0
============================================================================================================
class_indices: [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
[(0, 10), (1, 10), (2, 10), (3, 10), (4, 10), (5, 10), (6, 10), (7, 10), (8, 10), (9, 10)]
************************************************************************************************************
Task  0
************************************************************************************************************
LLL_Net(
  (model): ResNet50_32SOW(
    in_channels=3, in_H=32, in_W=32, training=True, fix_features=True, fix_classifier=False
    (features): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
      (3): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (4): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (3): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (5): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (3): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (4): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (5): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (6): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (7): AdaptiveAvgPool2d(output_size=(1, 1))
    )
    (classifier): Sequential(
      (0): Dropout(p=0.417598542370663, inplace=False)
      (1): SOW_V3(in_features=1024, out_features=1024, num_tasks=10, dtype=torch.float64, device=cuda:0)
      (2): ReLU()
    )
    (fc): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1024, out_features=10, bias=True)
  )
)
SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.0263039750973134
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0

Parameter Group 1
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.05
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0
)
 Group[0]  param_num=161, free_param_num=2
 Group[1]  param_num=3, free_param_num=3
sow: rank=1024, freezed_rank=0
 a:        ['-6.76e+00', '+4.95e+00'] .... ['+1.29e+00', '-2.74e+00']
| Epoch   1, lr=2.6e-02 time=  3.5s/  1.9s | Train: loss=1.371, TAw acc= 53.6% | Valid: time=  0.5s loss=1.088, TAw acc= 59.4% | *
| Epoch   2, lr=2.6e-02 time=  3.0s/  1.9s | Train: loss=1.276, TAw acc= 57.5% | Valid: time=  0.4s loss=0.876, TAw acc= 70.0% | *
| Epoch   3, lr=2.6e-02 time=  3.0s/  2.0s | Train: loss=1.191, TAw acc= 59.1% | Valid: time=  0.5s loss=0.825, TAw acc= 70.0% | *
| Epoch   4, lr=2.6e-02 time=  3.0s/  2.9s | Train: loss=1.227, TAw acc= 56.7% | Valid: time=  0.4s loss=0.820, TAw acc= 70.8% | *
| Epoch   5, lr=2.6e-02 time=  3.4s/  2.4s | Train: loss=1.144, TAw acc= 62.4% | Valid: time=  0.5s loss=0.815, TAw acc= 72.2% | *
| Epoch   6, lr=2.6e-02 time=  3.9s/  2.4s | Train: loss=1.141, TAw acc= 62.2% | Valid: time=  0.5s loss=0.770, TAw acc= 72.6% | *
| Epoch   7, lr=2.6e-02 time=  3.9s/  2.4s | Train: loss=1.181, TAw acc= 60.5% | Valid: time=  0.5s loss=0.743, TAw acc= 75.2% | *
| Epoch   8, lr=2.6e-02 time=  3.6s/  1.9s | Train: loss=1.139, TAw acc= 61.4% | Valid: time=  0.4s loss=0.770, TAw acc= 74.4% |
| Epoch   9, lr=2.6e-02 time=  3.1s/  3.2s | Train: loss=1.121, TAw acc= 62.1% | Valid: time=  0.4s loss=0.758, TAw acc= 73.4% |
| Epoch  10, lr=2.6e-02 time=  3.2s/  2.5s | Train: loss=1.170, TAw acc= 60.1% | Valid: time=  0.5s loss=0.906, TAw acc= 70.0% |
| Epoch  11, lr=2.6e-02 time=  3.9s/  2.4s | Train: loss=1.082, TAw acc= 64.4% | Valid: time=  0.5s loss=0.712, TAw acc= 73.8% | *
| Epoch  12, lr=2.6e-02 time=  3.3s/  1.9s | Train: loss=1.126, TAw acc= 61.2% | Valid: time=  0.4s loss=0.779, TAw acc= 72.4% |
| Epoch  13, lr=2.6e-02 time=  3.6s/  1.9s | Train: loss=1.112, TAw acc= 63.1% | Valid: time=  0.4s loss=0.760, TAw acc= 73.8% |
| Epoch  14, lr=2.6e-02 time=  3.0s/  1.9s | Train: loss=1.078, TAw acc= 64.2% | Valid: time=  0.4s loss=0.724, TAw acc= 73.0% |
| Epoch  15, lr=2.6e-02 time=  3.0s/  3.1s | Train: loss=1.086, TAw acc= 63.6% | Valid: time=  0.4s loss=0.743, TAw acc= 75.2% |
| Epoch  16, lr=2.6e-02 time=  3.1s/  2.5s | Train: loss=1.069, TAw acc= 64.4% | Valid: time=  0.5s loss=0.693, TAw acc= 75.6% | *
| Epoch  17, lr=2.6e-02 time=  4.0s/  2.4s | Train: loss=1.020, TAw acc= 65.6% | Valid: time=  0.5s loss=0.767, TAw acc= 74.4% |
| Epoch  18, lr=2.6e-02 time=  3.9s/  2.4s | Train: loss=1.028, TAw acc= 65.0% | Valid: time=  0.5s loss=0.665, TAw acc= 77.4% | *
| Epoch  19, lr=2.6e-02 time=  3.0s/  1.9s | Train: loss=1.062, TAw acc= 63.9% | Valid: time=  0.5s loss=0.689, TAw acc= 75.6% |
| Epoch  20, lr=2.6e-02 time=  3.0s/  2.0s | Train: loss=1.033, TAw acc= 65.2% | Valid: time=  0.5s loss=0.805, TAw acc= 70.8% |
| Epoch  21, lr=2.6e-02 time=  4.3s/  1.9s | Train: loss=1.058, TAw acc= 64.9% | Valid: time=  0.4s loss=0.729, TAw acc= 74.4% |
| Epoch  22, lr=2.6e-02 time=  3.9s/  2.4s | Train: loss=1.036, TAw acc= 64.7% | Valid: time=  0.5s loss=0.618, TAw acc= 79.4% | *
| Epoch  23, lr=2.6e-02 time=  3.9s/  2.4s | Train: loss=1.022, TAw acc= 67.2% | Valid: time=  0.5s loss=0.707, TAw acc= 75.8% |
| Epoch  24, lr=2.6e-02 time=  3.5s/  2.1s | Train: loss=1.027, TAw acc= 65.3% | Valid: time=  0.4s loss=0.687, TAw acc= 75.6% |
| Epoch  25, lr=2.6e-02 time=  3.0s/  1.9s | Train: loss=1.026, TAw acc= 65.8% | Valid: time=  0.4s loss=0.639, TAw acc= 78.0% |
| Epoch  26, lr=2.6e-02 time=  3.1s/  3.1s | Train: loss=1.004, TAw acc= 66.8% | Valid: time=  0.4s loss=0.698, TAw acc= 76.2% |
| Epoch  27, lr=2.6e-02 time=  3.0s/  2.5s | Train: loss=1.027, TAw acc= 65.8% | Valid: time=  0.5s loss=0.757, TAw acc= 73.2% |
| Epoch  28, lr=2.6e-02 time=  4.0s/  2.4s | Train: loss=1.049, TAw acc= 65.7% | Valid: time=  0.5s loss=0.644, TAw acc= 79.4% |
| Epoch  29, lr=2.6e-02 time=  3.0s/  1.9s | Train: loss=1.025, TAw acc= 66.0% | Valid: time=  0.4s loss=0.693, TAw acc= 74.4% |
| Epoch  30, lr=2.6e-02 time=  3.0s/  3.2s | Train: loss=1.009, TAw acc= 66.7% | Valid: time=  0.4s loss=0.711, TAw acc= 75.6% |
| Epoch  31, lr=2.6e-02 time=  3.0s/  2.4s | Train: loss=0.992, TAw acc= 67.0% | Valid: time=  0.5s loss=0.660, TAw acc= 77.8% |
| Epoch  32, lr=2.6e-02 time=  3.8s/  2.4s | Train: loss=1.011, TAw acc= 66.0% | Valid: time=  0.5s loss=0.660, TAw acc= 77.6% |
| Epoch  33, lr=2.6e-02 time=  3.9s/  2.4s | Train: loss=0.978, TAw acc= 66.4% | Valid: time=  0.4s loss=0.697, TAw acc= 74.8% |
| Epoch  34, lr=2.6e-02 time=  3.2s/  2.0s | Train: loss=1.070, TAw acc= 62.9% | Valid: time=  0.5s loss=0.687, TAw acc= 75.8% |
| Epoch  35, lr=2.6e-02 time=  3.2s/  1.9s | Train: loss=1.018, TAw acc= 65.2% | Valid: time=  0.5s loss=0.735, TAw acc= 75.4% |
| Epoch  36, lr=2.6e-02 time=  3.0s/  3.1s | Train: loss=1.023, TAw acc= 65.0% | Valid: time=  0.4s loss=0.682, TAw acc= 77.6% |
| Epoch  37, lr=2.6e-02 time=  3.1s/  1.9s | Train: loss=1.029, TAw acc= 64.1% | Valid: time=  0.4s loss=0.728, TAw acc= 74.6% |
| Epoch  38, lr=2.6e-02 time=  3.5s/  2.0s | Train: loss=0.981, TAw acc= 66.4% | Valid: time=  0.4s loss=0.681, TAw acc= 75.8% |
| Epoch  39, lr=2.6e-02 time=  3.1s/  2.9s | Train: loss=0.995, TAw acc= 65.1% | Valid: time=  0.4s loss=0.726, TAw acc= 74.0% |
| Epoch  40, lr=2.6e-02 time=  3.0s/  2.4s | Train: loss=1.018, TAw acc= 66.6% | Valid: time=  0.5s loss=0.632, TAw acc= 77.4% |
| Epoch  41, lr=2.6e-02 time=  3.9s/  2.5s | Train: loss=0.992, TAw acc= 67.0% | Valid: time=  0.5s loss=0.663, TAw acc= 77.0% |
| Epoch  42, lr=2.6e-02 time=  3.9s/  2.4s | Train: loss=1.014, TAw acc= 66.0% | Valid: time=  0.5s loss=0.662, TAw acc= 77.8% |
| Epoch  43, lr=2.6e-02 time=  3.4s/  1.9s | Train: loss=1.030, TAw acc= 64.6% | Valid: time=  0.4s loss=0.756, TAw acc= 75.0% |
| Epoch  44, lr=2.6e-02 time=  3.7s/  1.9s | Train: loss=1.008, TAw acc= 67.6% | Valid: time=  0.4s loss=0.725, TAw acc= 75.0% |
| Epoch  45, lr=2.6e-02 time=  3.3s/  1.9s | Train: loss=1.035, TAw acc= 65.3% | Valid: time=  0.4s loss=0.664, TAw acc= 77.0% |
| Epoch  46, lr=2.6e-02 time=  3.2s/  1.9s | Train: loss=1.068, TAw acc= 62.9% | Valid: time=  0.4s loss=0.754, TAw acc= 72.8% |
| Epoch  47, lr=2.6e-02 time=  3.0s/  1.9s | Train: loss=0.987, TAw acc= 67.5% | Valid: time=  0.5s loss=0.677, TAw acc= 78.6% |
| Epoch  48, lr=2.6e-02 time=  3.0s/  3.1s | Train: loss=0.967, TAw acc= 67.5% | Valid: time=  0.4s loss=0.637, TAw acc= 77.2% |
| Epoch  49, lr=2.6e-02 time=  3.3s/  2.5s | Train: loss=0.979, TAw acc= 68.5% | Valid: time=  0.5s loss=0.695, TAw acc= 77.0% |
| Epoch  50, lr=2.6e-02 time=  3.9s/  2.4s | Train: loss=0.971, TAw acc= 66.7% | Valid: time=  0.5s loss=0.652, TAw acc= 77.2% |
| Epoch  51, lr=2.6e-02 time=  3.9s/  2.4s | Train: loss=1.028, TAw acc= 65.2% | Valid: time=  0.5s loss=0.623, TAw acc= 79.6% |
| Epoch  52, lr=2.6e-02 time=  3.3s/  1.9s | Train: loss=0.957, TAw acc= 68.2% | Valid: time=  0.4s loss=0.623, TAw acc= 78.8% | lr=8.8e-03
| Epoch  53, lr=8.8e-03 time=  3.0s/  1.9s | Train: loss=1.015, TAw acc= 65.8% | Valid: time=  0.4s loss=0.665, TAw acc= 76.4% |
| Epoch  54, lr=8.8e-03 time=  4.2s/  2.0s | Train: loss=1.129, TAw acc= 61.5% | Valid: time=  0.5s loss=0.791, TAw acc= 73.2% |
| Epoch  55, lr=8.8e-03 time=  3.9s/  2.4s | Train: loss=1.046, TAw acc= 65.0% | Valid: time=  0.5s loss=0.698, TAw acc= 75.6% |
| Epoch  56, lr=8.8e-03 time=  3.9s/  2.4s | Train: loss=1.046, TAw acc= 64.7% | Valid: time=  0.5s loss=0.675, TAw acc= 76.6% |
| Epoch  57, lr=8.8e-03 time=  4.0s/  2.3s | Train: loss=1.024, TAw acc= 65.0% | Valid: time=  0.4s loss=0.754, TAw acc= 72.2% |
| Epoch  58, lr=8.8e-03 time=  3.0s/  1.9s | Train: loss=1.019, TAw acc= 65.3% | Valid: time=  0.4s loss=0.680, TAw acc= 76.2% |
| Epoch  59, lr=8.8e-03 time=  3.0s/  1.9s | Train: loss=1.007, TAw acc= 66.4% | Valid: time=  0.4s loss=0.650, TAw acc= 77.8% |
| Epoch  60, lr=8.8e-03 time=  4.0s/  2.2s | Train: loss=0.974, TAw acc= 67.1% | Valid: time=  0.5s loss=0.704, TAw acc= 75.2% |
== Rank Reduction [task:0] ==
Debug-0:
  best_loss=0.618,   best_acc=0.794
loss_margin=0.006, acc_margin=0.004
sow: rank=1024, freezed_rank=0
 a:        ['-5.75e+00', '+4.96e+00'] .... ['+1.29e+00', '-2.74e+00']
 r=256, loss=1.517, acc=0.504 (rank=256)
 r=288, loss=1.451, acc=0.504 (rank=288)
 r=320, loss=1.330, acc=0.532 (rank=320)
 r=352, loss=1.156, acc=0.628 (rank=352)
 r=384, loss=1.013, acc=0.664 (rank=384)
 r=416, loss=0.881, acc=0.700 (rank=416)
 r=448, loss=0.748, acc=0.742 (rank=448)
 r=480, loss=0.679, acc=0.766 (rank=480)
 r=512, loss=0.634, acc=0.792 (rank=512)
 r=544, loss=0.638, acc=0.792 (rank=544)
 r=576, loss=0.625, acc=0.796 (rank=576)
 r=608, loss=0.626, acc=0.796 (rank=608)
 r=640, loss=0.633, acc=0.794 (rank=640)
 r=672, loss=0.637, acc=0.790 (rank=672)
 r=704, loss=0.632, acc=0.796 (rank=704)
 r=736, loss=0.620, acc=0.798 (rank=736)
 best_r=736, loss=0.620, acc=0.798
== Header Training for Low Rank [task:0] ==
Fix_Classifier: Done (training=False, fix_classifier=True)
Debug-1: loss=0.620, acc=0.798
sow: rank=736, freezed_rank=736
 a:        ['-5.75e+00', '+4.96e+00'] .... ['+5.57e+00', '-1.80e+308'] .... ['-1.80e+308', '-1.80e+308']
 a_backup: ['-5.75e+00', '+4.96e+00'] .... ['+5.57e+00', '+6.13e+00'] .... ['+1.29e+00', '-2.74e+00']
SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.008767991699104466
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0

Parameter Group 1
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.05
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0
)
 Group[0]  param_num=161, free_param_num=2
 Group[1]  param_num=3, free_param_num=0
sow: rank=736, freezed_rank=736
 a:        ['-5.75e+00', '+4.96e+00'] .... ['+5.57e+00', '-1.80e+308'] .... ['-1.80e+308', '-1.80e+308']
 a_backup: ['-5.75e+00', '+4.96e+00'] .... ['+5.57e+00', '+6.13e+00'] .... ['+1.29e+00', '-2.74e+00']
| Epoch   1, lr=8.8e-03 time=  2.1s/  2.4s | Train: loss=0.967, TAw acc= 68.8% | Valid: time=  0.5s loss=0.664, TAw acc= 77.2% |
| Epoch   2, lr=8.8e-03 time=  2.1s/  2.1s | Train: loss=0.946, TAw acc= 68.8% | Valid: time=  0.4s loss=0.663, TAw acc= 76.6% |
| Epoch   3, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.943, TAw acc= 67.9% | Valid: time=  0.4s loss=0.640, TAw acc= 78.0% |
| Epoch   4, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.927, TAw acc= 69.1% | Valid: time=  0.4s loss=0.639, TAw acc= 78.6% |
| Epoch   5, lr=8.8e-03 time=  1.7s/  3.1s | Train: loss=0.928, TAw acc= 69.3% | Valid: time=  0.4s loss=0.632, TAw acc= 78.8% |
| Epoch   6, lr=8.8e-03 time=  1.7s/  2.3s | Train: loss=0.920, TAw acc= 69.2% | Valid: time=  0.5s loss=0.670, TAw acc= 77.6% |
| Epoch   7, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=0.930, TAw acc= 68.2% | Valid: time=  0.5s loss=0.646, TAw acc= 78.8% |
| Epoch   8, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=0.917, TAw acc= 69.3% | Valid: time=  0.5s loss=0.651, TAw acc= 80.0% |
| Epoch   9, lr=8.8e-03 time=  2.1s/  2.0s | Train: loss=0.927, TAw acc= 68.9% | Valid: time=  0.4s loss=0.644, TAw acc= 80.2% |
| Epoch  10, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.907, TAw acc= 69.4% | Valid: time=  0.4s loss=0.686, TAw acc= 76.2% |
| Epoch  11, lr=8.8e-03 time=  1.6s/  1.9s | Train: loss=0.875, TAw acc= 71.0% | Valid: time=  0.9s loss=0.670, TAw acc= 78.8% |
| Epoch  12, lr=8.8e-03 time=  1.8s/  1.9s | Train: loss=0.920, TAw acc= 68.7% | Valid: time=  0.5s loss=0.673, TAw acc= 78.0% |
| Epoch  13, lr=8.8e-03 time=  2.1s/  2.4s | Train: loss=0.903, TAw acc= 69.9% | Valid: time=  0.5s loss=0.679, TAw acc= 78.8% |
| Epoch  14, lr=8.8e-03 time=  2.1s/  2.4s | Train: loss=0.900, TAw acc= 69.9% | Valid: time=  0.5s loss=0.652, TAw acc= 79.4% |
| Epoch  15, lr=8.8e-03 time=  2.1s/  2.4s | Train: loss=0.862, TAw acc= 71.5% | Valid: time=  0.5s loss=0.679, TAw acc= 79.4% |
| Epoch  16, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=0.867, TAw acc= 72.0% | Valid: time=  0.5s loss=0.655, TAw acc= 79.4% |
| Epoch  17, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=0.872, TAw acc= 70.2% | Valid: time=  0.5s loss=0.669, TAw acc= 78.4% |
| Epoch  18, lr=8.8e-03 time=  1.7s/  2.2s | Train: loss=0.878, TAw acc= 71.4% | Valid: time=  0.4s loss=0.688, TAw acc= 79.0% |
| Epoch  19, lr=8.8e-03 time=  1.7s/  2.8s | Train: loss=0.859, TAw acc= 71.3% | Valid: time=  0.4s loss=0.647, TAw acc= 78.8% |
| Epoch  20, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.875, TAw acc= 70.4% | Valid: time=  0.4s loss=0.657, TAw acc= 78.2% |
| Epoch  21, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.880, TAw acc= 70.5% | Valid: time=  0.4s loss=0.661, TAw acc= 78.6% |
| Epoch  22, lr=8.8e-03 time=  2.6s/  1.9s | Train: loss=0.854, TAw acc= 70.9% | Valid: time=  0.4s loss=0.661, TAw acc= 79.0% |
| Epoch  23, lr=8.8e-03 time=  1.7s/  2.1s | Train: loss=0.851, TAw acc= 71.2% | Valid: time=  0.4s loss=0.638, TAw acc= 79.8% |
| Epoch  24, lr=8.8e-03 time=  1.9s/  1.9s | Train: loss=0.853, TAw acc= 71.3% | Valid: time=  0.4s loss=0.663, TAw acc= 78.8% |
| Epoch  25, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.870, TAw acc= 70.6% | Valid: time=  0.4s loss=0.670, TAw acc= 79.0% |
| Epoch  26, lr=8.8e-03 time=  2.5s/  2.0s | Train: loss=0.849, TAw acc= 71.1% | Valid: time=  0.4s loss=0.670, TAw acc= 79.0% |
| Epoch  27, lr=8.8e-03 time=  1.7s/  2.5s | Train: loss=0.886, TAw acc= 69.6% | Valid: time=  0.5s loss=0.658, TAw acc= 80.6% |
| Epoch  28, lr=8.8e-03 time=  2.1s/  2.4s | Train: loss=0.868, TAw acc= 71.0% | Valid: time=  0.5s loss=0.679, TAw acc= 79.6% |
| Epoch  29, lr=8.8e-03 time=  1.9s/  2.1s | Train: loss=0.863, TAw acc= 71.3% | Valid: time=  0.4s loss=0.663, TAw acc= 79.0% |
| Epoch  30, lr=8.8e-03 time=  2.0s/  1.9s | Train: loss=0.841, TAw acc= 72.2% | Valid: time=  0.4s loss=0.668, TAw acc= 78.2% | lr=2.9e-03
| Epoch  31, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=1.003, TAw acc= 67.0% | Valid: time=  0.4s loss=0.663, TAw acc= 76.4% |
| Epoch  32, lr=2.9e-03 time=  1.7s/  2.0s | Train: loss=0.997, TAw acc= 67.4% | Valid: time=  0.4s loss=0.666, TAw acc= 76.0% |
| Epoch  33, lr=2.9e-03 time=  2.7s/  1.9s | Train: loss=0.958, TAw acc= 68.9% | Valid: time=  0.4s loss=0.654, TAw acc= 76.4% |
| Epoch  34, lr=2.9e-03 time=  2.1s/  2.4s | Train: loss=0.970, TAw acc= 68.4% | Valid: time=  0.5s loss=0.650, TAw acc= 77.4% |
| Epoch  35, lr=2.9e-03 time=  2.1s/  2.4s | Train: loss=0.959, TAw acc= 68.0% | Valid: time=  0.5s loss=0.654, TAw acc= 76.6% |
| Epoch  36, lr=2.9e-03 time=  2.1s/  2.4s | Train: loss=0.956, TAw acc= 68.7% | Valid: time=  0.5s loss=0.666, TAw acc= 76.6% |
| Epoch  37, lr=2.9e-03 time=  2.1s/  2.4s | Train: loss=0.955, TAw acc= 68.5% | Valid: time=  0.5s loss=0.650, TAw acc= 77.0% |
| Epoch  38, lr=2.9e-03 time=  2.1s/  2.4s | Train: loss=0.963, TAw acc= 68.4% | Valid: time=  0.5s loss=0.653, TAw acc= 77.0% |
| Epoch  39, lr=2.9e-03 time=  2.1s/  2.4s | Train: loss=0.968, TAw acc= 67.4% | Valid: time=  0.5s loss=0.644, TAw acc= 78.4% |
| Epoch  40, lr=2.9e-03 time=  2.1s/  2.4s | Train: loss=0.933, TAw acc= 69.0% | Valid: time=  0.5s loss=0.646, TAw acc= 77.6% |
| Epoch  41, lr=2.9e-03 time=  2.1s/  2.1s | Train: loss=0.934, TAw acc= 69.4% | Valid: time=  0.4s loss=0.651, TAw acc= 78.4% |
| Epoch  42, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=0.948, TAw acc= 68.5% | Valid: time=  0.5s loss=0.638, TAw acc= 78.0% |
| Epoch  43, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=0.948, TAw acc= 68.2% | Valid: time=  0.9s loss=0.644, TAw acc= 78.4% |
| Epoch  44, lr=2.9e-03 time=  2.1s/  1.9s | Train: loss=0.936, TAw acc= 68.8% | Valid: time=  0.5s loss=0.636, TAw acc= 78.8% |
| Epoch  45, lr=2.9e-03 time=  2.2s/  2.4s | Train: loss=0.938, TAw acc= 69.4% | Valid: time=  0.5s loss=0.639, TAw acc= 78.4% |
| Epoch  46, lr=2.9e-03 time=  2.1s/  2.4s | Train: loss=0.925, TAw acc= 70.0% | Valid: time=  0.5s loss=0.642, TAw acc= 78.6% |
| Epoch  47, lr=2.9e-03 time=  2.1s/  2.4s | Train: loss=0.942, TAw acc= 68.8% | Valid: time=  0.5s loss=0.644, TAw acc= 78.8% |
| Epoch  48, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=0.943, TAw acc= 68.4% | Valid: time=  0.4s loss=0.640, TAw acc= 79.0% |
| Epoch  49, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=0.931, TAw acc= 69.1% | Valid: time=  0.4s loss=0.640, TAw acc= 78.8% |
| Epoch  50, lr=2.9e-03 time=  2.0s/  2.5s | Train: loss=0.912, TAw acc= 70.1% | Valid: time=  0.4s loss=0.636, TAw acc= 79.2% |
| Epoch  51, lr=2.9e-03 time=  1.7s/  2.5s | Train: loss=0.934, TAw acc= 69.3% | Valid: time=  0.5s loss=0.646, TAw acc= 78.6% |
| Epoch  52, lr=2.9e-03 time=  2.2s/  2.4s | Train: loss=0.927, TAw acc= 69.3% | Valid: time=  0.5s loss=0.642, TAw acc= 79.2% |
| Epoch  53, lr=2.9e-03 time=  2.1s/  2.4s | Train: loss=0.910, TAw acc= 69.4% | Valid: time=  0.5s loss=0.643, TAw acc= 78.4% |
| Epoch  54, lr=2.9e-03 time=  2.2s/  2.2s | Train: loss=0.898, TAw acc= 70.5% | Valid: time=  0.4s loss=0.644, TAw acc= 79.2% |
| Epoch  55, lr=2.9e-03 time=  1.9s/  1.9s | Train: loss=0.912, TAw acc= 70.2% | Valid: time=  0.4s loss=0.641, TAw acc= 79.2% |
| Epoch  56, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=0.926, TAw acc= 68.8% | Valid: time=  0.4s loss=0.654, TAw acc= 79.4% |
| Epoch  57, lr=2.9e-03 time=  1.7s/  3.0s | Train: loss=0.894, TAw acc= 70.8% | Valid: time=  0.4s loss=0.644, TAw acc= 79.6% |
| Epoch  58, lr=2.9e-03 time=  1.7s/  2.4s | Train: loss=0.910, TAw acc= 69.2% | Valid: time=  0.5s loss=0.643, TAw acc= 79.2% |
| Epoch  59, lr=2.9e-03 time=  2.1s/  2.4s | Train: loss=0.891, TAw acc= 70.5% | Valid: time=  0.5s loss=0.662, TAw acc= 78.8% |
| Epoch  60, lr=2.9e-03 time=  2.1s/  2.4s | Train: loss=0.920, TAw acc= 69.7% | Valid: time=  0.5s loss=0.646, TAw acc= 79.4% | lr=9.7e-04
Debug-2: loss=0.620, acc=0.798
sow: rank=736, freezed_rank=736
 a:        ['-5.75e+00', '+4.96e+00'] .... ['+5.57e+00', '-1.80e+308'] .... ['-1.80e+308', '-1.80e+308']
 a_backup: ['-5.75e+00', '+4.96e+00'] .... ['+5.57e+00', '+6.13e+00'] .... ['+1.29e+00', '-2.74e+00']
Free_Classifier: Done (training=True, fix_classifier=False)
Debug-3: loss=0.618 acc=0.794
sow: rank=1024, freezed_rank=736
 a:        ['-5.75e+00', '+4.96e+00'] .... ['+5.57e+00', '+6.13e+00'] .... ['+1.29e+00', '-2.74e+00']
 a_backup: ['-5.75e+00', '+4.96e+00'] .... ['+5.57e+00', '+6.13e+00'] .... ['+1.29e+00', '-2.74e+00']
Header Training: Finished.
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.649 | TAw acc= 79.2%, forg=  0.0%| TAg acc= 79.2%, forg=  0.0% <<<
Save at ../RESULT_AAAI2025/CR10/0/cifar100_sow
************************************************************************************************************
Task  1
************************************************************************************************************
LLL_Net(
  (model): ResNet50_32SOW(
    in_channels=3, in_H=32, in_W=32, training=False, fix_features=True, fix_classifier=False
    (features): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
      (3): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (4): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (3): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (5): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (3): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (4): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (5): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (6): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (7): AdaptiveAvgPool2d(output_size=(1, 1))
    )
    (classifier): Sequential(
      (0): Dropout(p=0.417598542370663, inplace=False)
      (1): SOW_V3(in_features=1024, out_features=1024, num_tasks=10, dtype=torch.float64, device=cuda:0)
      (2): ReLU()
    )
    (fc): Sequential()
  )
  (heads): ModuleList(
    (0-1): 2 x Linear(in_features=1024, out_features=10, bias=True)
  )
)
SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.0263039750973134
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0

Parameter Group 1
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.05
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0
)
 Group[0]  param_num=161, free_param_num=2
 Group[1]  param_num=3, free_param_num=3
sow: rank=1024, freezed_rank=736
 a:        ['-5.75e+00', '+4.96e+00'] .... ['+5.57e+00', '+6.13e+00'] .... ['+1.29e+00', '-2.74e+00']
 a_backup: ['-5.75e+00', '+4.96e+00'] .... ['+5.57e+00', '+6.13e+00'] .... ['+1.29e+00', '-2.74e+00']
| Epoch   1, lr=2.6e-02 time=  3.7s/  2.2s | Train: loss=1.488, TAw acc= 48.6% | Valid: time=  0.4s loss=1.167, TAw acc= 60.8% | *
| Epoch   2, lr=2.6e-02 time=  3.4s/  1.9s | Train: loss=1.450, TAw acc= 50.3% | Valid: time=  0.4s loss=1.053, TAw acc= 63.2% | *
| Epoch   3, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=1.381, TAw acc= 51.8% | Valid: time=  0.4s loss=1.094, TAw acc= 62.0% |
| Epoch   4, lr=2.6e-02 time=  2.8s/  2.7s | Train: loss=1.383, TAw acc= 52.9% | Valid: time=  0.6s loss=1.084, TAw acc= 61.4% |
| Epoch   5, lr=2.6e-02 time=  2.8s/  2.4s | Train: loss=1.372, TAw acc= 52.4% | Valid: time=  0.5s loss=1.013, TAw acc= 63.8% | *
| Epoch   6, lr=2.6e-02 time=  3.6s/  2.4s | Train: loss=1.362, TAw acc= 53.0% | Valid: time=  0.5s loss=1.050, TAw acc= 64.4% |
| Epoch   7, lr=2.6e-02 time=  3.2s/  2.1s | Train: loss=1.325, TAw acc= 53.7% | Valid: time=  0.4s loss=1.047, TAw acc= 65.6% |
| Epoch   8, lr=2.6e-02 time=  3.0s/  1.9s | Train: loss=1.367, TAw acc= 52.1% | Valid: time=  0.4s loss=1.027, TAw acc= 63.2% |
| Epoch   9, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=1.345, TAw acc= 54.1% | Valid: time=  0.4s loss=1.062, TAw acc= 63.2% |
| Epoch  10, lr=2.6e-02 time=  2.8s/  3.2s | Train: loss=1.350, TAw acc= 54.1% | Valid: time=  0.4s loss=1.020, TAw acc= 64.2% |
| Epoch  11, lr=2.6e-02 time=  2.7s/  2.5s | Train: loss=1.336, TAw acc= 54.2% | Valid: time=  0.5s loss=1.042, TAw acc= 64.0% |
| Epoch  12, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.367, TAw acc= 54.0% | Valid: time=  0.5s loss=0.975, TAw acc= 65.0% | *
| Epoch  13, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.302, TAw acc= 56.8% | Valid: time=  0.5s loss=0.995, TAw acc= 64.2% |
| Epoch  14, lr=2.6e-02 time=  3.2s/  1.9s | Train: loss=1.327, TAw acc= 54.1% | Valid: time=  0.4s loss=1.030, TAw acc= 63.0% |
| Epoch  15, lr=2.6e-02 time=  4.0s/  1.9s | Train: loss=1.295, TAw acc= 56.2% | Valid: time=  0.4s loss=0.970, TAw acc= 67.4% | *
| Epoch  16, lr=2.6e-02 time=  2.9s/  2.1s | Train: loss=1.324, TAw acc= 54.9% | Valid: time=  0.4s loss=1.024, TAw acc= 64.8% |
| Epoch  17, lr=2.6e-02 time=  2.8s/  3.1s | Train: loss=1.291, TAw acc= 54.5% | Valid: time=  0.4s loss=1.005, TAw acc= 64.4% |
| Epoch  18, lr=2.6e-02 time=  2.9s/  2.1s | Train: loss=1.293, TAw acc= 55.1% | Valid: time=  0.4s loss=0.992, TAw acc= 64.6% |
| Epoch  19, lr=2.6e-02 time=  3.2s/  1.9s | Train: loss=1.295, TAw acc= 55.4% | Valid: time=  0.4s loss=0.990, TAw acc= 65.0% |
| Epoch  20, lr=2.6e-02 time=  2.9s/  1.9s | Train: loss=1.307, TAw acc= 56.4% | Valid: time=  0.4s loss=0.931, TAw acc= 67.6% | *
| Epoch  21, lr=2.6e-02 time=  4.1s/  2.0s | Train: loss=1.281, TAw acc= 56.0% | Valid: time=  0.5s loss=0.968, TAw acc= 66.2% |
| Epoch  22, lr=2.6e-02 time=  3.6s/  2.4s | Train: loss=1.335, TAw acc= 54.6% | Valid: time=  0.5s loss=0.996, TAw acc= 64.8% |
| Epoch  23, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.309, TAw acc= 55.4% | Valid: time=  0.5s loss=0.986, TAw acc= 66.6% |
| Epoch  24, lr=2.6e-02 time=  3.2s/  1.9s | Train: loss=1.282, TAw acc= 55.4% | Valid: time=  0.4s loss=0.998, TAw acc= 66.2% |
| Epoch  25, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=1.261, TAw acc= 56.8% | Valid: time=  0.4s loss=1.029, TAw acc= 65.6% |
| Epoch  26, lr=2.6e-02 time=  2.9s/  3.2s | Train: loss=1.278, TAw acc= 55.9% | Valid: time=  0.5s loss=0.989, TAw acc= 64.0% |
| Epoch  27, lr=2.6e-02 time=  2.8s/  2.5s | Train: loss=1.301, TAw acc= 54.4% | Valid: time=  0.5s loss=1.017, TAw acc= 63.0% |
| Epoch  28, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.309, TAw acc= 54.7% | Valid: time=  0.5s loss=0.995, TAw acc= 66.2% |
| Epoch  29, lr=2.6e-02 time=  3.3s/  1.9s | Train: loss=1.256, TAw acc= 56.8% | Valid: time=  0.4s loss=1.003, TAw acc= 65.8% |
| Epoch  30, lr=2.6e-02 time=  2.8s/  2.0s | Train: loss=1.274, TAw acc= 56.4% | Valid: time=  0.4s loss=0.953, TAw acc= 67.2% |
| Epoch  31, lr=2.6e-02 time=  2.8s/  3.0s | Train: loss=1.272, TAw acc= 55.6% | Valid: time=  0.4s loss=0.971, TAw acc= 67.0% |
| Epoch  32, lr=2.6e-02 time=  3.1s/  2.5s | Train: loss=1.261, TAw acc= 57.0% | Valid: time=  0.5s loss=0.976, TAw acc= 66.4% |
| Epoch  33, lr=2.6e-02 time=  3.6s/  2.4s | Train: loss=1.278, TAw acc= 55.2% | Valid: time=  0.5s loss=1.006, TAw acc= 64.0% |
| Epoch  34, lr=2.6e-02 time=  3.6s/  2.1s | Train: loss=1.264, TAw acc= 55.9% | Valid: time=  0.4s loss=0.954, TAw acc= 66.4% |
| Epoch  35, lr=2.6e-02 time=  2.8s/  2.7s | Train: loss=1.280, TAw acc= 55.1% | Valid: time=  0.4s loss=0.972, TAw acc= 65.6% |
| Epoch  36, lr=2.6e-02 time=  2.9s/  1.9s | Train: loss=1.253, TAw acc= 57.6% | Valid: time=  0.4s loss=1.004, TAw acc= 65.2% |
| Epoch  37, lr=2.6e-02 time=  3.4s/  1.9s | Train: loss=1.276, TAw acc= 55.0% | Valid: time=  0.4s loss=0.996, TAw acc= 66.2% |
| Epoch  38, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=1.280, TAw acc= 55.0% | Valid: time=  0.4s loss=0.949, TAw acc= 66.2% |
| Epoch  39, lr=2.6e-02 time=  2.9s/  2.3s | Train: loss=1.264, TAw acc= 56.0% | Valid: time=  0.9s loss=0.988, TAw acc= 66.0% |
| Epoch  40, lr=2.6e-02 time=  2.8s/  2.3s | Train: loss=1.285, TAw acc= 54.8% | Valid: time=  0.5s loss=1.001, TAw acc= 64.4% |
| Epoch  41, lr=2.6e-02 time=  3.6s/  2.4s | Train: loss=1.287, TAw acc= 55.2% | Valid: time=  0.5s loss=0.943, TAw acc= 66.6% |
| Epoch  42, lr=2.6e-02 time=  3.7s/  1.9s | Train: loss=1.246, TAw acc= 56.8% | Valid: time=  0.4s loss=0.964, TAw acc= 66.2% |
| Epoch  43, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=1.252, TAw acc= 56.6% | Valid: time=  0.4s loss=1.013, TAw acc= 65.8% |
| Epoch  44, lr=2.6e-02 time=  2.8s/  3.2s | Train: loss=1.237, TAw acc= 57.5% | Valid: time=  0.4s loss=0.984, TAw acc= 65.8% |
| Epoch  45, lr=2.6e-02 time=  2.9s/  2.5s | Train: loss=1.268, TAw acc= 55.9% | Valid: time=  0.5s loss=0.954, TAw acc= 67.8% |
| Epoch  46, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.255, TAw acc= 57.3% | Valid: time=  0.5s loss=0.986, TAw acc= 67.0% |
| Epoch  47, lr=2.6e-02 time=  3.6s/  2.4s | Train: loss=1.221, TAw acc= 57.3% | Valid: time=  0.5s loss=1.035, TAw acc= 64.6% |
| Epoch  48, lr=2.6e-02 time=  3.3s/  2.0s | Train: loss=1.251, TAw acc= 57.0% | Valid: time=  0.7s loss=0.970, TAw acc= 64.8% |
| Epoch  49, lr=2.6e-02 time=  3.0s/  2.0s | Train: loss=1.235, TAw acc= 57.5% | Valid: time=  0.4s loss=1.020, TAw acc= 66.4% |
| Epoch  50, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=1.261, TAw acc= 57.0% | Valid: time=  0.4s loss=0.956, TAw acc= 66.2% | lr=8.8e-03
| Epoch  51, lr=8.8e-03 time=  4.1s/  1.9s | Train: loss=1.281, TAw acc= 56.0% | Valid: time=  0.4s loss=0.969, TAw acc= 66.8% |
| Epoch  52, lr=8.8e-03 time=  3.6s/  2.4s | Train: loss=1.279, TAw acc= 56.6% | Valid: time=  0.5s loss=0.952, TAw acc= 66.0% |
| Epoch  53, lr=8.8e-03 time=  3.6s/  2.4s | Train: loss=1.274, TAw acc= 56.0% | Valid: time=  0.4s loss=0.983, TAw acc= 66.6% |
| Epoch  54, lr=8.8e-03 time=  3.0s/  2.0s | Train: loss=1.256, TAw acc= 56.5% | Valid: time=  0.4s loss=0.989, TAw acc= 65.6% |
| Epoch  55, lr=8.8e-03 time=  3.2s/  1.9s | Train: loss=1.255, TAw acc= 56.6% | Valid: time=  0.4s loss=0.969, TAw acc= 66.8% |
| Epoch  56, lr=8.8e-03 time=  2.8s/  1.9s | Train: loss=1.280, TAw acc= 56.0% | Valid: time=  0.4s loss=0.975, TAw acc= 66.8% |
| Epoch  57, lr=8.8e-03 time=  4.0s/  2.0s | Train: loss=1.257, TAw acc= 57.2% | Valid: time=  0.5s loss=0.964, TAw acc= 66.8% |
| Epoch  58, lr=8.8e-03 time=  3.7s/  2.5s | Train: loss=1.247, TAw acc= 57.3% | Valid: time=  0.5s loss=0.959, TAw acc= 66.4% |
| Epoch  59, lr=8.8e-03 time=  3.5s/  1.9s | Train: loss=1.282, TAw acc= 56.6% | Valid: time=  0.4s loss=1.021, TAw acc= 65.2% |
| Epoch  60, lr=8.8e-03 time=  3.4s/  2.0s | Train: loss=1.272, TAw acc= 56.4% | Valid: time=  0.4s loss=0.946, TAw acc= 66.6% |
== Rank Reduction [task:1] ==
Debug-0:
  best_loss=0.931,   best_acc=0.676
loss_margin=0.006, acc_margin=0.004
sow: rank=1024, freezed_rank=736
 a:        ['-6.23e+00', '+4.96e+00'] .... ['+5.58e+00', '+6.13e+00'] .... ['+1.29e+00', '-2.74e+00']
 a_backup: ['-6.23e+00', '+4.96e+00'] .... ['+5.58e+00', '+6.13e+00'] .... ['+1.29e+00', '-2.74e+00']
 r=256, loss=2.032, acc=0.206 (rank=256)
 r=288, loss=1.865, acc=0.312 (rank=288)
 r=320, loss=1.713, acc=0.368 (rank=320)
 r=352, loss=1.566, acc=0.464 (rank=352)
 r=384, loss=1.460, acc=0.488 (rank=384)
 r=416, loss=1.265, acc=0.534 (rank=416)
 r=448, loss=1.156, acc=0.590 (rank=448)
 r=480, loss=1.077, acc=0.620 (rank=480)
 r=512, loss=0.955, acc=0.674 (rank=512)
 r=544, loss=0.957, acc=0.680 (rank=544)
 r=576, loss=0.958, acc=0.672 (rank=576)
 r=608, loss=0.960, acc=0.666 (rank=608)
 r=640, loss=0.957, acc=0.678 (rank=640)
 r=672, loss=0.957, acc=0.668 (rank=672)
 r=704, loss=0.952, acc=0.666 (rank=704)
 r=736, loss=0.945, acc=0.672 (rank=736)
 r=768, loss=0.936, acc=0.670 (rank=768)
 r=800, loss=0.937, acc=0.674 (rank=800)
 r=832, loss=0.933, acc=0.672 (rank=832)
 best_r=832, loss=0.933, acc=0.672
== Header Training for Low Rank [task:1] ==
Fix_Classifier: Done (training=False, fix_classifier=True)
Debug-1: loss=0.933, acc=0.672
sow: rank=832, freezed_rank=832
 a:        ['-6.23e+00', '+4.96e+00'] .... ['+5.41e+00', '-1.80e+308'] .... ['-1.80e+308', '-1.80e+308']
 a_backup: ['-6.23e+00', '+4.96e+00'] .... ['+5.41e+00', '+4.99e+00'] .... ['+1.29e+00', '-2.74e+00']
SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.008767991699104466
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0

Parameter Group 1
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.05
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0
)
 Group[0]  param_num=161, free_param_num=2
 Group[1]  param_num=3, free_param_num=0
sow: rank=832, freezed_rank=832
 a:        ['-6.23e+00', '+4.96e+00'] .... ['+5.41e+00', '-1.80e+308'] .... ['-1.80e+308', '-1.80e+308']
 a_backup: ['-6.23e+00', '+4.96e+00'] .... ['+5.41e+00', '+4.99e+00'] .... ['+1.29e+00', '-2.74e+00']
| Epoch   1, lr=8.8e-03 time=  2.4s/  1.9s | Train: loss=1.288, TAw acc= 57.6% | Valid: time=  0.4s loss=0.974, TAw acc= 66.6% |
| Epoch   2, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=1.305, TAw acc= 56.2% | Valid: time=  0.5s loss=0.972, TAw acc= 66.0% |
| Epoch   3, lr=8.8e-03 time=  2.1s/  2.4s | Train: loss=1.291, TAw acc= 55.3% | Valid: time=  0.5s loss=0.983, TAw acc= 65.0% |
| Epoch   4, lr=8.8e-03 time=  2.0s/  1.9s | Train: loss=1.269, TAw acc= 56.8% | Valid: time=  0.5s loss=0.969, TAw acc= 65.6% |
| Epoch   5, lr=8.8e-03 time=  2.4s/  1.9s | Train: loss=1.249, TAw acc= 57.5% | Valid: time=  0.4s loss=0.982, TAw acc= 65.6% |
| Epoch   6, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=1.272, TAw acc= 56.2% | Valid: time=  0.4s loss=0.958, TAw acc= 66.8% |
| Epoch   7, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=1.260, TAw acc= 57.1% | Valid: time=  0.4s loss=0.955, TAw acc= 66.6% |
| Epoch   8, lr=8.8e-03 time=  1.7s/  3.1s | Train: loss=1.246, TAw acc= 57.2% | Valid: time=  0.4s loss=0.967, TAw acc= 66.6% |
| Epoch   9, lr=8.8e-03 time=  1.7s/  2.1s | Train: loss=1.230, TAw acc= 58.4% | Valid: time=  0.4s loss=0.969, TAw acc= 66.0% |
| Epoch  10, lr=8.8e-03 time=  1.7s/  2.1s | Train: loss=1.263, TAw acc= 57.0% | Valid: time=  0.4s loss=0.982, TAw acc= 66.0% |
| Epoch  11, lr=8.8e-03 time=  1.8s/  2.2s | Train: loss=1.244, TAw acc= 58.8% | Valid: time=  0.4s loss=0.978, TAw acc= 66.8% |
| Epoch  12, lr=8.8e-03 time=  1.9s/  2.0s | Train: loss=1.254, TAw acc= 56.9% | Valid: time=  0.4s loss=0.980, TAw acc= 67.0% |
| Epoch  13, lr=8.8e-03 time=  1.8s/  1.9s | Train: loss=1.227, TAw acc= 58.0% | Valid: time=  0.4s loss=0.975, TAw acc= 66.8% |
| Epoch  14, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=1.240, TAw acc= 58.5% | Valid: time=  0.4s loss=0.969, TAw acc= 66.2% |
| Epoch  15, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=1.232, TAw acc= 57.8% | Valid: time=  0.7s loss=0.959, TAw acc= 66.2% |
| Epoch  16, lr=8.8e-03 time=  2.4s/  1.9s | Train: loss=1.232, TAw acc= 57.5% | Valid: time=  0.4s loss=0.969, TAw acc= 66.8% |
| Epoch  17, lr=8.8e-03 time=  2.1s/  2.4s | Train: loss=1.222, TAw acc= 58.3% | Valid: time=  0.5s loss=0.964, TAw acc= 67.0% |
| Epoch  18, lr=8.8e-03 time=  2.1s/  2.4s | Train: loss=1.222, TAw acc= 58.4% | Valid: time=  0.5s loss=0.962, TAw acc= 66.4% |
| Epoch  19, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=1.226, TAw acc= 58.0% | Valid: time=  0.5s loss=0.975, TAw acc= 67.0% |
| Epoch  20, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=1.205, TAw acc= 58.4% | Valid: time=  0.5s loss=0.972, TAw acc= 67.4% |
| Epoch  21, lr=8.8e-03 time=  2.1s/  2.4s | Train: loss=1.224, TAw acc= 59.1% | Valid: time=  0.5s loss=0.974, TAw acc= 66.0% |
| Epoch  22, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=1.235, TAw acc= 57.0% | Valid: time=  0.5s loss=0.968, TAw acc= 67.6% |
| Epoch  23, lr=8.8e-03 time=  1.8s/  1.9s | Train: loss=1.231, TAw acc= 57.4% | Valid: time=  0.4s loss=0.967, TAw acc= 67.2% |
| Epoch  24, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=1.196, TAw acc= 59.2% | Valid: time=  0.4s loss=0.955, TAw acc= 67.0% |
| Epoch  25, lr=8.8e-03 time=  1.7s/  3.0s | Train: loss=1.216, TAw acc= 58.2% | Valid: time=  0.4s loss=0.968, TAw acc= 66.6% |
| Epoch  26, lr=8.8e-03 time=  1.7s/  2.4s | Train: loss=1.214, TAw acc= 58.4% | Valid: time=  0.5s loss=0.971, TAw acc= 67.0% |
| Epoch  27, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=1.190, TAw acc= 59.5% | Valid: time=  0.5s loss=0.958, TAw acc= 66.8% |
| Epoch  28, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=1.212, TAw acc= 59.0% | Valid: time=  0.5s loss=0.965, TAw acc= 68.0% |
| Epoch  29, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=1.203, TAw acc= 59.4% | Valid: time=  0.5s loss=0.965, TAw acc= 66.6% |
| Epoch  30, lr=8.8e-03 time=  2.1s/  2.4s | Train: loss=1.203, TAw acc= 58.3% | Valid: time=  0.4s loss=0.956, TAw acc= 67.6% | lr=2.9e-03
| Epoch  31, lr=2.9e-03 time=  1.8s/  1.9s | Train: loss=1.300, TAw acc= 56.8% | Valid: time=  0.5s loss=0.957, TAw acc= 65.4% |
| Epoch  32, lr=2.9e-03 time=  1.7s/  2.5s | Train: loss=1.314, TAw acc= 56.2% | Valid: time=  0.4s loss=0.971, TAw acc= 66.0% |
| Epoch  33, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=1.264, TAw acc= 58.0% | Valid: time=  0.4s loss=0.978, TAw acc= 66.0% |
| Epoch  34, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=1.290, TAw acc= 57.5% | Valid: time=  0.5s loss=0.968, TAw acc= 66.0% |
| Epoch  35, lr=2.9e-03 time=  1.7s/  2.2s | Train: loss=1.279, TAw acc= 56.8% | Valid: time=  0.9s loss=0.966, TAw acc= 66.4% |
| Epoch  36, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=1.284, TAw acc= 56.6% | Valid: time=  0.5s loss=0.971, TAw acc= 66.0% |
| Epoch  37, lr=2.9e-03 time=  2.2s/  2.4s | Train: loss=1.278, TAw acc= 57.1% | Valid: time=  0.5s loss=0.968, TAw acc= 66.2% |
| Epoch  38, lr=2.9e-03 time=  2.2s/  2.4s | Train: loss=1.285, TAw acc= 56.9% | Valid: time=  0.5s loss=0.964, TAw acc= 66.4% |
| Epoch  39, lr=2.9e-03 time=  2.2s/  2.4s | Train: loss=1.275, TAw acc= 57.5% | Valid: time=  0.5s loss=0.969, TAw acc= 66.2% |
| Epoch  40, lr=2.9e-03 time=  2.0s/  2.1s | Train: loss=1.269, TAw acc= 58.5% | Valid: time=  0.5s loss=0.965, TAw acc= 66.2% |
| Epoch  41, lr=2.9e-03 time=  1.8s/  1.9s | Train: loss=1.275, TAw acc= 56.6% | Valid: time=  0.4s loss=0.972, TAw acc= 66.4% |
| Epoch  42, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=1.257, TAw acc= 58.0% | Valid: time=  0.4s loss=0.966, TAw acc= 66.6% |
| Epoch  43, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=1.268, TAw acc= 57.5% | Valid: time=  0.4s loss=0.972, TAw acc= 66.8% |
| Epoch  44, lr=2.9e-03 time=  2.7s/  1.9s | Train: loss=1.266, TAw acc= 57.4% | Valid: time=  0.4s loss=0.970, TAw acc= 65.8% |
| Epoch  45, lr=2.9e-03 time=  1.8s/  2.5s | Train: loss=1.268, TAw acc= 57.3% | Valid: time=  0.5s loss=0.965, TAw acc= 65.4% |
| Epoch  46, lr=2.9e-03 time=  2.2s/  2.4s | Train: loss=1.250, TAw acc= 57.4% | Valid: time=  0.5s loss=0.963, TAw acc= 65.6% |
| Epoch  47, lr=2.9e-03 time=  1.8s/  1.9s | Train: loss=1.262, TAw acc= 57.1% | Valid: time=  0.4s loss=0.967, TAw acc= 66.2% |
| Epoch  48, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=1.259, TAw acc= 57.3% | Valid: time=  0.4s loss=0.973, TAw acc= 66.2% |
| Epoch  49, lr=2.9e-03 time=  1.7s/  2.0s | Train: loss=1.255, TAw acc= 58.0% | Valid: time=  0.4s loss=0.967, TAw acc= 66.2% |
| Epoch  50, lr=2.9e-03 time=  2.3s/  2.3s | Train: loss=1.241, TAw acc= 58.4% | Valid: time=  0.4s loss=0.970, TAw acc= 66.2% |
| Epoch  51, lr=2.9e-03 time=  1.7s/  2.5s | Train: loss=1.276, TAw acc= 56.6% | Valid: time=  0.5s loss=0.973, TAw acc= 66.0% |
| Epoch  52, lr=2.9e-03 time=  2.2s/  2.4s | Train: loss=1.270, TAw acc= 57.0% | Valid: time=  0.5s loss=0.973, TAw acc= 66.6% |
| Epoch  53, lr=2.9e-03 time=  2.2s/  2.2s | Train: loss=1.260, TAw acc= 57.7% | Valid: time=  0.4s loss=0.967, TAw acc= 66.6% |
| Epoch  54, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=1.264, TAw acc= 57.1% | Valid: time=  0.4s loss=0.964, TAw acc= 67.4% |
| Epoch  55, lr=2.9e-03 time=  1.7s/  2.8s | Train: loss=1.262, TAw acc= 57.0% | Valid: time=  0.4s loss=0.966, TAw acc= 66.6% |
| Epoch  56, lr=2.9e-03 time=  1.7s/  2.4s | Train: loss=1.263, TAw acc= 57.7% | Valid: time=  0.5s loss=0.968, TAw acc= 65.4% |
| Epoch  57, lr=2.9e-03 time=  2.2s/  2.4s | Train: loss=1.244, TAw acc= 57.6% | Valid: time=  0.5s loss=0.965, TAw acc= 65.8% |
| Epoch  58, lr=2.9e-03 time=  2.2s/  2.3s | Train: loss=1.249, TAw acc= 57.5% | Valid: time=  0.4s loss=0.969, TAw acc= 66.8% |
| Epoch  59, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=1.248, TAw acc= 57.3% | Valid: time=  0.5s loss=0.972, TAw acc= 67.4% |
| Epoch  60, lr=2.9e-03 time=  2.1s/  2.0s | Train: loss=1.242, TAw acc= 58.7% | Valid: time=  0.4s loss=0.971, TAw acc= 66.2% | lr=9.7e-04
Debug-2: loss=0.933, acc=0.672
sow: rank=832, freezed_rank=832
 a:        ['-6.23e+00', '+4.96e+00'] .... ['+5.41e+00', '-1.80e+308'] .... ['-1.80e+308', '-1.80e+308']
 a_backup: ['-6.23e+00', '+4.96e+00'] .... ['+5.41e+00', '+4.99e+00'] .... ['+1.29e+00', '-2.74e+00']
Free_Classifier: Done (training=True, fix_classifier=False)
Debug-3: loss=0.931 acc=0.676
sow: rank=1024, freezed_rank=832
 a:        ['-6.23e+00', '+4.96e+00'] .... ['+5.41e+00', '+4.99e+00'] .... ['+1.29e+00', '-2.74e+00']
 a_backup: ['-6.23e+00', '+4.96e+00'] .... ['+5.41e+00', '+4.99e+00'] .... ['+1.29e+00', '-2.74e+00']
Header Training: Finished.
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.649 | TAw acc= 79.2%, forg=  0.0%| TAg acc= 56.4%, forg= 22.8% <<<
>>> Test on task  1 : loss=0.998 | TAw acc= 66.9%, forg=  0.0%| TAg acc= 52.2%, forg=  0.0% <<<
Save at ../RESULT_AAAI2025/CR10/0/cifar100_sow
************************************************************************************************************
Task  2
************************************************************************************************************
LLL_Net(
  (model): ResNet50_32SOW(
    in_channels=3, in_H=32, in_W=32, training=False, fix_features=True, fix_classifier=False
    (features): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
      (3): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (4): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (3): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (5): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (3): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (4): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (5): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (6): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (7): AdaptiveAvgPool2d(output_size=(1, 1))
    )
    (classifier): Sequential(
      (0): Dropout(p=0.417598542370663, inplace=False)
      (1): SOW_V3(in_features=1024, out_features=1024, num_tasks=10, dtype=torch.float64, device=cuda:0)
      (2): ReLU()
    )
    (fc): Sequential()
  )
  (heads): ModuleList(
    (0-2): 3 x Linear(in_features=1024, out_features=10, bias=True)
  )
)
SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.0263039750973134
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0

Parameter Group 1
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.05
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0
)
 Group[0]  param_num=161, free_param_num=2
 Group[1]  param_num=3, free_param_num=3
sow: rank=1024, freezed_rank=832
 a:        ['-6.23e+00', '+4.96e+00'] .... ['+5.41e+00', '+4.99e+00'] .... ['+1.29e+00', '-2.74e+00']
 a_backup: ['-6.23e+00', '+4.96e+00'] .... ['+5.41e+00', '+4.99e+00'] .... ['+1.29e+00', '-2.74e+00']
| Epoch   1, lr=2.6e-02 time=  2.8s/  2.7s | Train: loss=1.190, TAw acc= 60.4% | Valid: time=  0.5s loss=0.809, TAw acc= 72.4% | *
| Epoch   2, lr=2.6e-02 time=  2.9s/  1.9s | Train: loss=1.096, TAw acc= 62.9% | Valid: time=  0.4s loss=0.733, TAw acc= 74.6% | *
| Epoch   3, lr=2.6e-02 time=  2.7s/  1.9s | Train: loss=1.070, TAw acc= 64.5% | Valid: time=  0.4s loss=0.653, TAw acc= 77.2% | *
| Epoch   4, lr=2.6e-02 time=  2.8s/  3.1s | Train: loss=1.084, TAw acc= 63.3% | Valid: time=  0.4s loss=0.691, TAw acc= 75.0% |
| Epoch   5, lr=2.6e-02 time=  3.0s/  2.4s | Train: loss=1.071, TAw acc= 64.2% | Valid: time=  0.5s loss=0.678, TAw acc= 76.4% |
| Epoch   6, lr=2.6e-02 time=  3.6s/  2.4s | Train: loss=1.040, TAw acc= 66.0% | Valid: time=  0.4s loss=0.656, TAw acc= 78.0% |
| Epoch   7, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=1.064, TAw acc= 65.9% | Valid: time=  0.5s loss=0.641, TAw acc= 77.8% | *
| Epoch   8, lr=2.6e-02 time=  3.2s/  1.9s | Train: loss=0.997, TAw acc= 66.0% | Valid: time=  0.4s loss=0.654, TAw acc= 76.8% |
| Epoch   9, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=1.035, TAw acc= 65.3% | Valid: time=  0.7s loss=0.635, TAw acc= 78.2% | *
| Epoch  10, lr=2.6e-02 time=  3.5s/  2.1s | Train: loss=1.023, TAw acc= 66.9% | Valid: time=  0.5s loss=0.587, TAw acc= 78.8% | *
| Epoch  11, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.016, TAw acc= 65.7% | Valid: time=  0.5s loss=0.659, TAw acc= 75.4% |
| Epoch  12, lr=2.6e-02 time=  3.4s/  2.1s | Train: loss=1.002, TAw acc= 66.9% | Valid: time=  0.4s loss=0.617, TAw acc= 78.4% |
| Epoch  13, lr=2.6e-02 time=  2.8s/  2.0s | Train: loss=0.987, TAw acc= 67.2% | Valid: time=  0.4s loss=0.623, TAw acc= 78.0% |
| Epoch  14, lr=2.6e-02 time=  4.0s/  1.9s | Train: loss=1.013, TAw acc= 65.9% | Valid: time=  0.4s loss=0.627, TAw acc= 76.2% |
| Epoch  15, lr=2.6e-02 time=  3.5s/  2.5s | Train: loss=1.025, TAw acc= 66.0% | Valid: time=  0.5s loss=0.631, TAw acc= 76.2% |
| Epoch  16, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.010, TAw acc= 67.2% | Valid: time=  0.5s loss=0.562, TAw acc= 78.4% | *
| Epoch  17, lr=2.6e-02 time=  2.9s/  1.9s | Train: loss=0.968, TAw acc= 67.5% | Valid: time=  0.4s loss=0.600, TAw acc= 79.2% |
| Epoch  18, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=0.981, TAw acc= 66.6% | Valid: time=  0.4s loss=0.604, TAw acc= 79.8% |
| Epoch  19, lr=2.6e-02 time=  4.0s/  1.9s | Train: loss=0.992, TAw acc= 67.6% | Valid: time=  0.4s loss=0.607, TAw acc= 77.4% |
| Epoch  20, lr=2.6e-02 time=  2.7s/  1.9s | Train: loss=0.992, TAw acc= 66.9% | Valid: time=  0.4s loss=0.602, TAw acc= 79.0% |
| Epoch  21, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=0.972, TAw acc= 68.2% | Valid: time=  0.9s loss=0.624, TAw acc= 77.6% |
| Epoch  22, lr=2.6e-02 time=  3.3s/  2.1s | Train: loss=0.941, TAw acc= 68.6% | Valid: time=  0.5s loss=0.638, TAw acc= 78.4% |
| Epoch  23, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.965, TAw acc= 68.8% | Valid: time=  0.5s loss=0.637, TAw acc= 77.8% |
| Epoch  24, lr=2.6e-02 time=  3.6s/  1.9s | Train: loss=0.979, TAw acc= 66.9% | Valid: time=  0.4s loss=0.585, TAw acc= 79.4% |
| Epoch  25, lr=2.6e-02 time=  3.1s/  2.1s | Train: loss=0.951, TAw acc= 68.2% | Valid: time=  0.4s loss=0.604, TAw acc= 78.0% |
| Epoch  26, lr=2.6e-02 time=  3.0s/  2.1s | Train: loss=0.979, TAw acc= 67.6% | Valid: time=  0.5s loss=0.678, TAw acc= 76.4% |
| Epoch  27, lr=2.6e-02 time=  3.0s/  1.9s | Train: loss=0.989, TAw acc= 67.6% | Valid: time=  0.5s loss=0.599, TAw acc= 80.2% |
| Epoch  28, lr=2.6e-02 time=  2.9s/  1.9s | Train: loss=0.962, TAw acc= 68.1% | Valid: time=  0.4s loss=0.588, TAw acc= 79.6% |
| Epoch  29, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=0.976, TAw acc= 68.5% | Valid: time=  0.4s loss=0.609, TAw acc= 78.8% |
| Epoch  30, lr=2.6e-02 time=  4.0s/  1.9s | Train: loss=0.947, TAw acc= 67.8% | Valid: time=  0.4s loss=0.629, TAw acc= 78.0% |
| Epoch  31, lr=2.6e-02 time=  3.4s/  2.4s | Train: loss=0.963, TAw acc= 67.9% | Valid: time=  0.5s loss=0.589, TAw acc= 79.0% |
| Epoch  32, lr=2.6e-02 time=  3.6s/  2.4s | Train: loss=0.943, TAw acc= 67.8% | Valid: time=  0.5s loss=0.577, TAw acc= 79.8% |
| Epoch  33, lr=2.6e-02 time=  3.6s/  2.4s | Train: loss=0.950, TAw acc= 68.8% | Valid: time=  0.5s loss=0.596, TAw acc= 79.4% |
| Epoch  34, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=0.991, TAw acc= 66.7% | Valid: time=  0.4s loss=0.608, TAw acc= 78.8% |
| Epoch  35, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=0.943, TAw acc= 68.2% | Valid: time=  0.9s loss=0.641, TAw acc= 78.2% |
| Epoch  36, lr=2.6e-02 time=  3.0s/  2.2s | Train: loss=0.948, TAw acc= 68.2% | Valid: time=  0.5s loss=0.644, TAw acc= 77.4% |
| Epoch  37, lr=2.6e-02 time=  3.6s/  2.3s | Train: loss=0.936, TAw acc= 68.2% | Valid: time=  0.4s loss=0.635, TAw acc= 77.2% |
| Epoch  38, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=0.931, TAw acc= 68.6% | Valid: time=  0.4s loss=0.618, TAw acc= 78.4% |
| Epoch  39, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=0.953, TAw acc= 67.1% | Valid: time=  0.4s loss=0.640, TAw acc= 77.2% |
| Epoch  40, lr=2.6e-02 time=  4.0s/  2.0s | Train: loss=0.952, TAw acc= 68.8% | Valid: time=  0.5s loss=0.603, TAw acc= 78.0% |
| Epoch  41, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.930, TAw acc= 68.5% | Valid: time=  0.5s loss=0.591, TAw acc= 78.6% |
| Epoch  42, lr=2.6e-02 time=  3.7s/  2.4s | Train: loss=0.967, TAw acc= 68.7% | Valid: time=  0.5s loss=0.570, TAw acc= 79.8% |
| Epoch  43, lr=2.6e-02 time=  3.2s/  1.9s | Train: loss=0.949, TAw acc= 68.5% | Valid: time=  0.4s loss=0.568, TAw acc= 81.4% |
| Epoch  44, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=0.965, TAw acc= 67.7% | Valid: time=  0.4s loss=0.631, TAw acc= 78.0% |
| Epoch  45, lr=2.6e-02 time=  4.0s/  1.9s | Train: loss=0.939, TAw acc= 68.8% | Valid: time=  0.4s loss=0.614, TAw acc= 78.4% |
| Epoch  46, lr=2.6e-02 time=  3.4s/  2.4s | Train: loss=0.934, TAw acc= 69.1% | Valid: time=  0.5s loss=0.588, TAw acc= 78.8% | lr=8.8e-03
| Epoch  47, lr=8.8e-03 time=  3.6s/  2.2s | Train: loss=0.968, TAw acc= 67.5% | Valid: time=  0.4s loss=0.597, TAw acc= 79.0% |
| Epoch  48, lr=8.8e-03 time=  2.8s/  1.9s | Train: loss=0.954, TAw acc= 68.9% | Valid: time=  0.4s loss=0.582, TAw acc= 79.8% |
| Epoch  49, lr=8.8e-03 time=  2.8s/  1.9s | Train: loss=0.960, TAw acc= 68.2% | Valid: time=  0.4s loss=0.603, TAw acc= 77.4% |
| Epoch  50, lr=8.8e-03 time=  4.0s/  1.9s | Train: loss=0.922, TAw acc= 68.5% | Valid: time=  0.4s loss=0.619, TAw acc= 77.6% |
| Epoch  51, lr=8.8e-03 time=  3.5s/  2.4s | Train: loss=0.945, TAw acc= 69.1% | Valid: time=  0.5s loss=0.587, TAw acc= 78.8% |
| Epoch  52, lr=8.8e-03 time=  3.6s/  2.5s | Train: loss=0.960, TAw acc= 68.7% | Valid: time=  0.5s loss=0.572, TAw acc= 78.8% |
| Epoch  53, lr=8.8e-03 time=  3.5s/  1.9s | Train: loss=0.958, TAw acc= 68.1% | Valid: time=  0.4s loss=0.576, TAw acc= 79.4% |
| Epoch  54, lr=8.8e-03 time=  2.8s/  1.9s | Train: loss=0.938, TAw acc= 69.0% | Valid: time=  0.6s loss=0.607, TAw acc= 79.2% |
| Epoch  55, lr=8.8e-03 time=  3.5s/  2.1s | Train: loss=0.969, TAw acc= 68.1% | Valid: time=  0.5s loss=0.588, TAw acc= 79.4% |
| Epoch  56, lr=8.8e-03 time=  3.6s/  2.4s | Train: loss=0.943, TAw acc= 69.2% | Valid: time=  0.5s loss=0.604, TAw acc= 78.8% |
| Epoch  57, lr=8.8e-03 time=  3.6s/  2.4s | Train: loss=0.942, TAw acc= 69.2% | Valid: time=  0.4s loss=0.593, TAw acc= 79.0% |
| Epoch  58, lr=8.8e-03 time=  2.8s/  1.9s | Train: loss=0.938, TAw acc= 68.7% | Valid: time=  0.4s loss=0.565, TAw acc= 80.2% |
| Epoch  59, lr=8.8e-03 time=  2.8s/  1.9s | Train: loss=0.932, TAw acc= 68.4% | Valid: time=  0.4s loss=0.605, TAw acc= 79.2% |
| Epoch  60, lr=8.8e-03 time=  4.0s/  1.9s | Train: loss=0.950, TAw acc= 68.3% | Valid: time=  0.4s loss=0.561, TAw acc= 80.0% | *
== Rank Reduction [task:2] ==
Debug-0:
  best_loss=0.561,   best_acc=0.800
loss_margin=0.006, acc_margin=0.004
sow: rank=1024, freezed_rank=832
 a:        ['-5.89e+00', '+4.96e+00'] .... ['+5.42e+00', '+5.01e+00'] .... ['+1.29e+00', '-2.74e+00']
 a_backup: ['-5.89e+00', '+4.96e+00'] .... ['+5.42e+00', '+5.01e+00'] .... ['+1.29e+00', '-2.74e+00']
 r=256, loss=1.805, acc=0.318 (rank=256)
 r=288, loss=1.720, acc=0.302 (rank=288)
 r=320, loss=1.476, acc=0.460 (rank=320)
 r=352, loss=1.239, acc=0.602 (rank=352)
 r=384, loss=1.034, acc=0.660 (rank=384)
 r=416, loss=0.897, acc=0.700 (rank=416)
 r=448, loss=0.771, acc=0.760 (rank=448)
 r=480, loss=0.647, acc=0.784 (rank=480)
 r=512, loss=0.554, acc=0.812 (rank=512)
 best_r=512, loss=0.554, acc=0.812
== Header Training for Low Rank [task:2] ==
Fix_Classifier: Done (training=False, fix_classifier=True)
Debug-1: loss=0.554, acc=0.812
sow: rank=512, freezed_rank=832
 a:        ['-5.89e+00', '+4.96e+00'] .... ['-1.80e+308', '-1.80e+308'] .... ['-1.80e+308', '-1.80e+308']
 a_backup: ['-5.89e+00', '+4.96e+00'] .... ['+5.42e+00', '+5.01e+00'] .... ['+1.29e+00', '-2.74e+00']
SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.008767991699104466
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0

Parameter Group 1
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.05
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0
)
 Group[0]  param_num=161, free_param_num=2
 Group[1]  param_num=3, free_param_num=0
sow: rank=512, freezed_rank=832
 a:        ['-5.89e+00', '+4.96e+00'] .... ['-1.80e+308', '-1.80e+308'] .... ['-1.80e+308', '-1.80e+308']
 a_backup: ['-5.89e+00', '+4.96e+00'] .... ['+5.42e+00', '+5.01e+00'] .... ['+1.29e+00', '-2.74e+00']
| Epoch   1, lr=8.8e-03 time=  2.0s/  1.9s | Train: loss=0.993, TAw acc= 69.3% | Valid: time=  0.4s loss=0.580, TAw acc= 80.8% |
| Epoch   2, lr=8.8e-03 time=  1.7s/  2.5s | Train: loss=0.985, TAw acc= 68.0% | Valid: time=  0.4s loss=0.580, TAw acc= 80.6% |
| Epoch   3, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.990, TAw acc= 67.2% | Valid: time=  0.5s loss=0.576, TAw acc= 80.8% |
| Epoch   4, lr=8.8e-03 time=  2.5s/  1.8s | Train: loss=0.968, TAw acc= 68.4% | Valid: time=  0.4s loss=0.592, TAw acc= 79.2% |
| Epoch   5, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.942, TAw acc= 68.9% | Valid: time=  0.4s loss=0.586, TAw acc= 80.2% |
| Epoch   6, lr=8.8e-03 time=  1.6s/  1.8s | Train: loss=0.935, TAw acc= 69.5% | Valid: time=  0.4s loss=0.570, TAw acc= 81.8% |
| Epoch   7, lr=8.8e-03 time=  1.7s/  1.8s | Train: loss=0.951, TAw acc= 69.2% | Valid: time=  0.4s loss=0.577, TAw acc= 81.2% |
| Epoch   8, lr=8.8e-03 time=  2.9s/  1.8s | Train: loss=0.944, TAw acc= 68.3% | Valid: time=  0.4s loss=0.578, TAw acc= 80.4% |
| Epoch   9, lr=8.8e-03 time=  2.0s/  2.4s | Train: loss=0.951, TAw acc= 68.7% | Valid: time=  0.5s loss=0.568, TAw acc= 81.4% |
| Epoch  10, lr=8.8e-03 time=  2.1s/  2.3s | Train: loss=0.922, TAw acc= 70.1% | Valid: time=  0.5s loss=0.590, TAw acc= 79.4% |
| Epoch  11, lr=8.8e-03 time=  2.1s/  2.3s | Train: loss=0.934, TAw acc= 68.9% | Valid: time=  0.5s loss=0.570, TAw acc= 81.6% |
| Epoch  12, lr=8.8e-03 time=  2.1s/  2.4s | Train: loss=0.913, TAw acc= 69.4% | Valid: time=  0.5s loss=0.571, TAw acc= 81.4% |
| Epoch  13, lr=8.8e-03 time=  1.9s/  1.8s | Train: loss=0.932, TAw acc= 69.2% | Valid: time=  0.4s loss=0.570, TAw acc= 81.6% |
| Epoch  14, lr=8.8e-03 time=  1.6s/  1.8s | Train: loss=0.917, TAw acc= 69.0% | Valid: time=  0.4s loss=0.577, TAw acc= 81.2% |
| Epoch  15, lr=8.8e-03 time=  1.6s/  1.9s | Train: loss=0.921, TAw acc= 69.5% | Valid: time=  0.4s loss=0.579, TAw acc= 81.0% |
| Epoch  16, lr=8.8e-03 time=  1.6s/  3.1s | Train: loss=0.908, TAw acc= 69.8% | Valid: time=  0.4s loss=0.577, TAw acc= 81.8% |
| Epoch  17, lr=8.8e-03 time=  1.7s/  2.2s | Train: loss=0.897, TAw acc= 69.8% | Valid: time=  0.5s loss=0.562, TAw acc= 81.4% |
| Epoch  18, lr=8.8e-03 time=  2.1s/  2.4s | Train: loss=0.914, TAw acc= 69.9% | Valid: time=  0.5s loss=0.565, TAw acc= 82.0% |
| Epoch  19, lr=8.8e-03 time=  2.1s/  2.3s | Train: loss=0.897, TAw acc= 70.7% | Valid: time=  0.5s loss=0.570, TAw acc= 81.4% |
| Epoch  20, lr=8.8e-03 time=  1.9s/  1.8s | Train: loss=0.901, TAw acc= 70.4% | Valid: time=  0.4s loss=0.577, TAw acc= 80.6% |
| Epoch  21, lr=8.8e-03 time=  1.6s/  1.8s | Train: loss=0.897, TAw acc= 70.1% | Valid: time=  0.4s loss=0.569, TAw acc= 81.2% |
| Epoch  22, lr=8.8e-03 time=  1.6s/  1.9s | Train: loss=0.904, TAw acc= 69.1% | Valid: time=  0.4s loss=0.570, TAw acc= 81.6% |
| Epoch  23, lr=8.8e-03 time=  1.7s/  3.1s | Train: loss=0.898, TAw acc= 70.1% | Valid: time=  0.4s loss=0.562, TAw acc= 81.8% |
| Epoch  24, lr=8.8e-03 time=  1.6s/  2.2s | Train: loss=0.893, TAw acc= 70.6% | Valid: time=  0.5s loss=0.585, TAw acc= 80.8% |
| Epoch  25, lr=8.8e-03 time=  2.1s/  2.4s | Train: loss=0.876, TAw acc= 70.8% | Valid: time=  0.5s loss=0.575, TAw acc= 80.4% |
| Epoch  26, lr=8.8e-03 time=  2.1s/  2.1s | Train: loss=0.883, TAw acc= 70.5% | Valid: time=  0.4s loss=0.575, TAw acc= 80.6% |
| Epoch  27, lr=8.8e-03 time=  1.6s/  2.3s | Train: loss=0.872, TAw acc= 71.6% | Valid: time=  0.4s loss=0.576, TAw acc= 81.2% |
| Epoch  28, lr=8.8e-03 time=  1.7s/  1.8s | Train: loss=0.868, TAw acc= 71.7% | Valid: time=  0.4s loss=0.562, TAw acc= 81.6% |
| Epoch  29, lr=8.8e-03 time=  2.4s/  1.8s | Train: loss=0.865, TAw acc= 71.6% | Valid: time=  0.4s loss=0.565, TAw acc= 82.0% |
| Epoch  30, lr=8.8e-03 time=  1.6s/  2.2s | Train: loss=0.866, TAw acc= 71.2% | Valid: time=  0.4s loss=0.561, TAw acc= 82.0% | lr=2.9e-03
| Epoch  31, lr=2.9e-03 time=  1.7s/  2.1s | Train: loss=0.997, TAw acc= 68.1% | Valid: time=  0.4s loss=0.589, TAw acc= 80.0% |
| Epoch  32, lr=2.9e-03 time=  1.6s/  1.9s | Train: loss=0.997, TAw acc= 67.6% | Valid: time=  0.4s loss=0.581, TAw acc= 80.2% |
| Epoch  33, lr=2.9e-03 time=  1.7s/  3.1s | Train: loss=0.985, TAw acc= 68.9% | Valid: time=  0.4s loss=0.583, TAw acc= 80.0% |
| Epoch  34, lr=2.9e-03 time=  1.6s/  2.1s | Train: loss=0.985, TAw acc= 68.5% | Valid: time=  0.5s loss=0.576, TAw acc= 80.6% |
| Epoch  35, lr=2.9e-03 time=  2.1s/  2.4s | Train: loss=0.984, TAw acc= 68.4% | Valid: time=  0.5s loss=0.576, TAw acc= 80.6% |
| Epoch  36, lr=2.9e-03 time=  2.1s/  2.4s | Train: loss=0.975, TAw acc= 69.1% | Valid: time=  0.5s loss=0.574, TAw acc= 80.2% |
| Epoch  37, lr=2.9e-03 time=  2.1s/  1.8s | Train: loss=0.965, TAw acc= 68.5% | Valid: time=  0.4s loss=0.576, TAw acc= 80.2% |
| Epoch  38, lr=2.9e-03 time=  1.6s/  1.8s | Train: loss=0.976, TAw acc= 68.3% | Valid: time=  0.4s loss=0.573, TAw acc= 79.8% |
| Epoch  39, lr=2.9e-03 time=  1.6s/  1.8s | Train: loss=0.971, TAw acc= 68.7% | Valid: time=  0.4s loss=0.579, TAw acc= 80.0% |
| Epoch  40, lr=2.9e-03 time=  1.8s/  2.6s | Train: loss=0.965, TAw acc= 68.8% | Valid: time=  0.5s loss=0.571, TAw acc= 81.0% |
| Epoch  41, lr=2.9e-03 time=  1.6s/  2.4s | Train: loss=0.963, TAw acc= 69.1% | Valid: time=  0.5s loss=0.571, TAw acc= 80.8% |
| Epoch  42, lr=2.9e-03 time=  2.1s/  2.3s | Train: loss=0.957, TAw acc= 69.3% | Valid: time=  0.5s loss=0.568, TAw acc= 80.8% |
| Epoch  43, lr=2.9e-03 time=  2.1s/  2.3s | Train: loss=0.980, TAw acc= 68.6% | Valid: time=  0.5s loss=0.571, TAw acc= 80.4% |
| Epoch  44, lr=2.9e-03 time=  2.1s/  1.8s | Train: loss=0.967, TAw acc= 68.3% | Valid: time=  0.4s loss=0.565, TAw acc= 80.6% |
| Epoch  45, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=0.944, TAw acc= 69.2% | Valid: time=  0.4s loss=0.569, TAw acc= 80.2% |
| Epoch  46, lr=2.9e-03 time=  1.6s/  1.8s | Train: loss=0.954, TAw acc= 69.1% | Valid: time=  0.4s loss=0.566, TAw acc= 80.8% |
| Epoch  47, lr=2.9e-03 time=  1.6s/  1.9s | Train: loss=0.933, TAw acc= 69.5% | Valid: time=  0.4s loss=0.569, TAw acc= 80.0% |
| Epoch  48, lr=2.9e-03 time=  2.8s/  1.8s | Train: loss=0.962, TAw acc= 68.6% | Valid: time=  0.4s loss=0.572, TAw acc= 80.4% |
| Epoch  49, lr=2.9e-03 time=  1.8s/  2.4s | Train: loss=0.943, TAw acc= 68.7% | Valid: time=  0.5s loss=0.570, TAw acc= 80.2% |
| Epoch  50, lr=2.9e-03 time=  2.1s/  2.4s | Train: loss=0.944, TAw acc= 68.2% | Valid: time=  0.5s loss=0.572, TAw acc= 80.4% |
| Epoch  51, lr=2.9e-03 time=  2.1s/  2.1s | Train: loss=0.953, TAw acc= 68.7% | Valid: time=  0.4s loss=0.570, TAw acc= 80.0% |
| Epoch  52, lr=2.9e-03 time=  1.6s/  1.8s | Train: loss=0.931, TAw acc= 69.6% | Valid: time=  0.4s loss=0.569, TAw acc= 80.4% |
| Epoch  53, lr=2.9e-03 time=  1.7s/  1.8s | Train: loss=0.916, TAw acc= 70.3% | Valid: time=  0.9s loss=0.573, TAw acc= 80.6% |
| Epoch  54, lr=2.9e-03 time=  1.6s/  1.9s | Train: loss=0.933, TAw acc= 69.3% | Valid: time=  0.4s loss=0.575, TAw acc= 79.8% |
| Epoch  55, lr=2.9e-03 time=  2.0s/  1.9s | Train: loss=0.925, TAw acc= 69.1% | Valid: time=  0.4s loss=0.573, TAw acc= 81.0% |
| Epoch  56, lr=2.9e-03 time=  1.6s/  1.8s | Train: loss=0.928, TAw acc= 68.6% | Valid: time=  0.4s loss=0.573, TAw acc= 80.4% |
| Epoch  57, lr=2.9e-03 time=  1.7s/  1.8s | Train: loss=0.946, TAw acc= 68.4% | Valid: time=  0.4s loss=0.562, TAw acc= 81.0% |
| Epoch  58, lr=2.9e-03 time=  2.9s/  1.8s | Train: loss=0.938, TAw acc= 69.2% | Valid: time=  0.4s loss=0.571, TAw acc= 80.2% |
| Epoch  59, lr=2.9e-03 time=  1.9s/  2.3s | Train: loss=0.945, TAw acc= 68.7% | Valid: time=  0.5s loss=0.573, TAw acc= 79.8% |
| Epoch  60, lr=2.9e-03 time=  2.1s/  2.4s | Train: loss=0.928, TAw acc= 69.0% | Valid: time=  0.5s loss=0.572, TAw acc= 80.2% | lr=9.7e-04
Debug-2: loss=0.554, acc=0.812
sow: rank=512, freezed_rank=832
 a:        ['-5.89e+00', '+4.96e+00'] .... ['-1.80e+308', '-1.80e+308'] .... ['-1.80e+308', '-1.80e+308']
 a_backup: ['-5.89e+00', '+4.96e+00'] .... ['+5.42e+00', '+5.01e+00'] .... ['+1.29e+00', '-2.74e+00']
Free_Classifier: Done (training=True, fix_classifier=False)
Debug-3: loss=0.561 acc=0.800
sow: rank=1024, freezed_rank=832
 a:        ['-5.89e+00', '+4.96e+00'] .... ['+5.42e+00', '+5.01e+00'] .... ['+1.29e+00', '-2.74e+00']
 a_backup: ['-5.89e+00', '+4.96e+00'] .... ['+5.42e+00', '+5.01e+00'] .... ['+1.29e+00', '-2.74e+00']
Header Training: Finished.
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.649 | TAw acc= 79.2%, forg=  0.0%| TAg acc= 45.1%, forg= 34.1% <<<
>>> Test on task  1 : loss=0.998 | TAw acc= 66.9%, forg=  0.0%| TAg acc= 45.3%, forg=  6.9% <<<
>>> Test on task  2 : loss=0.655 | TAw acc= 77.0%, forg=  0.0%| TAg acc= 51.9%, forg=  0.0% <<<
Save at ../RESULT_AAAI2025/CR10/0/cifar100_sow
************************************************************************************************************
Task  3
************************************************************************************************************
LLL_Net(
  (model): ResNet50_32SOW(
    in_channels=3, in_H=32, in_W=32, training=False, fix_features=True, fix_classifier=False
    (features): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
      (3): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (4): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (3): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (5): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (3): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (4): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (5): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (6): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (7): AdaptiveAvgPool2d(output_size=(1, 1))
    )
    (classifier): Sequential(
      (0): Dropout(p=0.417598542370663, inplace=False)
      (1): SOW_V3(in_features=1024, out_features=1024, num_tasks=10, dtype=torch.float64, device=cuda:0)
      (2): ReLU()
    )
    (fc): Sequential()
  )
  (heads): ModuleList(
    (0-3): 4 x Linear(in_features=1024, out_features=10, bias=True)
  )
)
SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.0263039750973134
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0

Parameter Group 1
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.05
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0
)
 Group[0]  param_num=161, free_param_num=2
 Group[1]  param_num=3, free_param_num=3
sow: rank=1024, freezed_rank=832
 a:        ['-5.89e+00', '+4.96e+00'] .... ['+5.42e+00', '+5.01e+00'] .... ['+1.29e+00', '-2.74e+00']
 a_backup: ['-5.89e+00', '+4.96e+00'] .... ['+5.42e+00', '+5.01e+00'] .... ['+1.29e+00', '-2.74e+00']
| Epoch   1, lr=2.6e-02 time=  2.9s/  2.6s | Train: loss=1.272, TAw acc= 57.2% | Valid: time=  0.4s loss=0.833, TAw acc= 69.8% | *
| Epoch   2, lr=2.6e-02 time=  2.9s/  1.9s | Train: loss=1.134, TAw acc= 60.6% | Valid: time=  0.4s loss=0.758, TAw acc= 72.0% | *
| Epoch   3, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=1.135, TAw acc= 59.8% | Valid: time=  0.4s loss=0.822, TAw acc= 69.4% |
| Epoch   4, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=1.071, TAw acc= 64.0% | Valid: time=  0.4s loss=0.719, TAw acc= 72.0% | *
| Epoch   5, lr=2.6e-02 time=  4.1s/  2.0s | Train: loss=1.106, TAw acc= 62.0% | Valid: time=  0.5s loss=0.755, TAw acc= 71.8% |
| Epoch   6, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.061, TAw acc= 63.1% | Valid: time=  0.5s loss=0.770, TAw acc= 73.0% |
| Epoch   7, lr=2.6e-02 time=  3.1s/  2.2s | Train: loss=1.088, TAw acc= 62.0% | Valid: time=  0.5s loss=0.701, TAw acc= 74.4% | *
| Epoch   8, lr=2.6e-02 time=  2.8s/  2.0s | Train: loss=1.069, TAw acc= 63.4% | Valid: time=  0.4s loss=0.729, TAw acc= 72.2% |
| Epoch   9, lr=2.6e-02 time=  2.8s/  3.1s | Train: loss=1.076, TAw acc= 63.8% | Valid: time=  0.4s loss=0.684, TAw acc= 74.0% | *
| Epoch  10, lr=2.6e-02 time=  3.1s/  2.4s | Train: loss=1.066, TAw acc= 63.2% | Valid: time=  0.5s loss=0.700, TAw acc= 74.4% |
| Epoch  11, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.065, TAw acc= 62.7% | Valid: time=  0.5s loss=0.704, TAw acc= 73.6% |
| Epoch  12, lr=2.6e-02 time=  3.0s/  2.1s | Train: loss=1.072, TAw acc= 61.8% | Valid: time=  0.5s loss=0.756, TAw acc= 73.0% |
| Epoch  13, lr=2.6e-02 time=  3.1s/  1.9s | Train: loss=1.055, TAw acc= 63.7% | Valid: time=  0.4s loss=0.665, TAw acc= 75.8% | *
| Epoch  14, lr=2.6e-02 time=  2.8s/  2.0s | Train: loss=1.027, TAw acc= 64.5% | Valid: time=  0.5s loss=0.676, TAw acc= 75.8% |
| Epoch  15, lr=2.6e-02 time=  4.1s/  1.9s | Train: loss=1.054, TAw acc= 63.0% | Valid: time=  0.5s loss=0.694, TAw acc= 73.4% |
| Epoch  16, lr=2.6e-02 time=  3.6s/  2.4s | Train: loss=1.034, TAw acc= 63.8% | Valid: time=  0.5s loss=0.755, TAw acc= 73.4% |
| Epoch  17, lr=2.6e-02 time=  3.2s/  2.0s | Train: loss=1.038, TAw acc= 64.1% | Valid: time=  0.5s loss=0.692, TAw acc= 72.4% |
| Epoch  18, lr=2.6e-02 time=  2.9s/  1.9s | Train: loss=1.027, TAw acc= 65.0% | Valid: time=  0.4s loss=0.716, TAw acc= 70.8% |
| Epoch  19, lr=2.6e-02 time=  2.8s/  2.7s | Train: loss=1.027, TAw acc= 66.2% | Valid: time=  0.5s loss=0.663, TAw acc= 74.8% | *
| Epoch  20, lr=2.6e-02 time=  2.8s/  2.0s | Train: loss=1.059, TAw acc= 63.6% | Valid: time=  0.4s loss=0.686, TAw acc= 73.6% |
| Epoch  21, lr=2.6e-02 time=  2.9s/  1.9s | Train: loss=0.992, TAw acc= 66.1% | Valid: time=  0.4s loss=0.687, TAw acc= 73.2% |
| Epoch  22, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=1.042, TAw acc= 64.8% | Valid: time=  0.4s loss=0.663, TAw acc= 75.6% | *
| Epoch  23, lr=2.6e-02 time=  2.8s/  3.1s | Train: loss=1.002, TAw acc= 65.6% | Valid: time=  0.4s loss=0.673, TAw acc= 76.4% |
| Epoch  24, lr=2.6e-02 time=  3.1s/  2.5s | Train: loss=1.011, TAw acc= 65.4% | Valid: time=  0.5s loss=0.666, TAw acc= 75.8% |
| Epoch  25, lr=2.6e-02 time=  3.2s/  1.9s | Train: loss=1.005, TAw acc= 65.7% | Valid: time=  0.4s loss=0.665, TAw acc= 74.2% |
| Epoch  26, lr=2.6e-02 time=  2.8s/  2.3s | Train: loss=1.002, TAw acc= 65.8% | Valid: time=  0.9s loss=0.666, TAw acc= 75.0% |
| Epoch  27, lr=2.6e-02 time=  2.8s/  2.3s | Train: loss=1.014, TAw acc= 65.5% | Valid: time=  0.5s loss=0.680, TAw acc= 74.2% |
| Epoch  28, lr=2.6e-02 time=  3.6s/  2.4s | Train: loss=1.008, TAw acc= 66.0% | Valid: time=  0.5s loss=0.676, TAw acc= 74.4% |
| Epoch  29, lr=2.6e-02 time=  3.3s/  1.9s | Train: loss=0.993, TAw acc= 66.1% | Valid: time=  0.4s loss=0.689, TAw acc= 76.0% |
| Epoch  30, lr=2.6e-02 time=  3.5s/  1.9s | Train: loss=0.984, TAw acc= 66.0% | Valid: time=  0.4s loss=0.674, TAw acc= 75.8% |
| Epoch  31, lr=2.6e-02 time=  3.0s/  2.1s | Train: loss=0.988, TAw acc= 65.5% | Valid: time=  0.4s loss=0.693, TAw acc= 74.2% |
| Epoch  32, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=0.999, TAw acc= 65.7% | Valid: time=  0.4s loss=0.682, TAw acc= 74.4% |
| Epoch  33, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=0.991, TAw acc= 66.0% | Valid: time=  0.4s loss=0.677, TAw acc= 73.8% |
| Epoch  34, lr=2.6e-02 time=  4.1s/  1.9s | Train: loss=1.004, TAw acc= 65.7% | Valid: time=  0.5s loss=0.676, TAw acc= 75.8% |
| Epoch  35, lr=2.6e-02 time=  3.7s/  2.5s | Train: loss=1.010, TAw acc= 65.6% | Valid: time=  0.5s loss=0.706, TAw acc= 73.2% |
| Epoch  36, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.007, TAw acc= 65.6% | Valid: time=  0.5s loss=0.661, TAw acc= 74.4% | *
| Epoch  37, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.978, TAw acc= 66.5% | Valid: time=  0.5s loss=0.641, TAw acc= 75.4% | *
| Epoch  38, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.010, TAw acc= 65.8% | Valid: time=  0.5s loss=0.647, TAw acc= 74.6% |
| Epoch  39, lr=2.6e-02 time=  3.7s/  2.3s | Train: loss=0.995, TAw acc= 65.6% | Valid: time=  0.4s loss=0.661, TAw acc= 77.0% |
| Epoch  40, lr=2.6e-02 time=  2.8s/  2.0s | Train: loss=1.018, TAw acc= 65.4% | Valid: time=  0.4s loss=0.711, TAw acc= 74.0% |
| Epoch  41, lr=2.6e-02 time=  3.8s/  1.9s | Train: loss=1.043, TAw acc= 65.2% | Valid: time=  0.5s loss=0.674, TAw acc= 75.2% |
| Epoch  42, lr=2.6e-02 time=  3.1s/  2.0s | Train: loss=0.997, TAw acc= 66.5% | Valid: time=  0.5s loss=0.652, TAw acc= 75.8% |
| Epoch  43, lr=2.6e-02 time=  3.2s/  1.9s | Train: loss=0.993, TAw acc= 66.0% | Valid: time=  0.4s loss=0.655, TAw acc= 75.4% |
| Epoch  44, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=0.985, TAw acc= 66.6% | Valid: time=  0.4s loss=0.681, TAw acc= 76.4% |
| Epoch  45, lr=2.6e-02 time=  2.8s/  3.1s | Train: loss=0.971, TAw acc= 66.8% | Valid: time=  0.4s loss=0.652, TAw acc= 76.0% |
| Epoch  46, lr=2.6e-02 time=  3.0s/  2.5s | Train: loss=0.961, TAw acc= 67.3% | Valid: time=  0.5s loss=0.644, TAw acc= 77.2% |
| Epoch  47, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.955, TAw acc= 66.9% | Valid: time=  0.5s loss=0.669, TAw acc= 75.4% |
| Epoch  48, lr=2.6e-02 time=  3.4s/  1.9s | Train: loss=0.971, TAw acc= 66.7% | Valid: time=  0.4s loss=0.702, TAw acc= 72.8% |
| Epoch  49, lr=2.6e-02 time=  3.4s/  2.0s | Train: loss=0.968, TAw acc= 66.7% | Valid: time=  0.5s loss=0.681, TAw acc= 75.8% |
| Epoch  50, lr=2.6e-02 time=  2.8s/  2.3s | Train: loss=0.942, TAw acc= 67.2% | Valid: time=  0.4s loss=0.662, TAw acc= 76.8% |
| Epoch  51, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=0.978, TAw acc= 67.1% | Valid: time=  0.4s loss=0.682, TAw acc= 75.8% |
| Epoch  52, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=0.949, TAw acc= 67.0% | Valid: time=  0.5s loss=0.688, TAw acc= 76.8% |
| Epoch  53, lr=2.6e-02 time=  4.1s/  2.0s | Train: loss=1.010, TAw acc= 66.3% | Valid: time=  0.5s loss=0.640, TAw acc= 75.2% | *
| Epoch  54, lr=2.6e-02 time=  3.7s/  2.5s | Train: loss=0.960, TAw acc= 67.0% | Valid: time=  0.5s loss=0.649, TAw acc= 78.0% |
| Epoch  55, lr=2.6e-02 time=  3.4s/  2.3s | Train: loss=0.967, TAw acc= 67.0% | Valid: time=  0.4s loss=0.652, TAw acc= 76.2% |
| Epoch  56, lr=2.6e-02 time=  2.8s/  2.0s | Train: loss=0.990, TAw acc= 66.6% | Valid: time=  0.4s loss=0.644, TAw acc= 76.6% |
| Epoch  57, lr=2.6e-02 time=  4.1s/  1.9s | Train: loss=0.938, TAw acc= 68.2% | Valid: time=  0.4s loss=0.661, TAw acc= 77.0% |
| Epoch  58, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=0.997, TAw acc= 65.5% | Valid: time=  0.5s loss=0.639, TAw acc= 75.8% | *
| Epoch  59, lr=2.6e-02 time=  3.9s/  2.0s | Train: loss=1.015, TAw acc= 65.0% | Valid: time=  0.4s loss=0.671, TAw acc= 73.8% |
| Epoch  60, lr=2.6e-02 time=  2.9s/  2.0s | Train: loss=0.978, TAw acc= 66.6% | Valid: time=  0.5s loss=0.655, TAw acc= 76.2% |
== Rank Reduction [task:3] ==
Debug-0:
  best_loss=0.639,   best_acc=0.758
loss_margin=0.006, acc_margin=0.004
sow: rank=1024, freezed_rank=832
 a:        ['-6.40e+00', '+4.96e+00'] .... ['+5.46e+00', '+5.06e+00'] .... ['+1.29e+00', '-2.74e+00']
 a_backup: ['-6.40e+00', '+4.96e+00'] .... ['+5.46e+00', '+5.06e+00'] .... ['+1.29e+00', '-2.74e+00']
 r=256, loss=1.755, acc=0.360 (rank=256)
 r=288, loss=1.641, acc=0.362 (rank=288)
 r=320, loss=1.437, acc=0.436 (rank=320)
 r=352, loss=1.175, acc=0.588 (rank=352)
 r=384, loss=1.118, acc=0.588 (rank=384)
 r=416, loss=0.948, acc=0.676 (rank=416)
 r=448, loss=0.850, acc=0.682 (rank=448)
 r=480, loss=0.756, acc=0.720 (rank=480)
 r=512, loss=0.688, acc=0.762 (rank=512)
 r=544, loss=0.673, acc=0.760 (rank=544)
 r=576, loss=0.679, acc=0.754 (rank=576)
 r=608, loss=0.698, acc=0.752 (rank=608)
 r=640, loss=0.704, acc=0.740 (rank=640)
 r=672, loss=0.687, acc=0.760 (rank=672)
 r=704, loss=0.664, acc=0.772 (rank=704)
 r=736, loss=0.661, acc=0.772 (rank=736)
 r=768, loss=0.658, acc=0.768 (rank=768)
 r=800, loss=0.662, acc=0.768 (rank=800)
 r=832, loss=0.659, acc=0.770 (rank=832)
 r=864, loss=0.655, acc=0.764 (rank=864)
 r=896, loss=0.634, acc=0.768 (rank=896)
 best_r=896, loss=0.634, acc=0.768
== Header Training for Low Rank [task:3] ==
Fix_Classifier: Done (training=False, fix_classifier=True)
Debug-1: loss=0.634, acc=0.768
sow: rank=896, freezed_rank=896
 a:        ['-6.40e+00', '+4.96e+00'] .... ['+5.18e+00', '-1.80e+308'] .... ['-1.80e+308', '-1.80e+308']
 a_backup: ['-6.40e+00', '+4.96e+00'] .... ['+5.18e+00', '+5.27e+00'] .... ['+1.29e+00', '-2.74e+00']
SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.008767991699104466
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0

Parameter Group 1
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.05
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0
)
 Group[0]  param_num=161, free_param_num=2
 Group[1]  param_num=3, free_param_num=0
sow: rank=896, freezed_rank=896
 a:        ['-6.40e+00', '+4.96e+00'] .... ['+5.18e+00', '-1.80e+308'] .... ['-1.80e+308', '-1.80e+308']
 a_backup: ['-6.40e+00', '+4.96e+00'] .... ['+5.18e+00', '+5.27e+00'] .... ['+1.29e+00', '-2.74e+00']
| Epoch   1, lr=8.8e-03 time=  1.7s/  2.0s | Train: loss=0.969, TAw acc= 67.8% | Valid: time=  0.5s loss=0.620, TAw acc= 76.6% | *
| Epoch   2, lr=8.8e-03 time=  2.0s/  2.5s | Train: loss=0.981, TAw acc= 67.1% | Valid: time=  0.4s loss=0.624, TAw acc= 76.8% |
| Epoch   3, lr=8.8e-03 time=  1.7s/  2.5s | Train: loss=0.982, TAw acc= 67.2% | Valid: time=  0.5s loss=0.622, TAw acc= 77.4% |
| Epoch   4, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=0.970, TAw acc= 67.2% | Valid: time=  0.5s loss=0.619, TAw acc= 77.2% | *
| Epoch   5, lr=8.8e-03 time=  1.9s/  1.9s | Train: loss=0.969, TAw acc= 67.4% | Valid: time=  0.5s loss=0.617, TAw acc= 77.0% | *
| Epoch   6, lr=8.8e-03 time=  1.7s/  2.6s | Train: loss=0.982, TAw acc= 67.2% | Valid: time=  0.4s loss=0.620, TAw acc= 77.4% |
| Epoch   7, lr=8.8e-03 time=  1.8s/  2.0s | Train: loss=0.962, TAw acc= 67.8% | Valid: time=  0.4s loss=0.621, TAw acc= 76.6% |
| Epoch   8, lr=8.8e-03 time=  1.9s/  1.9s | Train: loss=0.959, TAw acc= 68.2% | Valid: time=  0.4s loss=0.622, TAw acc= 77.0% |
| Epoch   9, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.960, TAw acc= 67.6% | Valid: time=  0.4s loss=0.612, TAw acc= 77.2% | *
| Epoch  10, lr=8.8e-03 time=  1.7s/  2.0s | Train: loss=0.959, TAw acc= 67.4% | Valid: time=  0.4s loss=0.618, TAw acc= 77.0% |
| Epoch  11, lr=8.8e-03 time=  2.1s/  2.5s | Train: loss=0.934, TAw acc= 68.7% | Valid: time=  0.4s loss=0.616, TAw acc= 76.8% |
| Epoch  12, lr=8.8e-03 time=  1.7s/  2.5s | Train: loss=0.951, TAw acc= 68.0% | Valid: time=  0.5s loss=0.615, TAw acc= 77.6% |
| Epoch  13, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=0.944, TAw acc= 68.2% | Valid: time=  0.5s loss=0.619, TAw acc= 76.8% |
| Epoch  14, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=0.936, TAw acc= 68.1% | Valid: time=  0.5s loss=0.612, TAw acc= 77.0% | *
| Epoch  15, lr=8.8e-03 time=  2.0s/  2.1s | Train: loss=0.952, TAw acc= 68.1% | Valid: time=  0.4s loss=0.610, TAw acc= 77.6% | *
| Epoch  16, lr=8.8e-03 time=  1.9s/  2.1s | Train: loss=0.941, TAw acc= 68.5% | Valid: time=  0.4s loss=0.618, TAw acc= 77.2% |
| Epoch  17, lr=8.8e-03 time=  2.0s/  1.9s | Train: loss=0.929, TAw acc= 69.0% | Valid: time=  0.4s loss=0.617, TAw acc= 77.2% |
| Epoch  18, lr=8.8e-03 time=  1.7s/  2.0s | Train: loss=0.950, TAw acc= 68.2% | Valid: time=  0.4s loss=0.616, TAw acc= 76.8% |
| Epoch  19, lr=8.8e-03 time=  1.7s/  3.1s | Train: loss=0.940, TAw acc= 68.8% | Valid: time=  0.4s loss=0.611, TAw acc= 76.8% |
| Epoch  20, lr=8.8e-03 time=  1.7s/  2.3s | Train: loss=0.929, TAw acc= 68.3% | Valid: time=  0.5s loss=0.624, TAw acc= 76.6% |
| Epoch  21, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=0.916, TAw acc= 69.1% | Valid: time=  0.5s loss=0.621, TAw acc= 77.0% |
| Epoch  22, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=0.945, TAw acc= 67.4% | Valid: time=  0.5s loss=0.623, TAw acc= 76.8% |
| Epoch  23, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=0.927, TAw acc= 68.4% | Valid: time=  0.5s loss=0.620, TAw acc= 76.8% |
| Epoch  24, lr=8.8e-03 time=  2.1s/  2.5s | Train: loss=0.923, TAw acc= 68.9% | Valid: time=  0.5s loss=0.615, TAw acc= 77.0% |
| Epoch  25, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=0.937, TAw acc= 67.4% | Valid: time=  0.5s loss=0.621, TAw acc= 77.0% |
| Epoch  26, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=0.911, TAw acc= 68.3% | Valid: time=  0.5s loss=0.617, TAw acc= 76.6% |
| Epoch  27, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=0.905, TAw acc= 69.3% | Valid: time=  0.5s loss=0.621, TAw acc= 76.8% |
| Epoch  28, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=0.920, TAw acc= 69.1% | Valid: time=  0.5s loss=0.625, TAw acc= 77.0% |
| Epoch  29, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=0.899, TAw acc= 69.3% | Valid: time=  0.5s loss=0.626, TAw acc= 76.6% |
| Epoch  30, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=0.925, TAw acc= 68.0% | Valid: time=  0.5s loss=0.637, TAw acc= 76.0% |
| Epoch  31, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=0.920, TAw acc= 69.1% | Valid: time=  0.5s loss=0.628, TAw acc= 76.4% |
| Epoch  32, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=0.914, TAw acc= 69.4% | Valid: time=  0.5s loss=0.621, TAw acc= 76.6% |
| Epoch  33, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=0.909, TAw acc= 69.6% | Valid: time=  0.5s loss=0.621, TAw acc= 76.2% |
| Epoch  34, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=0.904, TAw acc= 69.4% | Valid: time=  0.5s loss=0.628, TAw acc= 76.6% |
| Epoch  35, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=0.924, TAw acc= 68.7% | Valid: time=  0.5s loss=0.624, TAw acc= 76.6% |
| Epoch  36, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=0.910, TAw acc= 68.7% | Valid: time=  0.5s loss=0.620, TAw acc= 76.8% |
| Epoch  37, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=0.925, TAw acc= 68.4% | Valid: time=  0.5s loss=0.622, TAw acc= 76.8% |
| Epoch  38, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=0.902, TAw acc= 69.5% | Valid: time=  0.5s loss=0.621, TAw acc= 76.4% |
| Epoch  39, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=0.896, TAw acc= 69.6% | Valid: time=  0.5s loss=0.619, TAw acc= 76.6% |
| Epoch  40, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=0.902, TAw acc= 69.4% | Valid: time=  0.5s loss=0.618, TAw acc= 77.0% |
| Epoch  41, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=0.920, TAw acc= 67.9% | Valid: time=  0.5s loss=0.621, TAw acc= 76.8% |
| Epoch  42, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=0.900, TAw acc= 68.9% | Valid: time=  0.5s loss=0.625, TAw acc= 76.6% |
| Epoch  43, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=0.897, TAw acc= 69.8% | Valid: time=  0.5s loss=0.619, TAw acc= 76.6% |
| Epoch  44, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=0.904, TAw acc= 69.1% | Valid: time=  0.5s loss=0.628, TAw acc= 76.8% |
| Epoch  45, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=0.909, TAw acc= 69.0% | Valid: time=  0.5s loss=0.623, TAw acc= 76.4% | lr=2.9e-03
| Epoch  46, lr=2.9e-03 time=  2.2s/  2.4s | Train: loss=0.951, TAw acc= 68.2% | Valid: time=  0.5s loss=0.612, TAw acc= 77.2% |
| Epoch  47, lr=2.9e-03 time=  2.2s/  2.4s | Train: loss=0.923, TAw acc= 68.9% | Valid: time=  0.5s loss=0.616, TAw acc= 77.2% |
| Epoch  48, lr=2.9e-03 time=  2.2s/  2.4s | Train: loss=0.936, TAw acc= 69.0% | Valid: time=  0.5s loss=0.614, TAw acc= 76.6% |
| Epoch  49, lr=2.9e-03 time=  2.2s/  2.5s | Train: loss=0.947, TAw acc= 67.3% | Valid: time=  0.5s loss=0.615, TAw acc= 76.6% |
| Epoch  50, lr=2.9e-03 time=  2.2s/  2.5s | Train: loss=0.925, TAw acc= 69.0% | Valid: time=  0.5s loss=0.615, TAw acc= 77.2% |
| Epoch  51, lr=2.9e-03 time=  2.2s/  2.4s | Train: loss=0.932, TAw acc= 68.1% | Valid: time=  0.5s loss=0.616, TAw acc= 76.8% |
| Epoch  52, lr=2.9e-03 time=  2.2s/  2.5s | Train: loss=0.952, TAw acc= 68.1% | Valid: time=  0.5s loss=0.616, TAw acc= 76.8% |
| Epoch  53, lr=2.9e-03 time=  2.2s/  2.5s | Train: loss=0.943, TAw acc= 67.8% | Valid: time=  0.5s loss=0.617, TAw acc= 76.8% |
| Epoch  54, lr=2.9e-03 time=  2.2s/  2.4s | Train: loss=0.947, TAw acc= 68.3% | Valid: time=  0.5s loss=0.617, TAw acc= 76.6% |
| Epoch  55, lr=2.9e-03 time=  2.2s/  2.5s | Train: loss=0.948, TAw acc= 67.8% | Valid: time=  0.5s loss=0.617, TAw acc= 76.8% |
| Epoch  56, lr=2.9e-03 time=  2.2s/  2.5s | Train: loss=0.936, TAw acc= 67.9% | Valid: time=  0.5s loss=0.617, TAw acc= 76.8% |
| Epoch  57, lr=2.9e-03 time=  2.2s/  2.5s | Train: loss=0.950, TAw acc= 68.9% | Valid: time=  0.5s loss=0.619, TAw acc= 76.8% |
| Epoch  58, lr=2.9e-03 time=  2.2s/  2.5s | Train: loss=0.932, TAw acc= 68.7% | Valid: time=  0.5s loss=0.617, TAw acc= 77.2% |
| Epoch  59, lr=2.9e-03 time=  2.2s/  2.5s | Train: loss=0.929, TAw acc= 68.3% | Valid: time=  0.5s loss=0.618, TAw acc= 77.2% |
| Epoch  60, lr=2.9e-03 time=  2.2s/  2.4s | Train: loss=0.939, TAw acc= 68.5% | Valid: time=  0.5s loss=0.619, TAw acc= 76.8% |
Debug-2: loss=0.610, acc=0.776
sow: rank=896, freezed_rank=896
 a:        ['-6.40e+00', '+4.96e+00'] .... ['+5.18e+00', '-1.80e+308'] .... ['-1.80e+308', '-1.80e+308']
 a_backup: ['-6.40e+00', '+4.96e+00'] .... ['+5.18e+00', '+5.27e+00'] .... ['+1.29e+00', '-2.74e+00']
Free_Classifier: Done (training=True, fix_classifier=False)
Debug-3: loss=0.615 acc=0.776
sow: rank=1024, freezed_rank=896
 a:        ['-6.40e+00', '+4.96e+00'] .... ['+5.18e+00', '+5.27e+00'] .... ['+1.29e+00', '-2.74e+00']
 a_backup: ['-6.40e+00', '+4.96e+00'] .... ['+5.18e+00', '+5.27e+00'] .... ['+1.29e+00', '-2.74e+00']
Header Training: Finished.
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.649 | TAw acc= 79.2%, forg=  0.0%| TAg acc= 28.2%, forg= 51.0% <<<
>>> Test on task  1 : loss=0.998 | TAw acc= 66.9%, forg=  0.0%| TAg acc= 32.5%, forg= 19.7% <<<
>>> Test on task  2 : loss=0.655 | TAw acc= 77.0%, forg=  0.0%| TAg acc= 36.5%, forg= 15.4% <<<
>>> Test on task  3 : loss=0.620 | TAw acc= 79.1%, forg=  0.0%| TAg acc= 57.2%, forg=  0.0% <<<
Save at ../RESULT_AAAI2025/CR10/0/cifar100_sow
************************************************************************************************************
Task  4
************************************************************************************************************
LLL_Net(
  (model): ResNet50_32SOW(
    in_channels=3, in_H=32, in_W=32, training=False, fix_features=True, fix_classifier=False
    (features): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
      (3): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (4): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (3): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (5): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (3): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (4): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (5): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (6): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (7): AdaptiveAvgPool2d(output_size=(1, 1))
    )
    (classifier): Sequential(
      (0): Dropout(p=0.417598542370663, inplace=False)
      (1): SOW_V3(in_features=1024, out_features=1024, num_tasks=10, dtype=torch.float64, device=cuda:0)
      (2): ReLU()
    )
    (fc): Sequential()
  )
  (heads): ModuleList(
    (0-4): 5 x Linear(in_features=1024, out_features=10, bias=True)
  )
)
SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.0263039750973134
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0

Parameter Group 1
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.05
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0
)
 Group[0]  param_num=161, free_param_num=2
 Group[1]  param_num=3, free_param_num=3
sow: rank=1024, freezed_rank=896
 a:        ['-6.40e+00', '+4.96e+00'] .... ['+5.18e+00', '+5.27e+00'] .... ['+1.29e+00', '-2.74e+00']
 a_backup: ['-6.40e+00', '+4.96e+00'] .... ['+5.18e+00', '+5.27e+00'] .... ['+1.29e+00', '-2.74e+00']
| Epoch   1, lr=2.6e-02 time=  3.6s/  2.4s | Train: loss=1.141, TAw acc= 61.9% | Valid: time=  0.5s loss=0.694, TAw acc= 76.0% | *
| Epoch   2, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.064, TAw acc= 64.8% | Valid: time=  0.5s loss=0.669, TAw acc= 78.2% | *
| Epoch   3, lr=2.6e-02 time=  3.7s/  2.5s | Train: loss=1.015, TAw acc= 66.2% | Valid: time=  0.5s loss=0.561, TAw acc= 80.2% | *
| Epoch   4, lr=2.6e-02 time=  3.7s/  2.5s | Train: loss=0.967, TAw acc= 67.3% | Valid: time=  0.5s loss=0.549, TAw acc= 82.6% | *
| Epoch   5, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.013, TAw acc= 66.4% | Valid: time=  0.5s loss=0.521, TAw acc= 83.8% | *
| Epoch   6, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.972, TAw acc= 67.7% | Valid: time=  0.5s loss=0.493, TAw acc= 83.2% | *
| Epoch   7, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.988, TAw acc= 67.3% | Valid: time=  0.5s loss=0.535, TAw acc= 83.2% |
| Epoch   8, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.012, TAw acc= 66.5% | Valid: time=  0.5s loss=0.485, TAw acc= 83.0% | *
| Epoch   9, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.995, TAw acc= 66.2% | Valid: time=  0.5s loss=0.525, TAw acc= 83.0% |
| Epoch  10, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.954, TAw acc= 67.8% | Valid: time=  0.5s loss=0.513, TAw acc= 83.4% |
| Epoch  11, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.933, TAw acc= 68.3% | Valid: time=  0.5s loss=0.508, TAw acc= 84.2% |
| Epoch  12, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.958, TAw acc= 67.2% | Valid: time=  0.5s loss=0.495, TAw acc= 84.0% |
| Epoch  13, lr=2.6e-02 time=  3.7s/  2.5s | Train: loss=0.991, TAw acc= 65.9% | Valid: time=  0.5s loss=0.520, TAw acc= 82.4% |
| Epoch  14, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.960, TAw acc= 68.3% | Valid: time=  0.5s loss=0.496, TAw acc= 83.4% |
| Epoch  15, lr=2.6e-02 time=  3.6s/  2.4s | Train: loss=0.926, TAw acc= 69.1% | Valid: time=  0.5s loss=0.493, TAw acc= 83.6% |
| Epoch  16, lr=2.6e-02 time=  3.7s/  2.5s | Train: loss=0.944, TAw acc= 69.2% | Valid: time=  0.5s loss=0.490, TAw acc= 84.6% |
| Epoch  17, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.953, TAw acc= 68.2% | Valid: time=  0.5s loss=0.503, TAw acc= 82.4% |
| Epoch  18, lr=2.6e-02 time=  3.6s/  2.4s | Train: loss=0.949, TAw acc= 68.6% | Valid: time=  0.5s loss=0.469, TAw acc= 84.6% | *
| Epoch  19, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.959, TAw acc= 68.0% | Valid: time=  0.5s loss=0.440, TAw acc= 84.4% | *
| Epoch  20, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.930, TAw acc= 68.7% | Valid: time=  0.5s loss=0.471, TAw acc= 85.0% |
| Epoch  21, lr=2.6e-02 time=  3.6s/  2.4s | Train: loss=0.964, TAw acc= 67.8% | Valid: time=  0.5s loss=0.459, TAw acc= 84.4% |
| Epoch  22, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.947, TAw acc= 68.8% | Valid: time=  0.5s loss=0.506, TAw acc= 83.8% |
| Epoch  23, lr=2.6e-02 time=  3.6s/  2.4s | Train: loss=0.921, TAw acc= 69.6% | Valid: time=  0.5s loss=0.490, TAw acc= 84.0% |
| Epoch  24, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.950, TAw acc= 69.1% | Valid: time=  0.5s loss=0.477, TAw acc= 84.2% |
| Epoch  25, lr=2.6e-02 time=  3.6s/  2.4s | Train: loss=0.906, TAw acc= 69.9% | Valid: time=  0.5s loss=0.492, TAw acc= 84.2% |
| Epoch  26, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.915, TAw acc= 69.9% | Valid: time=  0.5s loss=0.472, TAw acc= 84.4% |
| Epoch  27, lr=2.6e-02 time=  3.7s/  2.5s | Train: loss=0.925, TAw acc= 68.6% | Valid: time=  0.5s loss=0.505, TAw acc= 83.8% |
| Epoch  28, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.944, TAw acc= 69.0% | Valid: time=  0.5s loss=0.469, TAw acc= 84.6% |
| Epoch  29, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.940, TAw acc= 68.1% | Valid: time=  0.5s loss=0.499, TAw acc= 84.0% |
| Epoch  30, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.920, TAw acc= 70.5% | Valid: time=  0.5s loss=0.495, TAw acc= 83.0% |
| Epoch  31, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.911, TAw acc= 69.6% | Valid: time=  0.5s loss=0.469, TAw acc= 84.2% |
| Epoch  32, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.929, TAw acc= 69.4% | Valid: time=  0.5s loss=0.481, TAw acc= 84.2% |
| Epoch  33, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.944, TAw acc= 68.6% | Valid: time=  0.5s loss=0.444, TAw acc= 84.2% |
| Epoch  34, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.904, TAw acc= 69.8% | Valid: time=  0.5s loss=0.493, TAw acc= 85.6% |
| Epoch  35, lr=2.6e-02 time=  3.3s/  1.9s | Train: loss=0.926, TAw acc= 69.1% | Valid: time=  0.4s loss=0.466, TAw acc= 84.4% |
| Epoch  36, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=0.887, TAw acc= 70.6% | Valid: time=  0.4s loss=0.478, TAw acc= 85.0% |
| Epoch  37, lr=2.6e-02 time=  4.1s/  1.9s | Train: loss=0.926, TAw acc= 69.2% | Valid: time=  0.5s loss=0.455, TAw acc= 84.2% |
| Epoch  38, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.899, TAw acc= 69.9% | Valid: time=  0.5s loss=0.468, TAw acc= 85.0% |
| Epoch  39, lr=2.6e-02 time=  3.6s/  2.3s | Train: loss=0.920, TAw acc= 69.2% | Valid: time=  0.5s loss=0.472, TAw acc= 84.8% |
| Epoch  40, lr=2.6e-02 time=  3.0s/  1.9s | Train: loss=0.906, TAw acc= 70.0% | Valid: time=  0.4s loss=0.451, TAw acc= 84.4% |
| Epoch  41, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=0.934, TAw acc= 69.0% | Valid: time=  0.5s loss=0.468, TAw acc= 84.8% |
| Epoch  42, lr=2.6e-02 time=  4.0s/  1.9s | Train: loss=0.933, TAw acc= 70.1% | Valid: time=  0.5s loss=0.463, TAw acc= 84.2% |
| Epoch  43, lr=2.6e-02 time=  3.6s/  2.4s | Train: loss=0.941, TAw acc= 69.5% | Valid: time=  0.5s loss=0.487, TAw acc= 83.6% |
| Epoch  44, lr=2.6e-02 time=  3.6s/  2.3s | Train: loss=0.952, TAw acc= 67.4% | Valid: time=  0.4s loss=0.495, TAw acc= 84.8% |
| Epoch  45, lr=2.6e-02 time=  7.3s/  1.9s | Train: loss=0.919, TAw acc= 69.8% | Valid: time=  0.4s loss=0.476, TAw acc= 84.2% |
| Epoch  46, lr=2.6e-02 time=  3.0s/  1.9s | Train: loss=0.925, TAw acc= 69.6% | Valid: time=  0.4s loss=0.453, TAw acc= 84.6% |
| Epoch  47, lr=2.6e-02 time=  3.3s/  2.0s | Train: loss=0.891, TAw acc= 70.5% | Valid: time=  0.4s loss=0.470, TAw acc= 84.6% |
| Epoch  48, lr=2.6e-02 time=  3.1s/  1.9s | Train: loss=0.935, TAw acc= 68.5% | Valid: time=  0.8s loss=0.460, TAw acc= 83.6% |
| Epoch  49, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=0.905, TAw acc= 70.0% | Valid: time=  0.8s loss=0.482, TAw acc= 84.2% | lr=8.8e-03
| Epoch  50, lr=8.8e-03 time=  2.8s/  1.9s | Train: loss=0.924, TAw acc= 69.2% | Valid: time=  0.6s loss=0.474, TAw acc= 84.4% |
| Epoch  51, lr=8.8e-03 time=  2.9s/  1.9s | Train: loss=0.938, TAw acc= 68.6% | Valid: time=  0.4s loss=0.490, TAw acc= 84.0% |
| Epoch  52, lr=8.8e-03 time=  3.1s/  1.9s | Train: loss=0.909, TAw acc= 70.4% | Valid: time=  0.4s loss=0.461, TAw acc= 85.0% |
| Epoch  53, lr=8.8e-03 time=  3.3s/  1.9s | Train: loss=0.923, TAw acc= 69.1% | Valid: time=  0.4s loss=0.491, TAw acc= 83.4% |
| Epoch  54, lr=8.8e-03 time=  3.0s/  1.9s | Train: loss=0.899, TAw acc= 70.4% | Valid: time=  0.4s loss=0.475, TAw acc= 84.0% |
| Epoch  55, lr=8.8e-03 time=  2.7s/  1.9s | Train: loss=0.919, TAw acc= 69.4% | Valid: time=  0.5s loss=0.463, TAw acc= 84.6% |
| Epoch  56, lr=8.8e-03 time=  2.8s/  3.1s | Train: loss=0.900, TAw acc= 70.5% | Valid: time=  0.4s loss=0.462, TAw acc= 85.4% |
| Epoch  57, lr=8.8e-03 time=  2.9s/  2.5s | Train: loss=0.906, TAw acc= 70.2% | Valid: time=  0.5s loss=0.456, TAw acc= 84.6% |
| Epoch  58, lr=8.8e-03 time=  3.6s/  2.5s | Train: loss=0.924, TAw acc= 69.9% | Valid: time=  0.5s loss=0.468, TAw acc= 85.0% |
| Epoch  59, lr=8.8e-03 time=  3.7s/  2.0s | Train: loss=0.955, TAw acc= 67.5% | Valid: time=  0.4s loss=0.484, TAw acc= 85.2% |
| Epoch  60, lr=8.8e-03 time=  2.9s/  1.9s | Train: loss=0.905, TAw acc= 69.5% | Valid: time=  0.4s loss=0.485, TAw acc= 84.8% |
== Rank Reduction [task:4] ==
Debug-0:
  best_loss=0.440,   best_acc=0.844
loss_margin=0.006, acc_margin=0.004
sow: rank=1024, freezed_rank=896
 a:        ['-5.99e+00', '+4.97e+00'] .... ['+5.18e+00', '+5.27e+00'] .... ['+1.29e+00', '-2.74e+00']
 a_backup: ['-5.99e+00', '+4.97e+00'] .... ['+5.18e+00', '+5.27e+00'] .... ['+1.29e+00', '-2.74e+00']
 r=256, loss=1.895, acc=0.302 (rank=256)
 r=288, loss=1.835, acc=0.278 (rank=288)
 r=320, loss=1.544, acc=0.434 (rank=320)
 r=352, loss=1.299, acc=0.554 (rank=352)
 r=384, loss=1.114, acc=0.622 (rank=384)
 r=416, loss=0.852, acc=0.744 (rank=416)
 r=448, loss=0.717, acc=0.776 (rank=448)
 r=480, loss=0.574, acc=0.822 (rank=480)
 r=512, loss=0.494, acc=0.830 (rank=512)
 r=544, loss=0.489, acc=0.832 (rank=544)
 r=576, loss=0.503, acc=0.828 (rank=576)
 r=608, loss=0.508, acc=0.818 (rank=608)
 r=640, loss=0.519, acc=0.810 (rank=640)
 r=672, loss=0.493, acc=0.828 (rank=672)
 r=704, loss=0.485, acc=0.834 (rank=704)
 r=736, loss=0.474, acc=0.838 (rank=736)
 r=768, loss=0.456, acc=0.840 (rank=768)
 r=800, loss=0.454, acc=0.836 (rank=800)
 r=832, loss=0.451, acc=0.832 (rank=832)
 r=864, loss=0.450, acc=0.834 (rank=864)
 r=896, loss=0.440, acc=0.844 (rank=896)
 best_r=896, loss=0.440, acc=0.844
== Header Training for Low Rank [task:4] ==
Fix_Classifier: Done (training=False, fix_classifier=True)
Debug-1: loss=0.440, acc=0.844
sow: rank=896, freezed_rank=896
 a:        ['-5.99e+00', '+4.97e+00'] .... ['+5.18e+00', '-1.80e+308'] .... ['-1.80e+308', '-1.80e+308']
 a_backup: ['-5.99e+00', '+4.97e+00'] .... ['+5.18e+00', '+5.27e+00'] .... ['+1.29e+00', '-2.74e+00']
SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.008767991699104466
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0

Parameter Group 1
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.05
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0
)
 Group[0]  param_num=161, free_param_num=2
 Group[1]  param_num=3, free_param_num=0
sow: rank=896, freezed_rank=896
 a:        ['-5.99e+00', '+4.97e+00'] .... ['+5.18e+00', '-1.80e+308'] .... ['-1.80e+308', '-1.80e+308']
 a_backup: ['-5.99e+00', '+4.97e+00'] .... ['+5.18e+00', '+5.27e+00'] .... ['+1.29e+00', '-2.74e+00']
| Epoch   1, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=0.928, TAw acc= 71.1% | Valid: time=  0.5s loss=0.483, TAw acc= 84.8% |
| Epoch   2, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=0.912, TAw acc= 70.6% | Valid: time=  0.5s loss=0.464, TAw acc= 85.2% |
| Epoch   3, lr=8.8e-03 time=  1.9s/  2.1s | Train: loss=0.921, TAw acc= 69.7% | Valid: time=  0.4s loss=0.463, TAw acc= 84.6% |
| Epoch   4, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.899, TAw acc= 70.7% | Valid: time=  0.4s loss=0.472, TAw acc= 85.2% |
| Epoch   5, lr=8.8e-03 time=  1.8s/  1.9s | Train: loss=0.902, TAw acc= 70.9% | Valid: time=  0.4s loss=0.467, TAw acc= 85.2% |
| Epoch   6, lr=8.8e-03 time=  3.0s/  1.9s | Train: loss=0.883, TAw acc= 71.6% | Valid: time=  0.4s loss=0.474, TAw acc= 85.0% |
| Epoch   7, lr=8.8e-03 time=  2.1s/  2.5s | Train: loss=0.892, TAw acc= 70.7% | Valid: time=  0.5s loss=0.469, TAw acc= 85.8% |
| Epoch   8, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=0.870, TAw acc= 71.8% | Valid: time=  0.5s loss=0.454, TAw acc= 85.8% |
| Epoch   9, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.891, TAw acc= 71.6% | Valid: time=  0.4s loss=0.461, TAw acc= 85.6% |
| Epoch  10, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.892, TAw acc= 70.7% | Valid: time=  0.4s loss=0.455, TAw acc= 84.4% |
| Epoch  11, lr=8.8e-03 time=  1.7s/  3.2s | Train: loss=0.876, TAw acc= 71.5% | Valid: time=  0.4s loss=0.463, TAw acc= 85.2% |
| Epoch  12, lr=8.8e-03 time=  1.7s/  2.1s | Train: loss=0.858, TAw acc= 71.8% | Valid: time=  0.5s loss=0.462, TAw acc= 85.2% |
| Epoch  13, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=0.873, TAw acc= 70.9% | Valid: time=  0.5s loss=0.449, TAw acc= 85.6% |
| Epoch  14, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=0.865, TAw acc= 71.3% | Valid: time=  0.5s loss=0.458, TAw acc= 85.8% |
| Epoch  15, lr=8.8e-03 time=  1.8s/  1.9s | Train: loss=0.852, TAw acc= 72.0% | Valid: time=  0.4s loss=0.445, TAw acc= 85.2% |
| Epoch  16, lr=8.8e-03 time=  1.7s/  2.5s | Train: loss=0.858, TAw acc= 72.7% | Valid: time=  0.4s loss=0.453, TAw acc= 85.6% |
| Epoch  17, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.845, TAw acc= 72.2% | Valid: time=  0.4s loss=0.439, TAw acc= 85.0% | *
| Epoch  18, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.854, TAw acc= 71.7% | Valid: time=  0.5s loss=0.457, TAw acc= 84.6% |
| Epoch  19, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.839, TAw acc= 72.4% | Valid: time=  0.9s loss=0.455, TAw acc= 85.2% |
| Epoch  20, lr=8.8e-03 time=  1.9s/  1.9s | Train: loss=0.844, TAw acc= 72.6% | Valid: time=  0.5s loss=0.461, TAw acc= 85.4% |
| Epoch  21, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=0.844, TAw acc= 72.5% | Valid: time=  0.5s loss=0.446, TAw acc= 84.8% |
| Epoch  22, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=0.836, TAw acc= 72.6% | Valid: time=  0.5s loss=0.451, TAw acc= 85.8% |
| Epoch  23, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=0.835, TAw acc= 73.0% | Valid: time=  0.5s loss=0.432, TAw acc= 85.2% | *
| Epoch  24, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=0.848, TAw acc= 71.7% | Valid: time=  0.5s loss=0.452, TAw acc= 85.8% |
| Epoch  25, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=0.829, TAw acc= 73.3% | Valid: time=  0.5s loss=0.443, TAw acc= 84.8% |
| Epoch  26, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.817, TAw acc= 72.8% | Valid: time=  0.4s loss=0.450, TAw acc= 85.2% |
| Epoch  27, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.828, TAw acc= 72.8% | Valid: time=  0.4s loss=0.446, TAw acc= 85.2% |
| Epoch  28, lr=8.8e-03 time=  3.0s/  1.9s | Train: loss=0.820, TAw acc= 73.4% | Valid: time=  0.4s loss=0.445, TAw acc= 85.0% |
| Epoch  29, lr=8.8e-03 time=  2.0s/  2.5s | Train: loss=0.832, TAw acc= 72.4% | Valid: time=  0.5s loss=0.444, TAw acc= 85.6% |
| Epoch  30, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=0.814, TAw acc= 73.0% | Valid: time=  0.5s loss=0.442, TAw acc= 85.4% |
| Epoch  31, lr=8.8e-03 time=  2.2s/  2.3s | Train: loss=0.811, TAw acc= 73.3% | Valid: time=  0.4s loss=0.451, TAw acc= 85.2% |
| Epoch  32, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.806, TAw acc= 73.7% | Valid: time=  0.4s loss=0.448, TAw acc= 84.8% |
| Epoch  33, lr=8.8e-03 time=  1.7s/  2.0s | Train: loss=0.808, TAw acc= 73.3% | Valid: time=  0.5s loss=0.429, TAw acc= 85.0% | *
| Epoch  34, lr=8.8e-03 time=  2.6s/  2.0s | Train: loss=0.806, TAw acc= 73.9% | Valid: time=  0.4s loss=0.440, TAw acc= 85.8% |
| Epoch  35, lr=8.8e-03 time=  1.8s/  2.5s | Train: loss=0.822, TAw acc= 72.8% | Valid: time=  0.5s loss=0.433, TAw acc= 85.4% |
| Epoch  36, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=0.813, TAw acc= 73.7% | Valid: time=  0.5s loss=0.442, TAw acc= 85.2% |
| Epoch  37, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=0.816, TAw acc= 73.3% | Valid: time=  0.5s loss=0.437, TAw acc= 85.4% |
| Epoch  38, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=0.798, TAw acc= 73.2% | Valid: time=  0.5s loss=0.440, TAw acc= 85.6% |
| Epoch  39, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=0.804, TAw acc= 73.1% | Valid: time=  0.5s loss=0.436, TAw acc= 85.4% |
| Epoch  40, lr=8.8e-03 time=  1.8s/  2.1s | Train: loss=0.794, TAw acc= 73.8% | Valid: time=  0.4s loss=0.430, TAw acc= 86.0% |
| Epoch  41, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.816, TAw acc= 73.0% | Valid: time=  0.5s loss=0.440, TAw acc= 86.2% |
| Epoch  42, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.799, TAw acc= 74.3% | Valid: time=  0.4s loss=0.445, TAw acc= 85.0% |
| Epoch  43, lr=8.8e-03 time=  3.0s/  1.9s | Train: loss=0.801, TAw acc= 73.7% | Valid: time=  0.4s loss=0.447, TAw acc= 85.6% |
| Epoch  44, lr=8.8e-03 time=  1.8s/  1.9s | Train: loss=0.788, TAw acc= 73.6% | Valid: time=  0.4s loss=0.446, TAw acc= 85.2% |
| Epoch  45, lr=8.8e-03 time=  1.8s/  2.0s | Train: loss=0.827, TAw acc= 72.8% | Valid: time=  0.4s loss=0.438, TAw acc= 85.2% |
| Epoch  46, lr=8.8e-03 time=  1.8s/  2.2s | Train: loss=0.807, TAw acc= 73.2% | Valid: time=  0.4s loss=0.438, TAw acc= 86.0% |
| Epoch  47, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.775, TAw acc= 74.6% | Valid: time=  0.4s loss=0.441, TAw acc= 84.6% |
| Epoch  48, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.797, TAw acc= 74.3% | Valid: time=  0.4s loss=0.440, TAw acc= 85.0% |
| Epoch  49, lr=8.8e-03 time=  1.7s/  2.8s | Train: loss=0.779, TAw acc= 73.9% | Valid: time=  0.4s loss=0.455, TAw acc= 85.0% |
| Epoch  50, lr=8.8e-03 time=  1.7s/  2.5s | Train: loss=0.790, TAw acc= 73.9% | Valid: time=  0.5s loss=0.453, TAw acc= 85.4% |
| Epoch  51, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=0.798, TAw acc= 73.2% | Valid: time=  0.5s loss=0.446, TAw acc= 85.2% |
| Epoch  52, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=0.783, TAw acc= 74.5% | Valid: time=  0.4s loss=0.434, TAw acc= 85.6% |
| Epoch  53, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.787, TAw acc= 74.1% | Valid: time=  0.4s loss=0.440, TAw acc= 85.2% |
| Epoch  54, lr=8.8e-03 time=  1.7s/  2.0s | Train: loss=0.803, TAw acc= 73.3% | Valid: time=  0.4s loss=0.437, TAw acc= 85.4% |
| Epoch  55, lr=8.8e-03 time=  1.7s/  3.1s | Train: loss=0.771, TAw acc= 74.7% | Valid: time=  0.4s loss=0.435, TAw acc= 85.4% |
| Epoch  56, lr=8.8e-03 time=  1.7s/  2.3s | Train: loss=0.790, TAw acc= 74.0% | Valid: time=  0.5s loss=0.444, TAw acc= 85.8% |
| Epoch  57, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=0.793, TAw acc= 74.1% | Valid: time=  0.5s loss=0.429, TAw acc= 86.0% |
| Epoch  58, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=0.785, TAw acc= 74.2% | Valid: time=  0.5s loss=0.436, TAw acc= 85.6% |
| Epoch  59, lr=8.8e-03 time=  1.9s/  1.9s | Train: loss=0.785, TAw acc= 73.3% | Valid: time=  0.4s loss=0.430, TAw acc= 85.8% |
| Epoch  60, lr=8.8e-03 time=  2.1s/  1.9s | Train: loss=0.797, TAw acc= 73.9% | Valid: time=  0.7s loss=0.440, TAw acc= 86.0% |
Debug-2: loss=0.429, acc=0.850
sow: rank=896, freezed_rank=896
 a:        ['-5.99e+00', '+4.97e+00'] .... ['+5.18e+00', '-1.80e+308'] .... ['-1.80e+308', '-1.80e+308']
 a_backup: ['-5.99e+00', '+4.97e+00'] .... ['+5.18e+00', '+5.27e+00'] .... ['+1.29e+00', '-2.74e+00']
Free_Classifier: Done (training=True, fix_classifier=False)
Debug-3: loss=0.433 acc=0.850
sow: rank=1024, freezed_rank=896
 a:        ['-5.99e+00', '+4.97e+00'] .... ['+5.18e+00', '+5.27e+00'] .... ['+1.29e+00', '-2.74e+00']
 a_backup: ['-5.99e+00', '+4.97e+00'] .... ['+5.18e+00', '+5.27e+00'] .... ['+1.29e+00', '-2.74e+00']
Header Training: Finished.
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.649 | TAw acc= 79.2%, forg=  0.0%| TAg acc= 22.3%, forg= 56.9% <<<
>>> Test on task  1 : loss=0.998 | TAw acc= 66.9%, forg=  0.0%| TAg acc= 28.1%, forg= 24.1% <<<
>>> Test on task  2 : loss=0.655 | TAw acc= 77.0%, forg=  0.0%| TAg acc= 31.1%, forg= 20.8% <<<
>>> Test on task  3 : loss=0.620 | TAw acc= 79.1%, forg=  0.0%| TAg acc= 52.6%, forg=  4.6% <<<
>>> Test on task  4 : loss=0.521 | TAw acc= 83.2%, forg=  0.0%| TAg acc= 50.4%, forg=  0.0% <<<
Save at ../RESULT_AAAI2025/CR10/0/cifar100_sow
************************************************************************************************************
Task  5
************************************************************************************************************
LLL_Net(
  (model): ResNet50_32SOW(
    in_channels=3, in_H=32, in_W=32, training=False, fix_features=True, fix_classifier=False
    (features): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
      (3): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (4): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (3): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (5): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (3): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (4): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (5): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (6): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (7): AdaptiveAvgPool2d(output_size=(1, 1))
    )
    (classifier): Sequential(
      (0): Dropout(p=0.417598542370663, inplace=False)
      (1): SOW_V3(in_features=1024, out_features=1024, num_tasks=10, dtype=torch.float64, device=cuda:0)
      (2): ReLU()
    )
    (fc): Sequential()
  )
  (heads): ModuleList(
    (0-5): 6 x Linear(in_features=1024, out_features=10, bias=True)
  )
)
SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.0263039750973134
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0

Parameter Group 1
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.05
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0
)
 Group[0]  param_num=161, free_param_num=2
 Group[1]  param_num=3, free_param_num=3
sow: rank=1024, freezed_rank=896
 a:        ['-5.99e+00', '+4.97e+00'] .... ['+5.18e+00', '+5.27e+00'] .... ['+1.29e+00', '-2.74e+00']
 a_backup: ['-5.99e+00', '+4.97e+00'] .... ['+5.18e+00', '+5.27e+00'] .... ['+1.29e+00', '-2.74e+00']
| Epoch   1, lr=2.6e-02 time=  2.8s/  2.0s | Train: loss=1.239, TAw acc= 55.5% | Valid: time=  0.5s loss=1.080, TAw acc= 56.8% | *
| Epoch   2, lr=2.6e-02 time=  2.9s/  1.9s | Train: loss=1.190, TAw acc= 56.4% | Valid: time=  0.4s loss=0.977, TAw acc= 63.2% | *
| Epoch   3, lr=2.6e-02 time=  2.8s/  2.6s | Train: loss=1.160, TAw acc= 57.9% | Valid: time=  0.7s loss=0.997, TAw acc= 63.8% |
| Epoch   4, lr=2.6e-02 time=  2.8s/  2.4s | Train: loss=1.157, TAw acc= 59.6% | Valid: time=  0.5s loss=1.029, TAw acc= 60.8% |
| Epoch   5, lr=2.6e-02 time=  3.6s/  2.4s | Train: loss=1.114, TAw acc= 60.6% | Valid: time=  0.5s loss=0.991, TAw acc= 63.0% |
| Epoch   6, lr=2.6e-02 time=  3.6s/  2.2s | Train: loss=1.131, TAw acc= 59.0% | Valid: time=  0.5s loss=0.990, TAw acc= 64.2% |
| Epoch   7, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=1.103, TAw acc= 60.8% | Valid: time=  0.4s loss=0.974, TAw acc= 62.8% | *
| Epoch   8, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=1.121, TAw acc= 60.4% | Valid: time=  0.4s loss=0.948, TAw acc= 63.6% | *
| Epoch   9, lr=2.6e-02 time=  4.0s/  1.9s | Train: loss=1.105, TAw acc= 59.3% | Valid: time=  0.5s loss=1.026, TAw acc= 61.6% |
| Epoch  10, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.088, TAw acc= 60.7% | Valid: time=  0.5s loss=0.996, TAw acc= 66.0% |
| Epoch  11, lr=2.6e-02 time=  3.6s/  2.1s | Train: loss=1.074, TAw acc= 62.1% | Valid: time=  0.5s loss=0.932, TAw acc= 65.2% | *
| Epoch  12, lr=2.6e-02 time=  3.0s/  1.9s | Train: loss=1.109, TAw acc= 61.6% | Valid: time=  0.4s loss=0.896, TAw acc= 68.8% | *
| Epoch  13, lr=2.6e-02 time=  3.4s/  2.0s | Train: loss=1.064, TAw acc= 61.9% | Valid: time=  0.4s loss=0.928, TAw acc= 66.6% |
| Epoch  14, lr=2.6e-02 time=  2.7s/  1.9s | Train: loss=1.050, TAw acc= 62.1% | Valid: time=  0.4s loss=0.894, TAw acc= 68.6% | *
| Epoch  15, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=1.066, TAw acc= 62.3% | Valid: time=  0.6s loss=0.886, TAw acc= 68.2% | *
| Epoch  16, lr=2.6e-02 time=  3.6s/  2.2s | Train: loss=1.085, TAw acc= 61.2% | Valid: time=  0.5s loss=0.889, TAw acc= 68.0% |
| Epoch  17, lr=2.6e-02 time=  3.7s/  2.4s | Train: loss=1.083, TAw acc= 61.7% | Valid: time=  0.5s loss=0.896, TAw acc= 67.6% |
| Epoch  18, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.055, TAw acc= 63.4% | Valid: time=  0.5s loss=0.855, TAw acc= 68.6% | *
| Epoch  19, lr=2.6e-02 time=  3.4s/  2.0s | Train: loss=1.080, TAw acc= 61.6% | Valid: time=  0.4s loss=0.873, TAw acc= 67.0% |
| Epoch  20, lr=2.6e-02 time=  3.3s/  1.9s | Train: loss=1.071, TAw acc= 62.5% | Valid: time=  0.4s loss=0.910, TAw acc= 65.2% |
| Epoch  21, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=1.057, TAw acc= 62.0% | Valid: time=  0.4s loss=0.944, TAw acc= 67.0% |
| Epoch  22, lr=2.6e-02 time=  4.0s/  1.9s | Train: loss=1.051, TAw acc= 62.3% | Valid: time=  0.4s loss=0.892, TAw acc= 68.0% |
| Epoch  23, lr=2.6e-02 time=  3.1s/  2.1s | Train: loss=1.057, TAw acc= 62.0% | Valid: time=  0.4s loss=0.932, TAw acc= 67.0% |
| Epoch  24, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=1.035, TAw acc= 62.6% | Valid: time=  0.4s loss=0.892, TAw acc= 67.8% |
| Epoch  25, lr=2.6e-02 time=  2.7s/  2.0s | Train: loss=1.037, TAw acc= 62.9% | Valid: time=  0.4s loss=0.897, TAw acc= 68.8% |
| Epoch  26, lr=2.6e-02 time=  4.1s/  1.9s | Train: loss=1.046, TAw acc= 63.2% | Valid: time=  0.4s loss=0.866, TAw acc= 69.4% |
| Epoch  27, lr=2.6e-02 time=  3.4s/  2.4s | Train: loss=1.048, TAw acc= 61.8% | Valid: time=  0.5s loss=0.900, TAw acc= 66.8% |
| Epoch  28, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.017, TAw acc= 63.5% | Valid: time=  0.5s loss=0.871, TAw acc= 70.0% |
| Epoch  29, lr=2.6e-02 time=  3.2s/  1.9s | Train: loss=1.040, TAw acc= 63.7% | Valid: time=  0.7s loss=0.843, TAw acc= 70.4% | *
| Epoch  30, lr=2.6e-02 time=  2.8s/  2.0s | Train: loss=1.109, TAw acc= 61.6% | Valid: time=  0.4s loss=0.846, TAw acc= 68.4% |
| Epoch  31, lr=2.6e-02 time=  3.1s/  2.1s | Train: loss=1.028, TAw acc= 63.6% | Valid: time=  0.4s loss=0.861, TAw acc= 71.0% |
| Epoch  32, lr=2.6e-02 time=  2.9s/  1.9s | Train: loss=1.026, TAw acc= 64.3% | Valid: time=  0.4s loss=0.876, TAw acc= 68.4% |
| Epoch  33, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=1.047, TAw acc= 62.4% | Valid: time=  0.5s loss=0.879, TAw acc= 67.4% |
| Epoch  34, lr=2.6e-02 time=  2.8s/  3.2s | Train: loss=1.065, TAw acc= 62.7% | Valid: time=  0.4s loss=0.855, TAw acc= 67.8% |
| Epoch  35, lr=2.6e-02 time=  2.8s/  2.5s | Train: loss=1.030, TAw acc= 64.4% | Valid: time=  0.5s loss=0.884, TAw acc= 67.2% |
| Epoch  36, lr=2.6e-02 time=  3.6s/  2.4s | Train: loss=1.082, TAw acc= 62.1% | Valid: time=  0.5s loss=0.842, TAw acc= 70.4% | *
| Epoch  37, lr=2.6e-02 time=  3.6s/  2.3s | Train: loss=1.038, TAw acc= 63.7% | Valid: time=  0.4s loss=0.869, TAw acc= 68.4% |
| Epoch  38, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=1.043, TAw acc= 62.8% | Valid: time=  0.4s loss=0.872, TAw acc= 67.4% |
| Epoch  39, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=1.048, TAw acc= 62.7% | Valid: time=  0.8s loss=0.865, TAw acc= 68.4% |
| Epoch  40, lr=2.6e-02 time=  3.4s/  2.1s | Train: loss=1.041, TAw acc= 63.3% | Valid: time=  0.5s loss=0.858, TAw acc= 67.8% |
| Epoch  41, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.040, TAw acc= 62.7% | Valid: time=  0.5s loss=0.875, TAw acc= 67.2% |
| Epoch  42, lr=2.6e-02 time=  3.6s/  1.9s | Train: loss=1.034, TAw acc= 62.7% | Valid: time=  0.4s loss=0.856, TAw acc= 68.6% |
| Epoch  43, lr=2.6e-02 time=  2.7s/  1.9s | Train: loss=1.046, TAw acc= 62.5% | Valid: time=  0.4s loss=0.851, TAw acc= 68.4% |
| Epoch  44, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=1.010, TAw acc= 64.2% | Valid: time=  0.9s loss=0.866, TAw acc= 69.4% |
| Epoch  45, lr=2.6e-02 time=  3.1s/  2.2s | Train: loss=1.063, TAw acc= 62.4% | Valid: time=  0.5s loss=0.842, TAw acc= 69.6% |
| Epoch  46, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.024, TAw acc= 64.1% | Valid: time=  0.5s loss=0.856, TAw acc= 70.0% |
| Epoch  47, lr=2.6e-02 time=  3.2s/  1.9s | Train: loss=1.055, TAw acc= 61.8% | Valid: time=  0.4s loss=0.845, TAw acc= 69.4% |
| Epoch  48, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=1.031, TAw acc= 63.5% | Valid: time=  0.4s loss=0.854, TAw acc= 70.8% |
| Epoch  49, lr=2.6e-02 time=  4.1s/  1.9s | Train: loss=1.019, TAw acc= 63.9% | Valid: time=  0.4s loss=0.873, TAw acc= 67.8% |
| Epoch  50, lr=2.6e-02 time=  3.4s/  2.5s | Train: loss=1.062, TAw acc= 61.9% | Valid: time=  0.5s loss=0.847, TAw acc= 69.4% |
| Epoch  51, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.011, TAw acc= 64.6% | Valid: time=  0.5s loss=0.903, TAw acc= 66.8% |
| Epoch  52, lr=2.6e-02 time=  3.6s/  2.0s | Train: loss=1.024, TAw acc= 63.8% | Valid: time=  0.4s loss=0.879, TAw acc= 69.8% |
| Epoch  53, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=1.020, TAw acc= 63.6% | Valid: time=  0.5s loss=0.947, TAw acc= 66.2% |
| Epoch  54, lr=2.6e-02 time=  2.8s/  3.1s | Train: loss=1.022, TAw acc= 63.7% | Valid: time=  0.4s loss=0.874, TAw acc= 68.4% |
| Epoch  55, lr=2.6e-02 time=  2.8s/  2.5s | Train: loss=1.010, TAw acc= 63.6% | Valid: time=  0.5s loss=0.930, TAw acc= 66.4% |
| Epoch  56, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.035, TAw acc= 63.9% | Valid: time=  0.5s loss=0.885, TAw acc= 68.6% |
| Epoch  57, lr=2.6e-02 time=  3.6s/  2.3s | Train: loss=1.031, TAw acc= 63.0% | Valid: time=  0.5s loss=0.973, TAw acc= 65.8% |
| Epoch  58, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=1.035, TAw acc= 63.0% | Valid: time=  0.4s loss=0.854, TAw acc= 70.2% |
| Epoch  59, lr=2.6e-02 time=  2.9s/  1.9s | Train: loss=0.997, TAw acc= 63.8% | Valid: time=  0.4s loss=0.885, TAw acc= 68.2% |
| Epoch  60, lr=2.6e-02 time=  2.8s/  3.1s | Train: loss=1.016, TAw acc= 64.1% | Valid: time=  0.4s loss=0.903, TAw acc= 67.2% |
== Rank Reduction [task:5] ==
Debug-0:
  best_loss=0.842,   best_acc=0.704
loss_margin=0.006, acc_margin=0.004
sow: rank=1024, freezed_rank=896
 a:        ['-6.30e+00', '+4.96e+00'] .... ['+5.19e+00', '+5.28e+00'] .... ['+1.29e+00', '-2.74e+00']
 a_backup: ['-6.30e+00', '+4.96e+00'] .... ['+5.19e+00', '+5.28e+00'] .... ['+1.29e+00', '-2.74e+00']
 r=256, loss=1.870, acc=0.308 (rank=256)
 r=288, loss=1.739, acc=0.364 (rank=288)
 r=320, loss=1.537, acc=0.446 (rank=320)
 r=352, loss=1.370, acc=0.532 (rank=352)
 r=384, loss=1.282, acc=0.546 (rank=384)
 r=416, loss=1.172, acc=0.598 (rank=416)
 r=448, loss=1.073, acc=0.618 (rank=448)
 r=480, loss=0.935, acc=0.670 (rank=480)
 r=512, loss=0.851, acc=0.700 (rank=512)
 r=544, loss=0.850, acc=0.702 (rank=544)
 r=576, loss=0.856, acc=0.692 (rank=576)
 r=608, loss=0.846, acc=0.708 (rank=608)
 best_r=608, loss=0.846, acc=0.708
== Header Training for Low Rank [task:5] ==
Fix_Classifier: Done (training=False, fix_classifier=True)
Debug-1: loss=0.846, acc=0.708
sow: rank=608, freezed_rank=896
 a:        ['-6.30e+00', '+4.96e+00'] .... ['-1.80e+308', '-1.80e+308'] .... ['-1.80e+308', '-1.80e+308']
 a_backup: ['-6.30e+00', '+4.96e+00'] .... ['+5.19e+00', '+5.28e+00'] .... ['+1.29e+00', '-2.74e+00']
SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.008767991699104466
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0

Parameter Group 1
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.05
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0
)
 Group[0]  param_num=161, free_param_num=2
 Group[1]  param_num=3, free_param_num=0
sow: rank=608, freezed_rank=896
 a:        ['-6.30e+00', '+4.96e+00'] .... ['-1.80e+308', '-1.80e+308'] .... ['-1.80e+308', '-1.80e+308']
 a_backup: ['-6.30e+00', '+4.96e+00'] .... ['+5.19e+00', '+5.28e+00'] .... ['+1.29e+00', '-2.74e+00']
| Epoch   1, lr=8.8e-03 time=  2.1s/  2.4s | Train: loss=1.103, TAw acc= 62.1% | Valid: time=  0.5s loss=0.894, TAw acc= 66.8% |
| Epoch   2, lr=8.8e-03 time=  2.2s/  1.9s | Train: loss=1.059, TAw acc= 63.9% | Valid: time=  0.4s loss=0.894, TAw acc= 66.6% |
| Epoch   3, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=1.072, TAw acc= 63.1% | Valid: time=  0.4s loss=0.873, TAw acc= 67.6% |
| Epoch   4, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=1.070, TAw acc= 63.8% | Valid: time=  0.5s loss=0.886, TAw acc= 67.2% |
| Epoch   5, lr=8.8e-03 time=  2.9s/  1.9s | Train: loss=1.074, TAw acc= 63.2% | Valid: time=  0.4s loss=0.880, TAw acc= 67.0% |
| Epoch   6, lr=8.8e-03 time=  2.1s/  2.4s | Train: loss=1.059, TAw acc= 62.9% | Valid: time=  0.5s loss=0.867, TAw acc= 67.4% |
| Epoch   7, lr=8.8e-03 time=  2.1s/  2.4s | Train: loss=1.052, TAw acc= 63.7% | Valid: time=  0.5s loss=0.891, TAw acc= 68.2% |
| Epoch   8, lr=8.8e-03 time=  2.1s/  2.4s | Train: loss=1.044, TAw acc= 64.0% | Valid: time=  0.5s loss=0.893, TAw acc= 65.8% |
| Epoch   9, lr=8.8e-03 time=  2.2s/  2.1s | Train: loss=1.052, TAw acc= 63.0% | Valid: time=  0.4s loss=0.893, TAw acc= 66.4% |
| Epoch  10, lr=8.8e-03 time=  1.8s/  1.9s | Train: loss=1.041, TAw acc= 63.4% | Valid: time=  0.4s loss=0.886, TAw acc= 67.2% |
| Epoch  11, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=1.032, TAw acc= 64.7% | Valid: time=  0.4s loss=0.886, TAw acc= 67.4% |
| Epoch  12, lr=8.8e-03 time=  1.7s/  3.1s | Train: loss=1.045, TAw acc= 63.0% | Valid: time=  0.4s loss=0.894, TAw acc= 68.4% |
| Epoch  13, lr=8.8e-03 time=  1.7s/  2.1s | Train: loss=1.024, TAw acc= 64.9% | Valid: time=  0.5s loss=0.883, TAw acc= 67.2% |
| Epoch  14, lr=8.8e-03 time=  2.1s/  2.4s | Train: loss=1.048, TAw acc= 63.3% | Valid: time=  0.5s loss=0.884, TAw acc= 68.8% |
| Epoch  15, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=1.056, TAw acc= 62.9% | Valid: time=  0.5s loss=0.877, TAw acc= 67.4% |
| Epoch  16, lr=8.8e-03 time=  2.1s/  2.4s | Train: loss=1.015, TAw acc= 64.2% | Valid: time=  0.5s loss=0.868, TAw acc= 68.2% |
| Epoch  17, lr=8.8e-03 time=  1.9s/  1.9s | Train: loss=1.050, TAw acc= 63.5% | Valid: time=  0.4s loss=0.895, TAw acc= 65.2% |
| Epoch  18, lr=8.8e-03 time=  2.0s/  1.9s | Train: loss=1.015, TAw acc= 65.0% | Valid: time=  0.4s loss=0.881, TAw acc= 67.0% |
| Epoch  19, lr=8.8e-03 time=  1.9s/  2.0s | Train: loss=1.009, TAw acc= 64.8% | Valid: time=  0.4s loss=0.883, TAw acc= 67.8% |
| Epoch  20, lr=8.8e-03 time=  1.8s/  1.9s | Train: loss=1.006, TAw acc= 65.1% | Valid: time=  0.4s loss=0.874, TAw acc= 68.8% |
| Epoch  21, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=1.020, TAw acc= 65.1% | Valid: time=  0.4s loss=0.877, TAw acc= 69.0% |
| Epoch  22, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=1.008, TAw acc= 64.6% | Valid: time=  0.4s loss=0.876, TAw acc= 68.6% |
| Epoch  23, lr=8.8e-03 time=  2.9s/  1.9s | Train: loss=1.004, TAw acc= 64.2% | Valid: time=  0.4s loss=0.862, TAw acc= 68.8% |
| Epoch  24, lr=8.8e-03 time=  1.8s/  2.3s | Train: loss=1.001, TAw acc= 65.4% | Valid: time=  0.4s loss=0.875, TAw acc= 67.8% |
| Epoch  25, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.995, TAw acc= 65.6% | Valid: time=  0.4s loss=0.880, TAw acc= 68.4% |
| Epoch  26, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=1.004, TAw acc= 65.6% | Valid: time=  0.4s loss=0.862, TAw acc= 69.4% |
| Epoch  27, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.991, TAw acc= 66.1% | Valid: time=  0.4s loss=0.891, TAw acc= 69.2% |
| Epoch  28, lr=8.8e-03 time=  2.9s/  1.9s | Train: loss=0.992, TAw acc= 65.6% | Valid: time=  0.4s loss=0.873, TAw acc= 69.4% |
| Epoch  29, lr=8.8e-03 time=  1.9s/  2.4s | Train: loss=1.012, TAw acc= 64.4% | Valid: time=  0.5s loss=0.872, TAw acc= 69.8% |
| Epoch  30, lr=8.8e-03 time=  2.1s/  2.4s | Train: loss=1.009, TAw acc= 64.5% | Valid: time=  0.5s loss=0.886, TAw acc= 67.0% | lr=2.9e-03
| Epoch  31, lr=2.9e-03 time=  2.1s/  2.4s | Train: loss=1.099, TAw acc= 63.2% | Valid: time=  0.5s loss=0.879, TAw acc= 68.0% |
| Epoch  32, lr=2.9e-03 time=  1.7s/  2.3s | Train: loss=1.093, TAw acc= 63.8% | Valid: time=  0.4s loss=0.891, TAw acc= 67.8% |
| Epoch  33, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=1.109, TAw acc= 62.3% | Valid: time=  0.5s loss=0.895, TAw acc= 67.4% |
| Epoch  34, lr=2.9e-03 time=  1.7s/  3.1s | Train: loss=1.088, TAw acc= 63.0% | Valid: time=  0.4s loss=0.898, TAw acc= 66.0% |
| Epoch  35, lr=2.9e-03 time=  1.7s/  2.3s | Train: loss=1.092, TAw acc= 62.7% | Valid: time=  0.5s loss=0.901, TAw acc= 65.8% |
| Epoch  36, lr=2.9e-03 time=  2.2s/  2.4s | Train: loss=1.088, TAw acc= 63.4% | Valid: time=  0.5s loss=0.894, TAw acc= 65.4% |
| Epoch  37, lr=2.9e-03 time=  2.1s/  2.4s | Train: loss=1.085, TAw acc= 63.1% | Valid: time=  0.5s loss=0.894, TAw acc= 65.4% |
| Epoch  38, lr=2.9e-03 time=  2.1s/  2.4s | Train: loss=1.076, TAw acc= 62.9% | Valid: time=  0.5s loss=0.894, TAw acc= 66.0% |
| Epoch  39, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=1.068, TAw acc= 63.5% | Valid: time=  0.4s loss=0.890, TAw acc= 66.8% |
| Epoch  40, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=1.084, TAw acc= 62.9% | Valid: time=  0.4s loss=0.887, TAw acc= 67.8% |
| Epoch  41, lr=2.9e-03 time=  1.7s/  2.5s | Train: loss=1.055, TAw acc= 63.5% | Valid: time=  0.8s loss=0.886, TAw acc= 66.8% |
| Epoch  42, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=1.060, TAw acc= 63.6% | Valid: time=  0.5s loss=0.887, TAw acc= 66.4% |
| Epoch  43, lr=2.9e-03 time=  2.2s/  2.4s | Train: loss=1.073, TAw acc= 63.7% | Valid: time=  0.5s loss=0.881, TAw acc= 66.2% |
| Epoch  44, lr=2.9e-03 time=  2.1s/  2.4s | Train: loss=1.058, TAw acc= 63.5% | Valid: time=  0.5s loss=0.886, TAw acc= 67.0% |
| Epoch  45, lr=2.9e-03 time=  2.1s/  2.4s | Train: loss=1.063, TAw acc= 62.5% | Valid: time=  0.5s loss=0.890, TAw acc= 66.4% |
| Epoch  46, lr=2.9e-03 time=  2.2s/  2.4s | Train: loss=1.060, TAw acc= 63.4% | Valid: time=  0.5s loss=0.886, TAw acc= 66.6% |
| Epoch  47, lr=2.9e-03 time=  2.1s/  2.4s | Train: loss=1.053, TAw acc= 63.9% | Valid: time=  0.5s loss=0.888, TAw acc= 67.2% |
| Epoch  48, lr=2.9e-03 time=  1.8s/  2.1s | Train: loss=1.047, TAw acc= 64.6% | Valid: time=  0.5s loss=0.886, TAw acc= 66.0% |
| Epoch  49, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=1.053, TAw acc= 63.8% | Valid: time=  0.4s loss=0.891, TAw acc= 65.6% |
| Epoch  50, lr=2.9e-03 time=  2.0s/  1.9s | Train: loss=1.062, TAw acc= 64.0% | Valid: time=  0.4s loss=0.879, TAw acc= 67.2% |
| Epoch  51, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=1.061, TAw acc= 62.9% | Valid: time=  0.4s loss=0.885, TAw acc= 66.8% |
| Epoch  52, lr=2.9e-03 time=  1.7s/  3.0s | Train: loss=1.058, TAw acc= 63.1% | Valid: time=  0.4s loss=0.883, TAw acc= 67.0% |
| Epoch  53, lr=2.9e-03 time=  1.7s/  2.0s | Train: loss=1.042, TAw acc= 64.4% | Valid: time=  0.5s loss=0.886, TAw acc= 67.0% |
| Epoch  54, lr=2.9e-03 time=  2.2s/  2.4s | Train: loss=1.042, TAw acc= 64.6% | Valid: time=  0.5s loss=0.875, TAw acc= 67.8% |
| Epoch  55, lr=2.9e-03 time=  2.1s/  2.4s | Train: loss=1.045, TAw acc= 64.2% | Valid: time=  0.5s loss=0.875, TAw acc= 67.4% |
| Epoch  56, lr=2.9e-03 time=  1.9s/  1.9s | Train: loss=1.058, TAw acc= 63.1% | Valid: time=  0.4s loss=0.876, TAw acc= 68.0% |
| Epoch  57, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=1.051, TAw acc= 63.2% | Valid: time=  0.4s loss=0.877, TAw acc= 67.6% |
| Epoch  58, lr=2.9e-03 time=  1.7s/  2.6s | Train: loss=1.043, TAw acc= 62.8% | Valid: time=  0.7s loss=0.876, TAw acc= 68.0% |
| Epoch  59, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=1.036, TAw acc= 64.2% | Valid: time=  0.4s loss=0.882, TAw acc= 67.2% |
| Epoch  60, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=1.036, TAw acc= 64.5% | Valid: time=  0.4s loss=0.882, TAw acc= 66.8% | lr=9.7e-04
Debug-2: loss=0.846, acc=0.708
sow: rank=608, freezed_rank=896
 a:        ['-6.30e+00', '+4.96e+00'] .... ['-1.80e+308', '-1.80e+308'] .... ['-1.80e+308', '-1.80e+308']
 a_backup: ['-6.30e+00', '+4.96e+00'] .... ['+5.19e+00', '+5.28e+00'] .... ['+1.29e+00', '-2.74e+00']
Free_Classifier: Done (training=True, fix_classifier=False)
Debug-3: loss=0.842 acc=0.704
sow: rank=1024, freezed_rank=896
 a:        ['-6.30e+00', '+4.96e+00'] .... ['+5.19e+00', '+5.28e+00'] .... ['+1.29e+00', '-2.74e+00']
 a_backup: ['-6.30e+00', '+4.96e+00'] .... ['+5.19e+00', '+5.28e+00'] .... ['+1.29e+00', '-2.74e+00']
Header Training: Finished.
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.649 | TAw acc= 79.2%, forg=  0.0%| TAg acc= 19.1%, forg= 60.1% <<<
>>> Test on task  1 : loss=0.998 | TAw acc= 66.9%, forg=  0.0%| TAg acc= 24.9%, forg= 27.3% <<<
>>> Test on task  2 : loss=0.655 | TAw acc= 77.0%, forg=  0.0%| TAg acc= 26.8%, forg= 25.1% <<<
>>> Test on task  3 : loss=0.620 | TAw acc= 79.1%, forg=  0.0%| TAg acc= 46.4%, forg= 10.8% <<<
>>> Test on task  4 : loss=0.521 | TAw acc= 83.2%, forg=  0.0%| TAg acc= 45.3%, forg=  5.1% <<<
>>> Test on task  5 : loss=0.848 | TAw acc= 66.3%, forg=  0.0%| TAg acc= 32.8%, forg=  0.0% <<<
Save at ../RESULT_AAAI2025/CR10/0/cifar100_sow
************************************************************************************************************
Task  6
************************************************************************************************************
LLL_Net(
  (model): ResNet50_32SOW(
    in_channels=3, in_H=32, in_W=32, training=False, fix_features=True, fix_classifier=False
    (features): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
      (3): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (4): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (3): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (5): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (3): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (4): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (5): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (6): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (7): AdaptiveAvgPool2d(output_size=(1, 1))
    )
    (classifier): Sequential(
      (0): Dropout(p=0.417598542370663, inplace=False)
      (1): SOW_V3(in_features=1024, out_features=1024, num_tasks=10, dtype=torch.float64, device=cuda:0)
      (2): ReLU()
    )
    (fc): Sequential()
  )
  (heads): ModuleList(
    (0-6): 7 x Linear(in_features=1024, out_features=10, bias=True)
  )
)
SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.0263039750973134
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0

Parameter Group 1
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.05
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0
)
 Group[0]  param_num=161, free_param_num=2
 Group[1]  param_num=3, free_param_num=3
sow: rank=1024, freezed_rank=896
 a:        ['-6.30e+00', '+4.96e+00'] .... ['+5.19e+00', '+5.28e+00'] .... ['+1.29e+00', '-2.74e+00']
 a_backup: ['-6.30e+00', '+4.96e+00'] .... ['+5.19e+00', '+5.28e+00'] .... ['+1.29e+00', '-2.74e+00']
| Epoch   1, lr=2.6e-02 time=  4.1s/  2.0s | Train: loss=1.200, TAw acc= 60.7% | Valid: time=  0.5s loss=0.773, TAw acc= 73.2% | *
| Epoch   2, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.154, TAw acc= 61.9% | Valid: time=  0.5s loss=0.700, TAw acc= 77.0% | *
| Epoch   3, lr=2.6e-02 time=  3.4s/  1.9s | Train: loss=1.117, TAw acc= 62.8% | Valid: time=  0.4s loss=0.720, TAw acc= 74.0% |
| Epoch   4, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=1.060, TAw acc= 65.6% | Valid: time=  0.4s loss=0.678, TAw acc= 76.8% | *
| Epoch   5, lr=2.6e-02 time=  4.0s/  1.9s | Train: loss=1.060, TAw acc= 65.7% | Valid: time=  0.5s loss=0.749, TAw acc= 73.6% |
| Epoch   6, lr=2.6e-02 time=  3.6s/  2.4s | Train: loss=1.058, TAw acc= 65.3% | Valid: time=  0.5s loss=0.714, TAw acc= 74.2% |
| Epoch   7, lr=2.6e-02 time=  3.7s/  2.5s | Train: loss=1.056, TAw acc= 65.2% | Valid: time=  0.5s loss=0.676, TAw acc= 77.2% | *
| Epoch   8, lr=2.6e-02 time=  3.6s/  2.4s | Train: loss=1.024, TAw acc= 66.7% | Valid: time=  0.5s loss=0.714, TAw acc= 73.2% |
| Epoch   9, lr=2.6e-02 time=  3.1s/  1.9s | Train: loss=1.068, TAw acc= 64.8% | Valid: time=  0.4s loss=0.653, TAw acc= 77.2% | *
| Epoch  10, lr=2.6e-02 time=  2.8s/  3.1s | Train: loss=1.020, TAw acc= 66.2% | Valid: time=  0.4s loss=0.724, TAw acc= 74.2% |
| Epoch  11, lr=2.6e-02 time=  3.0s/  2.5s | Train: loss=1.053, TAw acc= 64.7% | Valid: time=  0.5s loss=0.641, TAw acc= 77.4% | *
| Epoch  12, lr=2.6e-02 time=  3.6s/  2.4s | Train: loss=1.028, TAw acc= 66.0% | Valid: time=  0.5s loss=0.613, TAw acc= 78.6% | *
| Epoch  13, lr=2.6e-02 time=  3.6s/  2.1s | Train: loss=1.025, TAw acc= 66.4% | Valid: time=  0.4s loss=0.616, TAw acc= 79.2% |
| Epoch  14, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=1.022, TAw acc= 66.4% | Valid: time=  0.5s loss=0.691, TAw acc= 75.4% |
| Epoch  15, lr=2.6e-02 time=  3.9s/  1.9s | Train: loss=1.010, TAw acc= 66.8% | Valid: time=  0.4s loss=0.657, TAw acc= 77.2% |
| Epoch  16, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=1.026, TAw acc= 65.5% | Valid: time=  0.4s loss=0.657, TAw acc= 79.6% |
| Epoch  17, lr=2.6e-02 time=  2.8s/  3.0s | Train: loss=1.024, TAw acc= 66.1% | Valid: time=  0.4s loss=0.651, TAw acc= 76.6% |
| Epoch  18, lr=2.6e-02 time=  3.1s/  2.5s | Train: loss=0.960, TAw acc= 68.6% | Valid: time=  0.5s loss=0.672, TAw acc= 76.4% |
| Epoch  19, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.055, TAw acc= 65.1% | Valid: time=  0.5s loss=0.619, TAw acc= 78.2% |
| Epoch  20, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.032, TAw acc= 66.3% | Valid: time=  0.5s loss=0.637, TAw acc= 79.4% |
| Epoch  21, lr=2.6e-02 time=  3.0s/  1.9s | Train: loss=1.014, TAw acc= 66.3% | Valid: time=  0.4s loss=0.682, TAw acc= 76.8% |
| Epoch  22, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=1.007, TAw acc= 66.6% | Valid: time=  0.9s loss=0.707, TAw acc= 73.8% |
| Epoch  23, lr=2.6e-02 time=  3.2s/  2.1s | Train: loss=1.003, TAw acc= 66.0% | Valid: time=  0.5s loss=0.623, TAw acc= 79.0% |
| Epoch  24, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.015, TAw acc= 66.6% | Valid: time=  0.5s loss=0.608, TAw acc= 79.4% | *
| Epoch  25, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.010, TAw acc= 66.8% | Valid: time=  0.5s loss=0.642, TAw acc= 76.8% |
| Epoch  26, lr=2.6e-02 time=  3.6s/  2.4s | Train: loss=0.973, TAw acc= 67.2% | Valid: time=  0.5s loss=0.662, TAw acc= 78.0% |
| Epoch  27, lr=2.6e-02 time=  3.3s/  1.9s | Train: loss=0.986, TAw acc= 67.5% | Valid: time=  0.4s loss=0.630, TAw acc= 78.4% |
| Epoch  28, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=0.994, TAw acc= 66.9% | Valid: time=  0.5s loss=0.677, TAw acc= 77.4% |
| Epoch  29, lr=2.6e-02 time=  4.0s/  1.9s | Train: loss=0.987, TAw acc= 67.0% | Valid: time=  0.5s loss=0.668, TAw acc= 77.2% |
| Epoch  30, lr=2.6e-02 time=  3.7s/  2.4s | Train: loss=0.969, TAw acc= 67.8% | Valid: time=  0.5s loss=0.622, TAw acc= 78.6% |
| Epoch  31, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.966, TAw acc= 68.5% | Valid: time=  0.5s loss=0.686, TAw acc= 77.0% |
| Epoch  32, lr=2.6e-02 time=  3.6s/  1.9s | Train: loss=1.022, TAw acc= 67.2% | Valid: time=  0.4s loss=0.637, TAw acc= 77.6% |
| Epoch  33, lr=2.6e-02 time=  2.8s/  2.7s | Train: loss=0.983, TAw acc= 67.0% | Valid: time=  0.4s loss=0.668, TAw acc= 77.2% |
| Epoch  34, lr=2.6e-02 time=  2.8s/  2.0s | Train: loss=0.982, TAw acc= 68.1% | Valid: time=  0.7s loss=0.656, TAw acc= 79.4% |
| Epoch  35, lr=2.6e-02 time=  3.0s/  1.9s | Train: loss=0.996, TAw acc= 67.3% | Valid: time=  0.6s loss=0.670, TAw acc= 76.6% |
| Epoch  36, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=0.970, TAw acc= 68.0% | Valid: time=  0.4s loss=0.657, TAw acc= 78.8% |
| Epoch  37, lr=2.6e-02 time=  2.8s/  2.0s | Train: loss=0.995, TAw acc= 67.3% | Valid: time=  0.4s loss=0.630, TAw acc= 78.8% |
| Epoch  38, lr=2.6e-02 time=  4.1s/  1.9s | Train: loss=0.980, TAw acc= 68.0% | Valid: time=  0.4s loss=0.685, TAw acc= 78.6% |
| Epoch  39, lr=2.6e-02 time=  3.5s/  2.5s | Train: loss=0.971, TAw acc= 68.4% | Valid: time=  0.5s loss=0.653, TAw acc= 79.4% |
| Epoch  40, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.000, TAw acc= 67.2% | Valid: time=  0.5s loss=0.604, TAw acc= 79.2% | *
| Epoch  41, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.002, TAw acc= 67.7% | Valid: time=  0.5s loss=0.607, TAw acc= 78.8% |
| Epoch  42, lr=2.6e-02 time=  2.9s/  1.9s | Train: loss=0.964, TAw acc= 68.0% | Valid: time=  0.4s loss=0.614, TAw acc= 80.6% |
| Epoch  43, lr=2.6e-02 time=  3.6s/  1.9s | Train: loss=0.992, TAw acc= 66.6% | Valid: time=  0.4s loss=0.595, TAw acc= 78.4% | *
| Epoch  44, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=0.991, TAw acc= 66.7% | Valid: time=  0.4s loss=0.634, TAw acc= 78.2% |
| Epoch  45, lr=2.6e-02 time=  2.8s/  3.2s | Train: loss=0.969, TAw acc= 68.2% | Valid: time=  0.4s loss=0.629, TAw acc= 80.0% |
| Epoch  46, lr=2.6e-02 time=  2.8s/  2.5s | Train: loss=1.032, TAw acc= 64.6% | Valid: time=  0.5s loss=0.585, TAw acc= 80.6% | *
| Epoch  47, lr=2.6e-02 time=  3.6s/  2.4s | Train: loss=0.973, TAw acc= 67.3% | Valid: time=  0.5s loss=0.631, TAw acc= 80.4% |
| Epoch  48, lr=2.6e-02 time=  3.6s/  2.3s | Train: loss=0.972, TAw acc= 67.8% | Valid: time=  0.4s loss=0.638, TAw acc= 79.2% |
| Epoch  49, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=0.987, TAw acc= 67.6% | Valid: time=  0.4s loss=0.605, TAw acc= 79.6% |
| Epoch  50, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=0.970, TAw acc= 67.8% | Valid: time=  0.4s loss=0.609, TAw acc= 79.4% |
| Epoch  51, lr=2.6e-02 time=  4.0s/  2.0s | Train: loss=1.004, TAw acc= 67.1% | Valid: time=  0.5s loss=0.582, TAw acc= 81.0% | *
| Epoch  52, lr=2.6e-02 time=  3.7s/  2.5s | Train: loss=0.995, TAw acc= 66.7% | Valid: time=  0.5s loss=0.604, TAw acc= 79.6% |
| Epoch  53, lr=2.6e-02 time=  3.6s/  2.1s | Train: loss=0.955, TAw acc= 67.6% | Valid: time=  0.4s loss=0.658, TAw acc= 77.4% |
| Epoch  54, lr=2.6e-02 time=  2.8s/  2.9s | Train: loss=0.985, TAw acc= 67.3% | Valid: time=  0.4s loss=0.605, TAw acc= 78.0% |
| Epoch  55, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=0.993, TAw acc= 66.8% | Valid: time=  0.4s loss=0.606, TAw acc= 79.8% |
| Epoch  56, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=0.980, TAw acc= 67.3% | Valid: time=  0.5s loss=0.615, TAw acc= 79.4% |
| Epoch  57, lr=2.6e-02 time=  2.8s/  3.2s | Train: loss=0.965, TAw acc= 67.5% | Valid: time=  0.4s loss=0.604, TAw acc= 79.8% |
| Epoch  58, lr=2.6e-02 time=  2.9s/  2.5s | Train: loss=1.003, TAw acc= 66.8% | Valid: time=  0.5s loss=0.612, TAw acc= 79.2% |
| Epoch  59, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.002, TAw acc= 67.2% | Valid: time=  0.5s loss=0.623, TAw acc= 80.2% |
| Epoch  60, lr=2.6e-02 time=  3.6s/  2.4s | Train: loss=0.968, TAw acc= 67.5% | Valid: time=  0.5s loss=0.610, TAw acc= 79.0% |
== Rank Reduction [task:6] ==
Debug-0:
  best_loss=0.582,   best_acc=0.810
loss_margin=0.006, acc_margin=0.004
sow: rank=1024, freezed_rank=896
 a:        ['-6.34e+00', '+4.96e+00'] .... ['+5.21e+00', '+5.30e+00'] .... ['+1.29e+00', '-2.74e+00']
 a_backup: ['-6.34e+00', '+4.96e+00'] .... ['+5.21e+00', '+5.30e+00'] .... ['+1.29e+00', '-2.74e+00']
 r=256, loss=2.507, acc=0.140 (rank=256)
 r=288, loss=2.332, acc=0.180 (rank=288)
 r=320, loss=1.985, acc=0.250 (rank=320)
 r=352, loss=1.707, acc=0.348 (rank=352)
 r=384, loss=1.512, acc=0.450 (rank=384)
 r=416, loss=1.241, acc=0.552 (rank=416)
 r=448, loss=0.925, acc=0.686 (rank=448)
 r=480, loss=0.844, acc=0.718 (rank=480)
 r=512, loss=0.734, acc=0.754 (rank=512)
 r=544, loss=0.732, acc=0.754 (rank=544)
 r=576, loss=0.714, acc=0.760 (rank=576)
 r=608, loss=0.719, acc=0.760 (rank=608)
 r=640, loss=0.702, acc=0.768 (rank=640)
 r=672, loss=0.684, acc=0.762 (rank=672)
 r=704, loss=0.647, acc=0.784 (rank=704)
 r=736, loss=0.644, acc=0.788 (rank=736)
 r=768, loss=0.614, acc=0.792 (rank=768)
 r=800, loss=0.609, acc=0.796 (rank=800)
 r=832, loss=0.607, acc=0.794 (rank=832)
 r=864, loss=0.603, acc=0.802 (rank=864)
 r=896, loss=0.588, acc=0.812 (rank=896)
 best_r=896, loss=0.588, acc=0.812
== Header Training for Low Rank [task:6] ==
Fix_Classifier: Done (training=False, fix_classifier=True)
Debug-1: loss=0.588, acc=0.812
sow: rank=896, freezed_rank=896
 a:        ['-6.34e+00', '+4.96e+00'] .... ['+5.21e+00', '-1.80e+308'] .... ['-1.80e+308', '-1.80e+308']
 a_backup: ['-6.34e+00', '+4.96e+00'] .... ['+5.21e+00', '+5.30e+00'] .... ['+1.29e+00', '-2.74e+00']
SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.008767991699104466
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0

Parameter Group 1
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.05
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0
)
 Group[0]  param_num=161, free_param_num=2
 Group[1]  param_num=3, free_param_num=0
sow: rank=896, freezed_rank=896
 a:        ['-6.34e+00', '+4.96e+00'] .... ['+5.21e+00', '-1.80e+308'] .... ['-1.80e+308', '-1.80e+308']
 a_backup: ['-6.34e+00', '+4.96e+00'] .... ['+5.21e+00', '+5.30e+00'] .... ['+1.29e+00', '-2.74e+00']
| Epoch   1, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=0.967, TAw acc= 67.9% | Valid: time=  0.5s loss=0.663, TAw acc= 77.4% |
| Epoch   2, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=0.956, TAw acc= 69.3% | Valid: time=  0.5s loss=0.661, TAw acc= 78.0% |
| Epoch   3, lr=8.8e-03 time=  2.2s/  2.1s | Train: loss=0.937, TAw acc= 70.0% | Valid: time=  0.5s loss=0.682, TAw acc= 77.4% |
| Epoch   4, lr=8.8e-03 time=  1.7s/  2.4s | Train: loss=0.935, TAw acc= 69.8% | Valid: time=  0.4s loss=0.666, TAw acc= 77.8% |
| Epoch   5, lr=8.8e-03 time=  1.8s/  1.9s | Train: loss=0.960, TAw acc= 69.3% | Valid: time=  0.4s loss=0.659, TAw acc= 77.8% |
| Epoch   6, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.937, TAw acc= 69.9% | Valid: time=  0.5s loss=0.664, TAw acc= 77.4% |
| Epoch   7, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.919, TAw acc= 70.0% | Valid: time=  0.9s loss=0.675, TAw acc= 77.2% |
| Epoch   8, lr=8.8e-03 time=  2.0s/  1.9s | Train: loss=0.948, TAw acc= 69.1% | Valid: time=  0.5s loss=0.680, TAw acc= 77.4% |
| Epoch   9, lr=8.8e-03 time=  1.7s/  2.3s | Train: loss=0.947, TAw acc= 69.0% | Valid: time=  0.5s loss=0.656, TAw acc= 78.6% |
| Epoch  10, lr=8.8e-03 time=  1.9s/  1.9s | Train: loss=0.937, TAw acc= 69.4% | Valid: time=  0.4s loss=0.660, TAw acc= 77.0% |
| Epoch  11, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.922, TAw acc= 69.9% | Valid: time=  0.4s loss=0.670, TAw acc= 77.8% |
| Epoch  12, lr=8.8e-03 time=  1.8s/  3.2s | Train: loss=0.915, TAw acc= 70.6% | Valid: time=  0.5s loss=0.660, TAw acc= 77.8% |
| Epoch  13, lr=8.8e-03 time=  1.8s/  2.3s | Train: loss=0.920, TAw acc= 69.5% | Valid: time=  0.6s loss=0.656, TAw acc= 77.4% |
| Epoch  14, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=0.910, TAw acc= 69.5% | Valid: time=  0.5s loss=0.661, TAw acc= 78.4% |
| Epoch  15, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=0.916, TAw acc= 70.2% | Valid: time=  0.5s loss=0.660, TAw acc= 77.6% |
| Epoch  16, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=0.931, TAw acc= 69.5% | Valid: time=  0.4s loss=0.651, TAw acc= 77.6% |
| Epoch  17, lr=8.8e-03 time=  1.9s/  2.0s | Train: loss=0.921, TAw acc= 69.8% | Valid: time=  0.5s loss=0.673, TAw acc= 77.4% |
| Epoch  18, lr=8.8e-03 time=  2.3s/  2.0s | Train: loss=0.921, TAw acc= 70.5% | Valid: time=  0.6s loss=0.656, TAw acc= 77.8% |
| Epoch  19, lr=8.8e-03 time=  1.8s/  2.1s | Train: loss=0.921, TAw acc= 69.9% | Valid: time=  0.5s loss=0.648, TAw acc= 77.6% |
| Epoch  20, lr=8.8e-03 time=  1.8s/  2.0s | Train: loss=0.923, TAw acc= 69.8% | Valid: time=  0.4s loss=0.649, TAw acc= 77.6% |
| Epoch  21, lr=8.8e-03 time=  1.7s/  3.1s | Train: loss=0.904, TAw acc= 70.7% | Valid: time=  0.4s loss=0.677, TAw acc= 77.0% |
| Epoch  22, lr=8.8e-03 time=  1.7s/  2.5s | Train: loss=0.918, TAw acc= 70.1% | Valid: time=  0.5s loss=0.672, TAw acc= 77.6% |
| Epoch  23, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=0.920, TAw acc= 70.5% | Valid: time=  0.5s loss=0.660, TAw acc= 78.0% |
| Epoch  24, lr=8.8e-03 time=  2.2s/  2.1s | Train: loss=0.922, TAw acc= 69.9% | Valid: time=  0.4s loss=0.658, TAw acc= 77.0% |
| Epoch  25, lr=8.8e-03 time=  1.7s/  2.4s | Train: loss=0.899, TAw acc= 70.2% | Valid: time=  0.4s loss=0.645, TAw acc= 77.8% |
| Epoch  26, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.897, TAw acc= 69.8% | Valid: time=  0.4s loss=0.662, TAw acc= 76.6% |
| Epoch  27, lr=8.8e-03 time=  1.7s/  2.0s | Train: loss=0.908, TAw acc= 70.2% | Valid: time=  0.4s loss=0.660, TAw acc= 77.0% |
| Epoch  28, lr=8.8e-03 time=  1.7s/  3.2s | Train: loss=0.902, TAw acc= 70.7% | Valid: time=  0.5s loss=0.655, TAw acc= 77.6% |
| Epoch  29, lr=8.8e-03 time=  1.7s/  2.5s | Train: loss=0.905, TAw acc= 69.9% | Valid: time=  0.5s loss=0.654, TAw acc= 77.6% |
| Epoch  30, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=0.918, TAw acc= 70.8% | Valid: time=  0.5s loss=0.652, TAw acc= 77.6% | lr=2.9e-03
| Epoch  31, lr=2.9e-03 time=  2.2s/  1.9s | Train: loss=0.966, TAw acc= 68.8% | Valid: time=  0.4s loss=0.642, TAw acc= 78.4% |
| Epoch  32, lr=2.9e-03 time=  1.8s/  1.9s | Train: loss=0.968, TAw acc= 69.1% | Valid: time=  0.4s loss=0.662, TAw acc= 77.6% |
| Epoch  33, lr=2.9e-03 time=  1.8s/  1.9s | Train: loss=0.964, TAw acc= 69.0% | Valid: time=  0.4s loss=0.669, TAw acc= 77.8% |
| Epoch  34, lr=2.9e-03 time=  2.9s/  1.9s | Train: loss=0.949, TAw acc= 69.8% | Valid: time=  0.4s loss=0.666, TAw acc= 77.4% |
| Epoch  35, lr=2.9e-03 time=  2.0s/  2.1s | Train: loss=0.952, TAw acc= 70.2% | Valid: time=  0.5s loss=0.666, TAw acc= 77.2% |
| Epoch  36, lr=2.9e-03 time=  1.7s/  2.0s | Train: loss=0.938, TAw acc= 70.1% | Valid: time=  0.9s loss=0.672, TAw acc= 77.2% |
| Epoch  37, lr=2.9e-03 time=  1.8s/  1.9s | Train: loss=0.965, TAw acc= 69.3% | Valid: time=  0.5s loss=0.663, TAw acc= 77.0% |
| Epoch  38, lr=2.9e-03 time=  2.2s/  2.5s | Train: loss=0.953, TAw acc= 70.2% | Valid: time=  0.5s loss=0.667, TAw acc= 77.2% |
| Epoch  39, lr=2.9e-03 time=  2.2s/  2.5s | Train: loss=0.939, TAw acc= 69.9% | Valid: time=  0.5s loss=0.664, TAw acc= 77.6% |
| Epoch  40, lr=2.9e-03 time=  2.2s/  2.2s | Train: loss=0.943, TAw acc= 69.8% | Valid: time=  0.6s loss=0.669, TAw acc= 76.8% |
| Epoch  41, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=0.945, TAw acc= 69.4% | Valid: time=  0.4s loss=0.661, TAw acc= 77.0% |
| Epoch  42, lr=2.9e-03 time=  1.8s/  1.9s | Train: loss=0.953, TAw acc= 68.9% | Valid: time=  0.4s loss=0.663, TAw acc= 76.6% |
| Epoch  43, lr=2.9e-03 time=  3.0s/  1.9s | Train: loss=0.958, TAw acc= 69.0% | Valid: time=  0.4s loss=0.663, TAw acc= 76.4% |
| Epoch  44, lr=2.9e-03 time=  2.1s/  2.5s | Train: loss=0.937, TAw acc= 70.1% | Valid: time=  0.5s loss=0.660, TAw acc= 77.2% |
| Epoch  45, lr=2.9e-03 time=  2.2s/  2.5s | Train: loss=0.938, TAw acc= 69.6% | Valid: time=  0.5s loss=0.657, TAw acc= 77.2% |
| Epoch  46, lr=2.9e-03 time=  2.2s/  2.5s | Train: loss=0.925, TAw acc= 70.8% | Valid: time=  0.4s loss=0.666, TAw acc= 76.8% |
| Epoch  47, lr=2.9e-03 time=  1.8s/  2.1s | Train: loss=0.937, TAw acc= 69.7% | Valid: time=  0.4s loss=0.665, TAw acc= 77.2% |
| Epoch  48, lr=2.9e-03 time=  1.7s/  2.4s | Train: loss=0.957, TAw acc= 69.4% | Valid: time=  0.4s loss=0.663, TAw acc= 76.6% |
| Epoch  49, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=0.955, TAw acc= 69.4% | Valid: time=  0.5s loss=0.663, TAw acc= 76.8% |
| Epoch  50, lr=2.9e-03 time=  2.2s/  2.1s | Train: loss=0.931, TAw acc= 69.7% | Valid: time=  0.5s loss=0.664, TAw acc= 77.0% |
| Epoch  51, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=0.949, TAw acc= 69.1% | Valid: time=  0.4s loss=0.665, TAw acc= 77.0% |
| Epoch  52, lr=2.9e-03 time=  1.7s/  2.0s | Train: loss=0.944, TAw acc= 69.0% | Valid: time=  0.4s loss=0.664, TAw acc= 77.2% |
| Epoch  53, lr=2.9e-03 time=  1.7s/  3.2s | Train: loss=0.956, TAw acc= 69.4% | Valid: time=  0.4s loss=0.661, TAw acc= 77.4% |
| Epoch  54, lr=2.9e-03 time=  1.7s/  2.1s | Train: loss=0.940, TAw acc= 70.3% | Valid: time=  0.6s loss=0.668, TAw acc= 77.0% |
| Epoch  55, lr=2.9e-03 time=  2.2s/  2.5s | Train: loss=0.921, TAw acc= 70.3% | Valid: time=  0.5s loss=0.670, TAw acc= 77.2% |
| Epoch  56, lr=2.9e-03 time=  2.2s/  2.4s | Train: loss=0.953, TAw acc= 68.8% | Valid: time=  0.5s loss=0.661, TAw acc= 77.0% |
| Epoch  57, lr=2.9e-03 time=  2.2s/  2.0s | Train: loss=0.925, TAw acc= 70.5% | Valid: time=  0.4s loss=0.660, TAw acc= 77.2% |
| Epoch  58, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=0.951, TAw acc= 68.6% | Valid: time=  0.4s loss=0.662, TAw acc= 77.0% |
| Epoch  59, lr=2.9e-03 time=  1.7s/  2.0s | Train: loss=0.930, TAw acc= 69.5% | Valid: time=  0.4s loss=0.655, TAw acc= 77.4% |
| Epoch  60, lr=2.9e-03 time=  1.7s/  3.2s | Train: loss=0.922, TAw acc= 70.9% | Valid: time=  0.4s loss=0.652, TAw acc= 77.2% | lr=9.7e-04
Debug-2: loss=0.588, acc=0.812
sow: rank=896, freezed_rank=896
 a:        ['-6.34e+00', '+4.96e+00'] .... ['+5.21e+00', '-1.80e+308'] .... ['-1.80e+308', '-1.80e+308']
 a_backup: ['-6.34e+00', '+4.96e+00'] .... ['+5.21e+00', '+5.30e+00'] .... ['+1.29e+00', '-2.74e+00']
Free_Classifier: Done (training=True, fix_classifier=False)
Debug-3: loss=0.582 acc=0.810
sow: rank=1024, freezed_rank=896
 a:        ['-6.34e+00', '+4.96e+00'] .... ['+5.21e+00', '+5.30e+00'] .... ['+1.29e+00', '-2.74e+00']
 a_backup: ['-6.34e+00', '+4.96e+00'] .... ['+5.21e+00', '+5.30e+00'] .... ['+1.29e+00', '-2.74e+00']
Header Training: Finished.
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.649 | TAw acc= 79.2%, forg=  0.0%| TAg acc= 15.2%, forg= 64.0% <<<
>>> Test on task  1 : loss=0.998 | TAw acc= 66.9%, forg=  0.0%| TAg acc= 22.7%, forg= 29.5% <<<
>>> Test on task  2 : loss=0.655 | TAw acc= 77.0%, forg=  0.0%| TAg acc= 22.8%, forg= 29.1% <<<
>>> Test on task  3 : loss=0.620 | TAw acc= 79.1%, forg=  0.0%| TAg acc= 41.0%, forg= 16.2% <<<
>>> Test on task  4 : loss=0.521 | TAw acc= 83.2%, forg=  0.0%| TAg acc= 42.4%, forg=  8.0% <<<
>>> Test on task  5 : loss=0.848 | TAw acc= 66.3%, forg=  0.0%| TAg acc= 26.6%, forg=  6.2% <<<
>>> Test on task  6 : loss=0.602 | TAw acc= 78.7%, forg=  0.0%| TAg acc= 45.8%, forg=  0.0% <<<
Save at ../RESULT_AAAI2025/CR10/0/cifar100_sow
************************************************************************************************************
Task  7
************************************************************************************************************
LLL_Net(
  (model): ResNet50_32SOW(
    in_channels=3, in_H=32, in_W=32, training=False, fix_features=True, fix_classifier=False
    (features): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
      (3): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (4): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (3): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (5): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (3): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (4): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (5): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (6): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (7): AdaptiveAvgPool2d(output_size=(1, 1))
    )
    (classifier): Sequential(
      (0): Dropout(p=0.417598542370663, inplace=False)
      (1): SOW_V3(in_features=1024, out_features=1024, num_tasks=10, dtype=torch.float64, device=cuda:0)
      (2): ReLU()
    )
    (fc): Sequential()
  )
  (heads): ModuleList(
    (0-7): 8 x Linear(in_features=1024, out_features=10, bias=True)
  )
)
SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.0263039750973134
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0

Parameter Group 1
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.05
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0
)
 Group[0]  param_num=161, free_param_num=2
 Group[1]  param_num=3, free_param_num=3
sow: rank=1024, freezed_rank=896
 a:        ['-6.34e+00', '+4.96e+00'] .... ['+5.21e+00', '+5.30e+00'] .... ['+1.29e+00', '-2.74e+00']
 a_backup: ['-6.34e+00', '+4.96e+00'] .... ['+5.21e+00', '+5.30e+00'] .... ['+1.29e+00', '-2.74e+00']
| Epoch   1, lr=2.6e-02 time=  3.7s/  2.5s | Train: loss=1.194, TAw acc= 59.0% | Valid: time=  0.5s loss=0.872, TAw acc= 70.0% | *
| Epoch   2, lr=2.6e-02 time=  3.3s/  2.0s | Train: loss=1.105, TAw acc= 62.0% | Valid: time=  0.5s loss=0.788, TAw acc= 72.2% | *
| Epoch   3, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=1.103, TAw acc= 62.2% | Valid: time=  0.4s loss=0.814, TAw acc= 71.6% |
| Epoch   4, lr=2.6e-02 time=  2.8s/  2.9s | Train: loss=1.046, TAw acc= 64.4% | Valid: time=  0.4s loss=0.768, TAw acc= 72.8% | *
| Epoch   5, lr=2.6e-02 time=  2.8s/  2.4s | Train: loss=1.012, TAw acc= 65.2% | Valid: time=  0.5s loss=0.728, TAw acc= 72.8% | *
| Epoch   6, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.028, TAw acc= 64.7% | Valid: time=  0.5s loss=0.749, TAw acc= 74.2% |
| Epoch   7, lr=2.6e-02 time=  3.6s/  2.1s | Train: loss=1.027, TAw acc= 65.5% | Valid: time=  0.4s loss=0.765, TAw acc= 73.4% |
| Epoch   8, lr=2.6e-02 time=  3.3s/  2.0s | Train: loss=0.997, TAw acc= 66.3% | Valid: time=  0.5s loss=0.777, TAw acc= 72.2% |
| Epoch   9, lr=2.6e-02 time=  3.4s/  2.0s | Train: loss=0.992, TAw acc= 66.6% | Valid: time=  0.4s loss=0.740, TAw acc= 72.4% |
| Epoch  10, lr=2.6e-02 time=  2.8s/  2.5s | Train: loss=1.004, TAw acc= 66.6% | Valid: time=  0.4s loss=0.737, TAw acc= 74.8% |
| Epoch  11, lr=2.6e-02 time=  2.8s/  2.3s | Train: loss=1.012, TAw acc= 65.1% | Valid: time=  0.4s loss=0.774, TAw acc= 73.4% |
| Epoch  12, lr=2.6e-02 time=  2.8s/  2.0s | Train: loss=1.013, TAw acc= 66.9% | Valid: time=  0.4s loss=0.760, TAw acc= 75.2% |
| Epoch  13, lr=2.6e-02 time=  2.8s/  3.2s | Train: loss=0.983, TAw acc= 66.2% | Valid: time=  0.4s loss=0.820, TAw acc= 71.6% |
| Epoch  14, lr=2.6e-02 time=  3.1s/  2.5s | Train: loss=0.979, TAw acc= 66.5% | Valid: time=  0.5s loss=0.766, TAw acc= 74.2% |
| Epoch  15, lr=2.6e-02 time=  3.6s/  2.3s | Train: loss=1.004, TAw acc= 66.0% | Valid: time=  0.4s loss=0.741, TAw acc= 75.2% |
| Epoch  16, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=0.977, TAw acc= 67.4% | Valid: time=  0.4s loss=0.765, TAw acc= 74.6% |
| Epoch  17, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=0.976, TAw acc= 66.8% | Valid: time=  0.4s loss=0.806, TAw acc= 73.8% |
| Epoch  18, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=0.990, TAw acc= 66.2% | Valid: time=  0.4s loss=0.735, TAw acc= 74.4% |
| Epoch  19, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=0.955, TAw acc= 67.2% | Valid: time=  0.4s loss=0.773, TAw acc= 74.0% |
| Epoch  20, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=0.981, TAw acc= 67.4% | Valid: time=  0.4s loss=0.720, TAw acc= 75.8% | *
| Epoch  21, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=0.964, TAw acc= 67.1% | Valid: time=  0.4s loss=0.743, TAw acc= 74.2% |
| Epoch  22, lr=2.6e-02 time=  3.8s/  2.1s | Train: loss=0.952, TAw acc= 67.9% | Valid: time=  0.5s loss=0.771, TAw acc= 74.8% |
| Epoch  23, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.018, TAw acc= 65.0% | Valid: time=  0.5s loss=0.738, TAw acc= 72.2% |
| Epoch  24, lr=2.6e-02 time=  3.6s/  2.4s | Train: loss=0.975, TAw acc= 67.3% | Valid: time=  0.5s loss=0.732, TAw acc= 73.0% |
| Epoch  25, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.964, TAw acc= 67.4% | Valid: time=  0.5s loss=0.734, TAw acc= 75.4% |
| Epoch  26, lr=2.6e-02 time=  3.1s/  1.9s | Train: loss=0.992, TAw acc= 65.1% | Valid: time=  0.6s loss=0.755, TAw acc= 73.8% |
| Epoch  27, lr=2.6e-02 time=  3.0s/  2.0s | Train: loss=0.969, TAw acc= 66.7% | Valid: time=  0.4s loss=0.782, TAw acc= 74.0% |
| Epoch  28, lr=2.6e-02 time=  3.5s/  2.5s | Train: loss=0.960, TAw acc= 67.6% | Valid: time=  0.4s loss=0.724, TAw acc= 73.4% |
| Epoch  29, lr=2.6e-02 time=  3.2s/  2.5s | Train: loss=0.945, TAw acc= 68.0% | Valid: time=  0.5s loss=0.746, TAw acc= 75.2% |
| Epoch  30, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.942, TAw acc= 68.4% | Valid: time=  0.5s loss=0.763, TAw acc= 74.0% |
| Epoch  31, lr=2.6e-02 time=  2.9s/  1.9s | Train: loss=0.977, TAw acc= 67.3% | Valid: time=  0.4s loss=0.716, TAw acc= 75.2% | *
| Epoch  32, lr=2.6e-02 time=  3.6s/  1.9s | Train: loss=0.985, TAw acc= 67.0% | Valid: time=  0.4s loss=0.713, TAw acc= 74.6% | *
| Epoch  33, lr=2.6e-02 time=  3.3s/  1.9s | Train: loss=0.960, TAw acc= 66.9% | Valid: time=  0.5s loss=0.743, TAw acc= 74.4% |
| Epoch  34, lr=2.6e-02 time=  3.1s/  1.9s | Train: loss=0.981, TAw acc= 67.4% | Valid: time=  0.4s loss=0.709, TAw acc= 74.4% | *
| Epoch  35, lr=2.6e-02 time=  3.2s/  2.0s | Train: loss=0.946, TAw acc= 68.6% | Valid: time=  0.7s loss=0.722, TAw acc= 75.4% |
| Epoch  36, lr=2.6e-02 time=  3.0s/  1.9s | Train: loss=0.949, TAw acc= 67.1% | Valid: time=  0.5s loss=0.713, TAw acc= 75.6% |
| Epoch  37, lr=2.6e-02 time=  3.4s/  2.1s | Train: loss=0.974, TAw acc= 67.2% | Valid: time=  0.5s loss=0.752, TAw acc= 73.6% |
| Epoch  38, lr=2.6e-02 time=  2.8s/  2.0s | Train: loss=0.972, TAw acc= 66.5% | Valid: time=  0.4s loss=0.734, TAw acc= 74.8% |
| Epoch  39, lr=2.6e-02 time=  4.0s/  1.9s | Train: loss=0.965, TAw acc= 66.7% | Valid: time=  0.5s loss=0.754, TAw acc= 74.0% |
| Epoch  40, lr=2.6e-02 time=  3.6s/  2.4s | Train: loss=0.928, TAw acc= 67.8% | Valid: time=  0.5s loss=0.739, TAw acc= 74.6% |
| Epoch  41, lr=2.6e-02 time=  3.9s/  2.3s | Train: loss=0.977, TAw acc= 66.4% | Valid: time=  0.4s loss=0.717, TAw acc= 73.0% |
| Epoch  42, lr=2.6e-02 time=  2.9s/  2.0s | Train: loss=0.997, TAw acc= 67.7% | Valid: time=  0.4s loss=0.696, TAw acc= 74.2% | *
| Epoch  43, lr=2.6e-02 time=  3.7s/  2.0s | Train: loss=0.944, TAw acc= 68.0% | Valid: time=  0.4s loss=0.674, TAw acc= 76.6% | *
| Epoch  44, lr=2.6e-02 time=  3.2s/  2.5s | Train: loss=0.973, TAw acc= 66.5% | Valid: time=  0.5s loss=0.710, TAw acc= 74.8% |
| Epoch  45, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.958, TAw acc= 66.6% | Valid: time=  0.5s loss=0.686, TAw acc= 75.8% |
| Epoch  46, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.009, TAw acc= 65.4% | Valid: time=  0.5s loss=0.701, TAw acc= 75.0% |
| Epoch  47, lr=2.6e-02 time=  3.3s/  2.1s | Train: loss=0.925, TAw acc= 68.4% | Valid: time=  0.4s loss=0.729, TAw acc= 74.4% |
| Epoch  48, lr=2.6e-02 time=  2.8s/  2.3s | Train: loss=0.956, TAw acc= 67.0% | Valid: time=  0.4s loss=0.716, TAw acc= 75.4% |
| Epoch  49, lr=2.6e-02 time=  3.3s/  2.0s | Train: loss=0.948, TAw acc= 67.5% | Valid: time=  0.4s loss=0.737, TAw acc= 73.8% |
| Epoch  50, lr=2.6e-02 time=  3.2s/  1.9s | Train: loss=0.957, TAw acc= 68.1% | Valid: time=  0.4s loss=0.707, TAw acc= 74.2% |
| Epoch  51, lr=2.6e-02 time=  2.8s/  2.5s | Train: loss=0.960, TAw acc= 67.5% | Valid: time=  0.8s loss=0.690, TAw acc= 76.4% |
| Epoch  52, lr=2.6e-02 time=  2.8s/  2.3s | Train: loss=0.975, TAw acc= 66.7% | Valid: time=  0.5s loss=0.698, TAw acc= 75.0% |
| Epoch  53, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=0.954, TAw acc= 67.7% | Valid: time=  0.5s loss=0.714, TAw acc= 75.4% |
| Epoch  54, lr=2.6e-02 time=  3.4s/  1.9s | Train: loss=0.948, TAw acc= 68.6% | Valid: time=  0.4s loss=0.728, TAw acc= 75.4% |
| Epoch  55, lr=2.6e-02 time=  3.3s/  1.9s | Train: loss=0.950, TAw acc= 67.5% | Valid: time=  0.4s loss=0.702, TAw acc= 75.6% |
| Epoch  56, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=0.944, TAw acc= 68.8% | Valid: time=  0.4s loss=0.687, TAw acc= 76.4% |
| Epoch  57, lr=2.6e-02 time=  4.1s/  1.9s | Train: loss=0.973, TAw acc= 67.4% | Valid: time=  0.5s loss=0.717, TAw acc= 76.2% |
| Epoch  58, lr=2.6e-02 time=  3.7s/  2.4s | Train: loss=0.928, TAw acc= 68.4% | Valid: time=  0.5s loss=0.706, TAw acc= 76.8% |
| Epoch  59, lr=2.6e-02 time=  3.4s/  2.1s | Train: loss=0.979, TAw acc= 67.1% | Valid: time=  0.4s loss=0.691, TAw acc= 75.2% |
| Epoch  60, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=0.935, TAw acc= 68.9% | Valid: time=  0.4s loss=0.687, TAw acc= 75.8% |
== Rank Reduction [task:7] ==
Debug-0:
  best_loss=0.674,   best_acc=0.766
loss_margin=0.006, acc_margin=0.004
sow: rank=1024, freezed_rank=896
 a:        ['-6.25e+00', '+4.96e+00'] .... ['+5.23e+00', '+5.31e+00'] .... ['+1.29e+00', '-2.74e+00']
 a_backup: ['-6.25e+00', '+4.96e+00'] .... ['+5.23e+00', '+5.31e+00'] .... ['+1.29e+00', '-2.74e+00']
 r=256, loss=2.032, acc=0.240 (rank=256)
 r=288, loss=1.838, acc=0.318 (rank=288)
 r=320, loss=1.576, acc=0.440 (rank=320)
 r=352, loss=1.452, acc=0.462 (rank=352)
 r=384, loss=1.339, acc=0.522 (rank=384)
 r=416, loss=1.146, acc=0.600 (rank=416)
 r=448, loss=0.991, acc=0.638 (rank=448)
 r=480, loss=0.834, acc=0.712 (rank=480)
 r=512, loss=0.699, acc=0.742 (rank=512)
 r=544, loss=0.707, acc=0.748 (rank=544)
 r=576, loss=0.709, acc=0.748 (rank=576)
 r=608, loss=0.690, acc=0.748 (rank=608)
 r=640, loss=0.690, acc=0.760 (rank=640)
 r=672, loss=0.695, acc=0.760 (rank=672)
 r=704, loss=0.691, acc=0.766 (rank=704)
 r=736, loss=0.690, acc=0.750 (rank=736)
 r=768, loss=0.683, acc=0.768 (rank=768)
 r=800, loss=0.682, acc=0.776 (rank=800)
 r=832, loss=0.687, acc=0.772 (rank=832)
 r=864, loss=0.688, acc=0.768 (rank=864)
 r=896, loss=0.678, acc=0.770 (rank=896)
 best_r=896, loss=0.678, acc=0.770
== Header Training for Low Rank [task:7] ==
Fix_Classifier: Done (training=False, fix_classifier=True)
Debug-1: loss=0.678, acc=0.770
sow: rank=896, freezed_rank=896
 a:        ['-6.25e+00', '+4.96e+00'] .... ['+5.23e+00', '-1.80e+308'] .... ['-1.80e+308', '-1.80e+308']
 a_backup: ['-6.25e+00', '+4.96e+00'] .... ['+5.23e+00', '+5.31e+00'] .... ['+1.29e+00', '-2.74e+00']
SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.008767991699104466
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0

Parameter Group 1
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.05
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0
)
 Group[0]  param_num=161, free_param_num=2
 Group[1]  param_num=3, free_param_num=0
sow: rank=896, freezed_rank=896
 a:        ['-6.25e+00', '+4.96e+00'] .... ['+5.23e+00', '-1.80e+308'] .... ['-1.80e+308', '-1.80e+308']
 a_backup: ['-6.25e+00', '+4.96e+00'] .... ['+5.23e+00', '+5.31e+00'] .... ['+1.29e+00', '-2.74e+00']
| Epoch   1, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=0.931, TAw acc= 69.9% | Valid: time=  0.5s loss=0.692, TAw acc= 76.0% |
| Epoch   2, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.936, TAw acc= 68.4% | Valid: time=  0.4s loss=0.698, TAw acc= 75.4% |
| Epoch   3, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.926, TAw acc= 69.2% | Valid: time=  0.9s loss=0.707, TAw acc= 76.6% |
| Epoch   4, lr=8.8e-03 time=  1.9s/  1.9s | Train: loss=0.931, TAw acc= 68.6% | Valid: time=  0.5s loss=0.699, TAw acc= 76.6% |
| Epoch   5, lr=8.8e-03 time=  1.8s/  2.2s | Train: loss=0.928, TAw acc= 69.2% | Valid: time=  0.4s loss=0.703, TAw acc= 77.0% |
| Epoch   6, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.940, TAw acc= 68.7% | Valid: time=  0.4s loss=0.700, TAw acc= 75.4% |
| Epoch   7, lr=8.8e-03 time=  1.7s/  3.1s | Train: loss=0.932, TAw acc= 68.2% | Valid: time=  0.4s loss=0.705, TAw acc= 75.0% |
| Epoch   8, lr=8.8e-03 time=  1.7s/  2.0s | Train: loss=0.924, TAw acc= 69.6% | Valid: time=  0.6s loss=0.717, TAw acc= 75.8% |
| Epoch   9, lr=8.8e-03 time=  1.8s/  2.1s | Train: loss=0.932, TAw acc= 68.4% | Valid: time=  0.5s loss=0.701, TAw acc= 75.6% |
| Epoch  10, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.923, TAw acc= 68.7% | Valid: time=  0.5s loss=0.696, TAw acc= 76.2% |
| Epoch  11, lr=8.8e-03 time=  1.7s/  3.1s | Train: loss=0.931, TAw acc= 68.9% | Valid: time=  0.4s loss=0.704, TAw acc= 76.0% |
| Epoch  12, lr=8.8e-03 time=  1.7s/  2.2s | Train: loss=0.931, TAw acc= 68.2% | Valid: time=  0.5s loss=0.703, TAw acc= 76.2% |
| Epoch  13, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=0.920, TAw acc= 69.7% | Valid: time=  0.5s loss=0.710, TAw acc= 76.4% |
| Epoch  14, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=0.909, TAw acc= 69.4% | Valid: time=  0.5s loss=0.705, TAw acc= 76.0% |
| Epoch  15, lr=8.8e-03 time=  2.1s/  1.9s | Train: loss=0.916, TAw acc= 69.3% | Valid: time=  0.4s loss=0.707, TAw acc= 76.2% |
| Epoch  16, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.905, TAw acc= 69.1% | Valid: time=  0.4s loss=0.707, TAw acc= 76.4% |
| Epoch  17, lr=8.8e-03 time=  2.7s/  2.0s | Train: loss=0.919, TAw acc= 69.1% | Valid: time=  0.4s loss=0.708, TAw acc= 76.8% |
| Epoch  18, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.909, TAw acc= 69.4% | Valid: time=  0.4s loss=0.704, TAw acc= 75.6% |
| Epoch  19, lr=8.8e-03 time=  1.8s/  1.9s | Train: loss=0.901, TAw acc= 69.8% | Valid: time=  0.4s loss=0.707, TAw acc= 76.2% |
| Epoch  20, lr=8.8e-03 time=  3.0s/  1.9s | Train: loss=0.898, TAw acc= 69.3% | Valid: time=  0.4s loss=0.704, TAw acc= 76.6% |
| Epoch  21, lr=8.8e-03 time=  2.0s/  2.5s | Train: loss=0.920, TAw acc= 69.0% | Valid: time=  0.5s loss=0.725, TAw acc= 76.0% |
| Epoch  22, lr=8.8e-03 time=  2.2s/  2.2s | Train: loss=0.908, TAw acc= 69.2% | Valid: time=  0.5s loss=0.724, TAw acc= 75.8% |
| Epoch  23, lr=8.8e-03 time=  1.8s/  1.9s | Train: loss=0.906, TAw acc= 69.8% | Valid: time=  0.8s loss=0.715, TAw acc= 75.6% |
| Epoch  24, lr=8.8e-03 time=  1.8s/  1.9s | Train: loss=0.920, TAw acc= 69.3% | Valid: time=  0.4s loss=0.705, TAw acc= 76.4% |
| Epoch  25, lr=8.8e-03 time=  1.7s/  2.0s | Train: loss=0.908, TAw acc= 69.0% | Valid: time=  0.9s loss=0.705, TAw acc= 75.6% |
| Epoch  26, lr=8.8e-03 time=  1.8s/  2.0s | Train: loss=0.911, TAw acc= 69.1% | Valid: time=  0.5s loss=0.726, TAw acc= 75.4% |
| Epoch  27, lr=8.8e-03 time=  1.8s/  1.9s | Train: loss=0.895, TAw acc= 69.7% | Valid: time=  0.4s loss=0.714, TAw acc= 75.8% |
| Epoch  28, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.893, TAw acc= 69.3% | Valid: time=  0.4s loss=0.712, TAw acc= 76.0% |
| Epoch  29, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.887, TAw acc= 70.7% | Valid: time=  0.4s loss=0.706, TAw acc= 76.4% |
| Epoch  30, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=0.897, TAw acc= 69.8% | Valid: time=  0.4s loss=0.708, TAw acc= 76.6% | lr=2.9e-03
| Epoch  31, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=0.942, TAw acc= 69.0% | Valid: time=  0.4s loss=0.683, TAw acc= 75.6% |
| Epoch  32, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=0.946, TAw acc= 69.1% | Valid: time=  0.4s loss=0.691, TAw acc= 76.0% |
| Epoch  33, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=0.958, TAw acc= 69.0% | Valid: time=  0.4s loss=0.693, TAw acc= 76.6% |
| Epoch  34, lr=2.9e-03 time=  1.8s/  1.9s | Train: loss=0.962, TAw acc= 67.0% | Valid: time=  0.5s loss=0.692, TAw acc= 76.4% |
| Epoch  35, lr=2.9e-03 time=  2.6s/  1.9s | Train: loss=0.949, TAw acc= 67.8% | Valid: time=  0.4s loss=0.694, TAw acc= 76.0% |
| Epoch  36, lr=2.9e-03 time=  2.2s/  2.5s | Train: loss=0.929, TAw acc= 69.0% | Valid: time=  0.5s loss=0.699, TAw acc= 76.4% |
| Epoch  37, lr=2.9e-03 time=  2.2s/  2.5s | Train: loss=0.925, TAw acc= 69.0% | Valid: time=  0.5s loss=0.697, TAw acc= 76.2% |
| Epoch  38, lr=2.9e-03 time=  2.2s/  2.4s | Train: loss=0.946, TAw acc= 68.8% | Valid: time=  0.5s loss=0.694, TAw acc= 75.2% |
| Epoch  39, lr=2.9e-03 time=  2.2s/  2.5s | Train: loss=0.954, TAw acc= 68.2% | Valid: time=  0.5s loss=0.697, TAw acc= 75.0% |
| Epoch  40, lr=2.9e-03 time=  2.2s/  2.1s | Train: loss=0.945, TAw acc= 68.2% | Valid: time=  0.5s loss=0.697, TAw acc= 75.6% |
| Epoch  41, lr=2.9e-03 time=  1.7s/  2.0s | Train: loss=0.959, TAw acc= 68.6% | Valid: time=  0.4s loss=0.700, TAw acc= 75.6% |
| Epoch  42, lr=2.9e-03 time=  1.7s/  3.2s | Train: loss=0.935, TAw acc= 68.7% | Valid: time=  0.4s loss=0.701, TAw acc= 75.8% |
| Epoch  43, lr=2.9e-03 time=  1.7s/  2.4s | Train: loss=0.913, TAw acc= 70.1% | Valid: time=  0.5s loss=0.703, TAw acc= 76.0% |
| Epoch  44, lr=2.9e-03 time=  2.2s/  2.4s | Train: loss=0.920, TAw acc= 69.4% | Valid: time=  0.5s loss=0.700, TAw acc= 75.8% |
| Epoch  45, lr=2.9e-03 time=  2.2s/  2.2s | Train: loss=0.917, TAw acc= 69.4% | Valid: time=  0.5s loss=0.699, TAw acc= 75.2% |
| Epoch  46, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=0.928, TAw acc= 68.4% | Valid: time=  0.4s loss=0.696, TAw acc= 75.8% |
| Epoch  47, lr=2.9e-03 time=  1.7s/  2.0s | Train: loss=0.928, TAw acc= 69.1% | Valid: time=  0.4s loss=0.701, TAw acc= 74.8% |
| Epoch  48, lr=2.9e-03 time=  1.7s/  3.2s | Train: loss=0.929, TAw acc= 68.7% | Valid: time=  0.4s loss=0.698, TAw acc= 75.8% |
| Epoch  49, lr=2.9e-03 time=  1.7s/  2.1s | Train: loss=0.940, TAw acc= 68.1% | Valid: time=  0.5s loss=0.697, TAw acc= 75.8% |
| Epoch  50, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=0.939, TAw acc= 68.6% | Valid: time=  0.5s loss=0.700, TAw acc= 75.6% |
| Epoch  51, lr=2.9e-03 time=  1.7s/  2.8s | Train: loss=0.936, TAw acc= 68.3% | Valid: time=  0.4s loss=0.704, TAw acc= 77.0% |
| Epoch  52, lr=2.9e-03 time=  1.8s/  1.9s | Train: loss=0.923, TAw acc= 68.7% | Valid: time=  0.4s loss=0.703, TAw acc= 76.0% |
| Epoch  53, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=0.934, TAw acc= 68.6% | Valid: time=  0.4s loss=0.702, TAw acc= 75.6% |
| Epoch  54, lr=2.9e-03 time=  2.7s/  1.9s | Train: loss=0.908, TAw acc= 69.2% | Valid: time=  0.5s loss=0.703, TAw acc= 76.0% |
| Epoch  55, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=0.915, TAw acc= 69.1% | Valid: time=  0.4s loss=0.703, TAw acc= 75.6% |
| Epoch  56, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=0.922, TAw acc= 69.1% | Valid: time=  0.4s loss=0.702, TAw acc= 75.8% |
| Epoch  57, lr=2.9e-03 time=  2.7s/  1.9s | Train: loss=0.917, TAw acc= 69.4% | Valid: time=  0.4s loss=0.700, TAw acc= 76.2% |
| Epoch  58, lr=2.9e-03 time=  2.2s/  2.4s | Train: loss=0.917, TAw acc= 68.8% | Valid: time=  0.5s loss=0.704, TAw acc= 75.8% |
| Epoch  59, lr=2.9e-03 time=  2.2s/  2.4s | Train: loss=0.920, TAw acc= 68.2% | Valid: time=  0.5s loss=0.705, TAw acc= 76.2% |
| Epoch  60, lr=2.9e-03 time=  2.2s/  2.4s | Train: loss=0.927, TAw acc= 68.6% | Valid: time=  0.5s loss=0.701, TAw acc= 76.8% | lr=9.7e-04
Debug-2: loss=0.678, acc=0.770
sow: rank=896, freezed_rank=896
 a:        ['-6.25e+00', '+4.96e+00'] .... ['+5.23e+00', '-1.80e+308'] .... ['-1.80e+308', '-1.80e+308']
 a_backup: ['-6.25e+00', '+4.96e+00'] .... ['+5.23e+00', '+5.31e+00'] .... ['+1.29e+00', '-2.74e+00']
Free_Classifier: Done (training=True, fix_classifier=False)
Debug-3: loss=0.674 acc=0.766
sow: rank=1024, freezed_rank=896
 a:        ['-6.25e+00', '+4.96e+00'] .... ['+5.23e+00', '+5.31e+00'] .... ['+1.29e+00', '-2.74e+00']
 a_backup: ['-6.25e+00', '+4.96e+00'] .... ['+5.23e+00', '+5.31e+00'] .... ['+1.29e+00', '-2.74e+00']
Header Training: Finished.
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.649 | TAw acc= 79.2%, forg=  0.0%| TAg acc= 12.5%, forg= 66.7% <<<
>>> Test on task  1 : loss=0.998 | TAw acc= 66.9%, forg=  0.0%| TAg acc= 19.7%, forg= 32.5% <<<
>>> Test on task  2 : loss=0.655 | TAw acc= 77.0%, forg=  0.0%| TAg acc= 21.2%, forg= 30.7% <<<
>>> Test on task  3 : loss=0.620 | TAw acc= 79.1%, forg=  0.0%| TAg acc= 38.6%, forg= 18.6% <<<
>>> Test on task  4 : loss=0.521 | TAw acc= 83.2%, forg=  0.0%| TAg acc= 37.0%, forg= 13.4% <<<
>>> Test on task  5 : loss=0.848 | TAw acc= 66.3%, forg=  0.0%| TAg acc= 23.8%, forg=  9.0% <<<
>>> Test on task  6 : loss=0.602 | TAw acc= 78.7%, forg=  0.0%| TAg acc= 43.0%, forg=  2.8% <<<
>>> Test on task  7 : loss=0.671 | TAw acc= 77.1%, forg=  0.0%| TAg acc= 33.8%, forg=  0.0% <<<
Save at ../RESULT_AAAI2025/CR10/0/cifar100_sow
************************************************************************************************************
Task  8
************************************************************************************************************
LLL_Net(
  (model): ResNet50_32SOW(
    in_channels=3, in_H=32, in_W=32, training=False, fix_features=True, fix_classifier=False
    (features): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
      (3): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (4): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (3): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (5): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (3): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (4): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (5): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (6): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (7): AdaptiveAvgPool2d(output_size=(1, 1))
    )
    (classifier): Sequential(
      (0): Dropout(p=0.417598542370663, inplace=False)
      (1): SOW_V3(in_features=1024, out_features=1024, num_tasks=10, dtype=torch.float64, device=cuda:0)
      (2): ReLU()
    )
    (fc): Sequential()
  )
  (heads): ModuleList(
    (0-8): 9 x Linear(in_features=1024, out_features=10, bias=True)
  )
)
SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.0263039750973134
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0

Parameter Group 1
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.05
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0
)
 Group[0]  param_num=161, free_param_num=2
 Group[1]  param_num=3, free_param_num=3
sow: rank=1024, freezed_rank=896
 a:        ['-6.25e+00', '+4.96e+00'] .... ['+5.23e+00', '+5.31e+00'] .... ['+1.29e+00', '-2.74e+00']
 a_backup: ['-6.25e+00', '+4.96e+00'] .... ['+5.23e+00', '+5.31e+00'] .... ['+1.29e+00', '-2.74e+00']
| Epoch   1, lr=2.6e-02 time=  2.8s/  2.8s | Train: loss=1.314, TAw acc= 55.5% | Valid: time=  0.5s loss=0.927, TAw acc= 68.2% | *
| Epoch   2, lr=2.6e-02 time=  2.8s/  2.4s | Train: loss=1.254, TAw acc= 57.4% | Valid: time=  0.5s loss=0.919, TAw acc= 66.4% | *
| Epoch   3, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.192, TAw acc= 59.1% | Valid: time=  0.5s loss=0.883, TAw acc= 69.4% | *
| Epoch   4, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.172, TAw acc= 61.0% | Valid: time=  0.5s loss=0.813, TAw acc= 71.4% | *
| Epoch   5, lr=2.6e-02 time=  3.3s/  2.1s | Train: loss=1.193, TAw acc= 59.5% | Valid: time=  0.4s loss=0.852, TAw acc= 71.0% |
| Epoch   6, lr=2.6e-02 time=  3.3s/  2.0s | Train: loss=1.141, TAw acc= 61.3% | Valid: time=  0.4s loss=0.801, TAw acc= 71.6% | *
| Epoch   7, lr=2.6e-02 time=  2.9s/  1.9s | Train: loss=1.146, TAw acc= 61.3% | Valid: time=  0.5s loss=0.804, TAw acc= 71.6% |
| Epoch   8, lr=2.6e-02 time=  4.1s/  2.0s | Train: loss=1.131, TAw acc= 60.7% | Valid: time=  0.5s loss=0.806, TAw acc= 71.2% |
| Epoch   9, lr=2.6e-02 time=  3.7s/  2.5s | Train: loss=1.097, TAw acc= 62.9% | Valid: time=  0.5s loss=0.882, TAw acc= 69.6% |
| Epoch  10, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.156, TAw acc= 60.9% | Valid: time=  0.5s loss=0.781, TAw acc= 72.2% | *
| Epoch  11, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.090, TAw acc= 63.4% | Valid: time=  0.5s loss=0.794, TAw acc= 72.0% |
| Epoch  12, lr=2.6e-02 time=  3.6s/  2.4s | Train: loss=1.097, TAw acc= 62.7% | Valid: time=  0.5s loss=0.802, TAw acc= 72.2% |
| Epoch  13, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=1.113, TAw acc= 63.0% | Valid: time=  0.4s loss=0.784, TAw acc= 73.0% |
| Epoch  14, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=1.100, TAw acc= 62.1% | Valid: time=  0.4s loss=0.792, TAw acc= 72.8% |
| Epoch  15, lr=2.6e-02 time=  4.1s/  1.9s | Train: loss=1.078, TAw acc= 63.9% | Valid: time=  0.4s loss=0.816, TAw acc= 72.8% |
| Epoch  16, lr=2.6e-02 time=  3.5s/  2.5s | Train: loss=1.097, TAw acc= 61.8% | Valid: time=  0.5s loss=0.854, TAw acc= 69.0% |
| Epoch  17, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.115, TAw acc= 62.7% | Valid: time=  0.5s loss=0.854, TAw acc= 72.8% |
| Epoch  18, lr=2.6e-02 time=  3.3s/  1.9s | Train: loss=1.094, TAw acc= 63.8% | Valid: time=  0.4s loss=0.799, TAw acc= 72.8% |
| Epoch  19, lr=2.6e-02 time=  3.2s/  1.9s | Train: loss=1.093, TAw acc= 62.4% | Valid: time=  0.4s loss=0.829, TAw acc= 72.2% |
| Epoch  20, lr=2.6e-02 time=  2.8s/  2.8s | Train: loss=1.126, TAw acc= 62.6% | Valid: time=  0.4s loss=0.803, TAw acc= 71.2% |
| Epoch  21, lr=2.6e-02 time=  2.8s/  2.0s | Train: loss=1.090, TAw acc= 63.5% | Valid: time=  0.6s loss=0.748, TAw acc= 71.4% | *
| Epoch  22, lr=2.6e-02 time=  2.9s/  1.9s | Train: loss=1.103, TAw acc= 62.2% | Valid: time=  0.4s loss=0.775, TAw acc= 72.4% |
| Epoch  23, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=1.072, TAw acc= 63.7% | Valid: time=  0.4s loss=0.800, TAw acc= 73.0% |
| Epoch  24, lr=2.6e-02 time=  3.7s/  1.9s | Train: loss=1.099, TAw acc= 62.2% | Valid: time=  0.4s loss=0.814, TAw acc= 71.0% |
| Epoch  25, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=1.068, TAw acc= 63.4% | Valid: time=  0.4s loss=0.796, TAw acc= 72.6% |
| Epoch  26, lr=2.6e-02 time=  4.1s/  1.9s | Train: loss=1.096, TAw acc= 63.1% | Valid: time=  0.4s loss=0.753, TAw acc= 74.6% |
| Epoch  27, lr=2.6e-02 time=  3.5s/  2.4s | Train: loss=1.081, TAw acc= 63.3% | Valid: time=  0.5s loss=0.761, TAw acc= 73.8% |
| Epoch  28, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.068, TAw acc= 64.2% | Valid: time=  0.4s loss=0.808, TAw acc= 73.2% |
| Epoch  29, lr=2.6e-02 time=  2.9s/  1.9s | Train: loss=1.115, TAw acc= 62.2% | Valid: time=  0.5s loss=0.796, TAw acc= 72.8% |
| Epoch  30, lr=2.6e-02 time=  3.6s/  2.0s | Train: loss=1.065, TAw acc= 64.8% | Valid: time=  0.4s loss=0.756, TAw acc= 73.4% |
| Epoch  31, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=1.105, TAw acc= 62.8% | Valid: time=  0.4s loss=0.855, TAw acc= 71.8% |
| Epoch  32, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=1.044, TAw acc= 64.7% | Valid: time=  0.4s loss=0.811, TAw acc= 73.6% |
| Epoch  33, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=1.076, TAw acc= 64.0% | Valid: time=  0.4s loss=0.789, TAw acc= 71.2% |
| Epoch  34, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=1.045, TAw acc= 65.2% | Valid: time=  0.4s loss=0.794, TAw acc= 72.8% |
| Epoch  35, lr=2.6e-02 time=  4.1s/  1.9s | Train: loss=1.059, TAw acc= 64.1% | Valid: time=  0.5s loss=0.781, TAw acc= 74.4% |
| Epoch  36, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.097, TAw acc= 63.1% | Valid: time=  0.5s loss=0.815, TAw acc= 74.2% |
| Epoch  37, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.106, TAw acc= 62.1% | Valid: time=  0.5s loss=0.791, TAw acc= 72.6% |
| Epoch  38, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.060, TAw acc= 64.4% | Valid: time=  0.5s loss=0.772, TAw acc= 73.2% |
| Epoch  39, lr=2.6e-02 time=  3.6s/  2.1s | Train: loss=1.050, TAw acc= 65.0% | Valid: time=  0.4s loss=0.814, TAw acc= 71.2% |
| Epoch  40, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=1.062, TAw acc= 64.0% | Valid: time=  0.4s loss=0.787, TAw acc= 73.0% |
| Epoch  41, lr=2.6e-02 time=  2.8s/  3.2s | Train: loss=1.054, TAw acc= 63.6% | Valid: time=  0.4s loss=0.784, TAw acc= 72.6% |
| Epoch  42, lr=2.6e-02 time=  2.8s/  2.5s | Train: loss=1.053, TAw acc= 63.9% | Valid: time=  0.5s loss=0.794, TAw acc= 72.6% |
| Epoch  43, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.068, TAw acc= 64.7% | Valid: time=  0.6s loss=0.805, TAw acc= 72.4% |
| Epoch  44, lr=2.6e-02 time=  3.6s/  2.3s | Train: loss=1.088, TAw acc= 64.2% | Valid: time=  0.4s loss=0.796, TAw acc= 73.4% |
| Epoch  45, lr=2.6e-02 time=  2.8s/  2.2s | Train: loss=1.060, TAw acc= 63.7% | Valid: time=  0.4s loss=0.757, TAw acc= 75.0% |
| Epoch  46, lr=2.6e-02 time=  2.8s/  2.8s | Train: loss=1.069, TAw acc= 64.7% | Valid: time=  0.4s loss=0.762, TAw acc= 73.8% |
| Epoch  47, lr=2.6e-02 time=  3.0s/  2.0s | Train: loss=1.100, TAw acc= 62.8% | Valid: time=  0.6s loss=0.786, TAw acc= 72.4% |
| Epoch  48, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=1.074, TAw acc= 64.1% | Valid: time=  0.4s loss=0.773, TAw acc= 73.4% |
| Epoch  49, lr=2.6e-02 time=  2.8s/  3.1s | Train: loss=1.075, TAw acc= 63.8% | Valid: time=  0.4s loss=0.786, TAw acc= 71.6% |
| Epoch  50, lr=2.6e-02 time=  2.8s/  2.1s | Train: loss=1.086, TAw acc= 63.3% | Valid: time=  0.4s loss=0.818, TAw acc= 73.6% |
| Epoch  51, lr=2.6e-02 time=  3.3s/  1.9s | Train: loss=1.095, TAw acc= 63.2% | Valid: time=  0.4s loss=0.796, TAw acc= 73.6% | lr=8.8e-03
| Epoch  52, lr=8.8e-03 time=  3.3s/  1.9s | Train: loss=1.060, TAw acc= 65.1% | Valid: time=  0.4s loss=0.802, TAw acc= 71.8% |
| Epoch  53, lr=8.8e-03 time=  2.8s/  1.9s | Train: loss=1.102, TAw acc= 63.2% | Valid: time=  0.4s loss=0.785, TAw acc= 72.6% |
| Epoch  54, lr=8.8e-03 time=  2.8s/  3.2s | Train: loss=1.087, TAw acc= 64.0% | Valid: time=  0.4s loss=0.801, TAw acc= 72.0% |
| Epoch  55, lr=8.8e-03 time=  3.0s/  2.5s | Train: loss=1.081, TAw acc= 63.5% | Valid: time=  0.5s loss=0.782, TAw acc= 73.0% |
| Epoch  56, lr=8.8e-03 time=  3.6s/  2.5s | Train: loss=1.097, TAw acc= 63.4% | Valid: time=  0.5s loss=0.763, TAw acc= 73.8% |
| Epoch  57, lr=8.8e-03 time=  3.7s/  2.4s | Train: loss=1.064, TAw acc= 63.4% | Valid: time=  0.5s loss=0.870, TAw acc= 71.2% |
| Epoch  58, lr=8.8e-03 time=  2.8s/  1.9s | Train: loss=1.052, TAw acc= 63.7% | Valid: time=  0.4s loss=0.807, TAw acc= 73.4% |
| Epoch  59, lr=8.8e-03 time=  2.8s/  1.9s | Train: loss=1.072, TAw acc= 63.4% | Valid: time=  0.4s loss=0.808, TAw acc= 72.0% |
| Epoch  60, lr=8.8e-03 time=  4.0s/  1.9s | Train: loss=1.099, TAw acc= 63.7% | Valid: time=  0.4s loss=0.758, TAw acc= 73.4% |
== Rank Reduction [task:8] ==
Debug-0:
  best_loss=0.748,   best_acc=0.714
loss_margin=0.006, acc_margin=0.004
sow: rank=1024, freezed_rank=896
 a:        ['-6.14e+00', '+4.97e+00'] .... ['+5.23e+00', '+5.32e+00'] .... ['+1.29e+00', '-2.74e+00']
 a_backup: ['-6.14e+00', '+4.97e+00'] .... ['+5.23e+00', '+5.32e+00'] .... ['+1.29e+00', '-2.74e+00']
 r=256, loss=1.857, acc=0.380 (rank=256)
 r=288, loss=1.747, acc=0.432 (rank=288)
 r=320, loss=1.590, acc=0.482 (rank=320)
 r=352, loss=1.372, acc=0.590 (rank=352)
 r=384, loss=1.277, acc=0.582 (rank=384)
 r=416, loss=1.172, acc=0.604 (rank=416)
 r=448, loss=1.023, acc=0.654 (rank=448)
 r=480, loss=0.874, acc=0.710 (rank=480)
 r=512, loss=0.772, acc=0.716 (rank=512)
 r=544, loss=0.779, acc=0.726 (rank=544)
 r=576, loss=0.777, acc=0.724 (rank=576)
 r=608, loss=0.786, acc=0.718 (rank=608)
 r=640, loss=0.773, acc=0.712 (rank=640)
 r=672, loss=0.775, acc=0.714 (rank=672)
 r=704, loss=0.759, acc=0.718 (rank=704)
 r=736, loss=0.749, acc=0.732 (rank=736)
 best_r=736, loss=0.749, acc=0.732
== Header Training for Low Rank [task:8] ==
Fix_Classifier: Done (training=False, fix_classifier=True)
Debug-1: loss=0.749, acc=0.732
sow: rank=736, freezed_rank=896
 a:        ['-6.14e+00', '+4.97e+00'] .... ['-1.80e+308', '-1.80e+308'] .... ['-1.80e+308', '-1.80e+308']
 a_backup: ['-6.14e+00', '+4.97e+00'] .... ['+5.23e+00', '+5.32e+00'] .... ['+1.29e+00', '-2.74e+00']
SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.008767991699104466
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0

Parameter Group 1
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.05
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0
)
 Group[0]  param_num=161, free_param_num=2
 Group[1]  param_num=3, free_param_num=0
sow: rank=736, freezed_rank=896
 a:        ['-6.14e+00', '+4.97e+00'] .... ['-1.80e+308', '-1.80e+308'] .... ['-1.80e+308', '-1.80e+308']
 a_backup: ['-6.14e+00', '+4.97e+00'] .... ['+5.23e+00', '+5.32e+00'] .... ['+1.29e+00', '-2.74e+00']
| Epoch   1, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=1.121, TAw acc= 63.9% | Valid: time=  0.5s loss=0.798, TAw acc= 71.6% |
| Epoch   2, lr=8.8e-03 time=  1.8s/  2.2s | Train: loss=1.101, TAw acc= 64.2% | Valid: time=  0.5s loss=0.797, TAw acc= 72.4% |
| Epoch   3, lr=8.8e-03 time=  1.9s/  1.9s | Train: loss=1.102, TAw acc= 63.9% | Valid: time=  0.4s loss=0.806, TAw acc= 72.0% |
| Epoch   4, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=1.083, TAw acc= 64.2% | Valid: time=  0.4s loss=0.829, TAw acc= 72.8% |
| Epoch   5, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=1.086, TAw acc= 63.6% | Valid: time=  0.8s loss=0.797, TAw acc= 71.8% |
| Epoch   6, lr=8.8e-03 time=  2.0s/  2.0s | Train: loss=1.080, TAw acc= 65.1% | Valid: time=  0.4s loss=0.788, TAw acc= 72.6% |
| Epoch   7, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=1.068, TAw acc= 65.2% | Valid: time=  0.4s loss=0.804, TAw acc= 71.6% |
| Epoch   8, lr=8.8e-03 time=  1.8s/  1.9s | Train: loss=1.060, TAw acc= 65.0% | Valid: time=  0.9s loss=0.802, TAw acc= 72.4% |
| Epoch   9, lr=8.8e-03 time=  2.0s/  1.9s | Train: loss=1.063, TAw acc= 65.3% | Valid: time=  0.5s loss=0.792, TAw acc= 72.6% |
| Epoch  10, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=1.075, TAw acc= 64.2% | Valid: time=  0.5s loss=0.769, TAw acc= 72.4% |
| Epoch  11, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=1.062, TAw acc= 65.3% | Valid: time=  0.5s loss=0.786, TAw acc= 72.8% |
| Epoch  12, lr=8.8e-03 time=  2.0s/  2.1s | Train: loss=1.064, TAw acc= 64.9% | Valid: time=  0.4s loss=0.813, TAw acc= 72.4% |
| Epoch  13, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=1.055, TAw acc= 64.5% | Valid: time=  0.4s loss=0.794, TAw acc= 72.2% |
| Epoch  14, lr=8.8e-03 time=  1.8s/  1.9s | Train: loss=1.026, TAw acc= 66.4% | Valid: time=  0.4s loss=0.790, TAw acc= 73.2% |
| Epoch  15, lr=8.8e-03 time=  2.8s/  1.9s | Train: loss=1.043, TAw acc= 65.7% | Valid: time=  0.4s loss=0.809, TAw acc= 72.0% |
| Epoch  16, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=1.044, TAw acc= 65.8% | Valid: time=  0.5s loss=0.798, TAw acc= 73.2% |
| Epoch  17, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=1.050, TAw acc= 65.6% | Valid: time=  0.5s loss=0.789, TAw acc= 73.4% |
| Epoch  18, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=1.037, TAw acc= 66.1% | Valid: time=  0.4s loss=0.794, TAw acc= 73.4% |
| Epoch  19, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=1.028, TAw acc= 65.4% | Valid: time=  0.4s loss=0.796, TAw acc= 74.4% |
| Epoch  20, lr=8.8e-03 time=  1.7s/  2.0s | Train: loss=1.035, TAw acc= 65.5% | Valid: time=  0.4s loss=0.786, TAw acc= 73.8% |
| Epoch  21, lr=8.8e-03 time=  3.0s/  1.9s | Train: loss=1.017, TAw acc= 66.1% | Valid: time=  0.4s loss=0.792, TAw acc= 74.0% |
| Epoch  22, lr=8.8e-03 time=  1.9s/  2.5s | Train: loss=1.003, TAw acc= 67.3% | Valid: time=  0.5s loss=0.774, TAw acc= 74.2% |
| Epoch  23, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=1.024, TAw acc= 66.8% | Valid: time=  0.5s loss=0.781, TAw acc= 72.8% |
| Epoch  24, lr=8.8e-03 time=  1.7s/  2.0s | Train: loss=1.020, TAw acc= 66.4% | Valid: time=  0.4s loss=0.798, TAw acc= 73.4% |
| Epoch  25, lr=8.8e-03 time=  1.7s/  3.2s | Train: loss=1.009, TAw acc= 67.0% | Valid: time=  0.5s loss=0.792, TAw acc= 73.6% |
| Epoch  26, lr=8.8e-03 time=  1.7s/  2.3s | Train: loss=1.010, TAw acc= 66.8% | Valid: time=  0.5s loss=0.796, TAw acc= 73.4% |
| Epoch  27, lr=8.8e-03 time=  2.2s/  2.4s | Train: loss=1.009, TAw acc= 66.5% | Valid: time=  0.5s loss=0.803, TAw acc= 74.2% |
| Epoch  28, lr=8.8e-03 time=  2.2s/  2.5s | Train: loss=1.007, TAw acc= 66.7% | Valid: time=  0.5s loss=0.798, TAw acc= 73.4% |
| Epoch  29, lr=8.8e-03 time=  2.2s/  2.1s | Train: loss=0.999, TAw acc= 66.8% | Valid: time=  0.4s loss=0.792, TAw acc= 73.4% |
| Epoch  30, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=1.012, TAw acc= 66.5% | Valid: time=  0.4s loss=0.801, TAw acc= 73.8% | lr=2.9e-03
| Epoch  31, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=1.128, TAw acc= 63.4% | Valid: time=  0.4s loss=0.792, TAw acc= 72.4% |
| Epoch  32, lr=2.9e-03 time=  1.7s/  3.1s | Train: loss=1.106, TAw acc= 63.9% | Valid: time=  0.4s loss=0.794, TAw acc= 72.2% |
| Epoch  33, lr=2.9e-03 time=  1.7s/  2.2s | Train: loss=1.122, TAw acc= 63.7% | Valid: time=  0.5s loss=0.792, TAw acc= 72.4% |
| Epoch  34, lr=2.9e-03 time=  2.2s/  2.4s | Train: loss=1.132, TAw acc= 63.4% | Valid: time=  0.5s loss=0.794, TAw acc= 72.0% |
| Epoch  35, lr=2.9e-03 time=  2.2s/  2.5s | Train: loss=1.104, TAw acc= 65.0% | Valid: time=  0.5s loss=0.793, TAw acc= 72.6% |
| Epoch  36, lr=2.9e-03 time=  1.8s/  1.9s | Train: loss=1.105, TAw acc= 64.1% | Valid: time=  0.4s loss=0.796, TAw acc= 72.6% |
| Epoch  37, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=1.090, TAw acc= 64.7% | Valid: time=  0.4s loss=0.796, TAw acc= 72.4% |
| Epoch  38, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=1.097, TAw acc= 64.7% | Valid: time=  0.4s loss=0.804, TAw acc= 73.0% |
| Epoch  39, lr=2.9e-03 time=  2.9s/  1.9s | Train: loss=1.092, TAw acc= 65.0% | Valid: time=  0.4s loss=0.795, TAw acc= 71.8% |
| Epoch  40, lr=2.9e-03 time=  2.2s/  2.5s | Train: loss=1.095, TAw acc= 64.4% | Valid: time=  0.5s loss=0.795, TAw acc= 72.2% |
| Epoch  41, lr=2.9e-03 time=  2.2s/  2.5s | Train: loss=1.083, TAw acc= 65.0% | Valid: time=  0.5s loss=0.794, TAw acc= 72.8% |
| Epoch  42, lr=2.9e-03 time=  1.8s/  1.9s | Train: loss=1.078, TAw acc= 65.1% | Valid: time=  0.4s loss=0.782, TAw acc= 72.2% |
| Epoch  43, lr=2.9e-03 time=  1.7s/  3.1s | Train: loss=1.066, TAw acc= 65.4% | Valid: time=  0.4s loss=0.789, TAw acc= 72.4% |
| Epoch  44, lr=2.9e-03 time=  1.7s/  2.0s | Train: loss=1.089, TAw acc= 64.7% | Valid: time=  0.4s loss=0.789, TAw acc= 72.2% |
| Epoch  45, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=1.099, TAw acc= 63.3% | Valid: time=  0.4s loss=0.789, TAw acc= 72.6% |
| Epoch  46, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=1.085, TAw acc= 64.7% | Valid: time=  0.4s loss=0.791, TAw acc= 72.6% |
| Epoch  47, lr=2.9e-03 time=  1.8s/  3.1s | Train: loss=1.062, TAw acc= 65.4% | Valid: time=  0.4s loss=0.791, TAw acc= 72.0% |
| Epoch  48, lr=2.9e-03 time=  1.7s/  2.1s | Train: loss=1.074, TAw acc= 65.0% | Valid: time=  0.5s loss=0.783, TAw acc= 72.6% |
| Epoch  49, lr=2.9e-03 time=  2.2s/  2.4s | Train: loss=1.069, TAw acc= 65.3% | Valid: time=  0.5s loss=0.785, TAw acc= 73.0% |
| Epoch  50, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=1.068, TAw acc= 65.2% | Valid: time=  0.4s loss=0.781, TAw acc= 73.0% |
| Epoch  51, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=1.066, TAw acc= 65.5% | Valid: time=  0.4s loss=0.788, TAw acc= 73.0% |
| Epoch  52, lr=2.9e-03 time=  2.0s/  2.6s | Train: loss=1.075, TAw acc= 64.2% | Valid: time=  0.4s loss=0.794, TAw acc= 73.2% |
| Epoch  53, lr=2.9e-03 time=  1.7s/  2.5s | Train: loss=1.057, TAw acc= 64.6% | Valid: time=  0.5s loss=0.783, TAw acc= 73.0% |
| Epoch  54, lr=2.9e-03 time=  2.2s/  2.4s | Train: loss=1.057, TAw acc= 65.2% | Valid: time=  0.5s loss=0.790, TAw acc= 72.8% |
| Epoch  55, lr=2.9e-03 time=  2.0s/  2.1s | Train: loss=1.049, TAw acc= 65.8% | Valid: time=  0.4s loss=0.792, TAw acc= 72.6% |
| Epoch  56, lr=2.9e-03 time=  1.9s/  2.0s | Train: loss=1.061, TAw acc= 65.1% | Valid: time=  0.5s loss=0.790, TAw acc= 72.0% |
| Epoch  57, lr=2.9e-03 time=  2.0s/  2.1s | Train: loss=1.046, TAw acc= 66.2% | Valid: time=  0.4s loss=0.786, TAw acc= 73.2% |
| Epoch  58, lr=2.9e-03 time=  1.8s/  1.9s | Train: loss=1.071, TAw acc= 64.6% | Valid: time=  0.5s loss=0.789, TAw acc= 72.4% |
| Epoch  59, lr=2.9e-03 time=  1.7s/  3.1s | Train: loss=1.074, TAw acc= 64.1% | Valid: time=  0.4s loss=0.793, TAw acc= 72.6% |
| Epoch  60, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=1.063, TAw acc= 65.4% | Valid: time=  0.4s loss=0.787, TAw acc= 73.0% | lr=9.7e-04
Debug-2: loss=0.749, acc=0.732
sow: rank=736, freezed_rank=896
 a:        ['-6.14e+00', '+4.97e+00'] .... ['-1.80e+308', '-1.80e+308'] .... ['-1.80e+308', '-1.80e+308']
 a_backup: ['-6.14e+00', '+4.97e+00'] .... ['+5.23e+00', '+5.32e+00'] .... ['+1.29e+00', '-2.74e+00']
Free_Classifier: Done (training=True, fix_classifier=False)
Debug-3: loss=0.748 acc=0.714
sow: rank=1024, freezed_rank=896
 a:        ['-6.14e+00', '+4.97e+00'] .... ['+5.23e+00', '+5.32e+00'] .... ['+1.29e+00', '-2.74e+00']
 a_backup: ['-6.14e+00', '+4.97e+00'] .... ['+5.23e+00', '+5.32e+00'] .... ['+1.29e+00', '-2.74e+00']
Header Training: Finished.
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.649 | TAw acc= 79.2%, forg=  0.0%| TAg acc=  9.3%, forg= 69.9% <<<
>>> Test on task  1 : loss=0.998 | TAw acc= 66.9%, forg=  0.0%| TAg acc= 19.3%, forg= 32.9% <<<
>>> Test on task  2 : loss=0.655 | TAw acc= 77.0%, forg=  0.0%| TAg acc= 20.5%, forg= 31.4% <<<
>>> Test on task  3 : loss=0.620 | TAw acc= 79.1%, forg=  0.0%| TAg acc= 38.1%, forg= 19.1% <<<
>>> Test on task  4 : loss=0.521 | TAw acc= 83.2%, forg=  0.0%| TAg acc= 34.2%, forg= 16.2% <<<
>>> Test on task  5 : loss=0.848 | TAw acc= 66.3%, forg=  0.0%| TAg acc= 22.8%, forg= 10.0% <<<
>>> Test on task  6 : loss=0.602 | TAw acc= 78.7%, forg=  0.0%| TAg acc= 40.9%, forg=  4.9% <<<
>>> Test on task  7 : loss=0.671 | TAw acc= 77.1%, forg=  0.0%| TAg acc= 30.9%, forg=  2.9% <<<
>>> Test on task  8 : loss=0.727 | TAw acc= 74.7%, forg=  0.0%| TAg acc= 23.5%, forg=  0.0% <<<
Save at ../RESULT_AAAI2025/CR10/0/cifar100_sow
************************************************************************************************************
Task  9
************************************************************************************************************
LLL_Net(
  (model): ResNet50_32SOW(
    in_channels=3, in_H=32, in_W=32, training=False, fix_features=True, fix_classifier=False
    (features): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
      (3): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (4): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (3): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (5): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (3): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (4): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (5): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (6): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (7): AdaptiveAvgPool2d(output_size=(1, 1))
    )
    (classifier): Sequential(
      (0): Dropout(p=0.417598542370663, inplace=False)
      (1): SOW_V3(in_features=1024, out_features=1024, num_tasks=10, dtype=torch.float64, device=cuda:0)
      (2): ReLU()
    )
    (fc): Sequential()
  )
  (heads): ModuleList(
    (0-9): 10 x Linear(in_features=1024, out_features=10, bias=True)
  )
)
SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.0263039750973134
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0

Parameter Group 1
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.05
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0
)
 Group[0]  param_num=161, free_param_num=2
 Group[1]  param_num=3, free_param_num=3
sow: rank=1024, freezed_rank=896
 a:        ['-6.14e+00', '+4.97e+00'] .... ['+5.23e+00', '+5.32e+00'] .... ['+1.29e+00', '-2.74e+00']
 a_backup: ['-6.14e+00', '+4.97e+00'] .... ['+5.23e+00', '+5.32e+00'] .... ['+1.29e+00', '-2.74e+00']
| Epoch   1, lr=2.6e-02 time=  4.1s/  1.9s | Train: loss=1.313, TAw acc= 54.0% | Valid: time=  0.5s loss=1.175, TAw acc= 60.8% | *
| Epoch   2, lr=2.6e-02 time=  3.7s/  2.5s | Train: loss=1.209, TAw acc= 59.4% | Valid: time=  0.5s loss=0.993, TAw acc= 65.2% | *
| Epoch   3, lr=2.6e-02 time=  3.5s/  1.9s | Train: loss=1.158, TAw acc= 59.8% | Valid: time=  0.4s loss=0.897, TAw acc= 68.0% | *
| Epoch   4, lr=2.6e-02 time=  3.6s/  1.9s | Train: loss=1.153, TAw acc= 60.0% | Valid: time=  0.5s loss=0.965, TAw acc= 66.2% |
| Epoch   5, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=1.147, TAw acc= 61.0% | Valid: time=  0.4s loss=0.901, TAw acc= 67.8% |
| Epoch   6, lr=2.6e-02 time=  4.1s/  1.9s | Train: loss=1.136, TAw acc= 61.3% | Valid: time=  0.5s loss=0.922, TAw acc= 69.2% |
| Epoch   7, lr=2.6e-02 time=  3.7s/  2.4s | Train: loss=1.119, TAw acc= 61.6% | Valid: time=  0.5s loss=0.945, TAw acc= 66.2% |
| Epoch   8, lr=2.6e-02 time=  3.6s/  2.3s | Train: loss=1.097, TAw acc= 62.7% | Valid: time=  0.4s loss=0.866, TAw acc= 67.8% | *
| Epoch   9, lr=2.6e-02 time=  2.8s/  2.6s | Train: loss=1.115, TAw acc= 61.8% | Valid: time=  0.4s loss=0.874, TAw acc= 71.2% |
| Epoch  10, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=1.115, TAw acc= 61.3% | Valid: time=  0.4s loss=0.848, TAw acc= 71.4% | *
| Epoch  11, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=1.105, TAw acc= 62.6% | Valid: time=  0.5s loss=0.887, TAw acc= 68.6% |
| Epoch  12, lr=2.6e-02 time=  4.1s/  2.0s | Train: loss=1.100, TAw acc= 63.1% | Valid: time=  0.5s loss=0.868, TAw acc= 68.6% |
| Epoch  13, lr=2.6e-02 time=  3.0s/  1.9s | Train: loss=1.104, TAw acc= 62.1% | Valid: time=  0.4s loss=0.855, TAw acc= 71.0% |
| Epoch  14, lr=2.6e-02 time=  2.8s/  3.1s | Train: loss=1.107, TAw acc= 63.4% | Valid: time=  0.4s loss=0.877, TAw acc= 69.4% |
| Epoch  15, lr=2.6e-02 time=  2.8s/  2.1s | Train: loss=1.130, TAw acc= 61.4% | Valid: time=  0.5s loss=0.883, TAw acc= 69.8% |
| Epoch  16, lr=2.6e-02 time=  3.1s/  1.9s | Train: loss=1.098, TAw acc= 62.8% | Valid: time=  0.4s loss=0.884, TAw acc= 69.0% |
| Epoch  17, lr=2.6e-02 time=  2.8s/  2.4s | Train: loss=1.063, TAw acc= 62.9% | Valid: time=  0.4s loss=0.885, TAw acc= 71.0% |
| Epoch  18, lr=2.6e-02 time=  3.1s/  1.9s | Train: loss=1.075, TAw acc= 63.0% | Valid: time=  0.5s loss=0.908, TAw acc= 67.6% |
| Epoch  19, lr=2.6e-02 time=  2.8s/  3.0s | Train: loss=1.057, TAw acc= 64.9% | Valid: time=  0.5s loss=0.848, TAw acc= 69.6% | *
| Epoch  20, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=1.074, TAw acc= 63.6% | Valid: time=  0.5s loss=0.865, TAw acc= 69.2% |
| Epoch  21, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=1.083, TAw acc= 63.8% | Valid: time=  0.4s loss=0.852, TAw acc= 69.6% |
| Epoch  22, lr=2.6e-02 time=  4.1s/  1.9s | Train: loss=1.064, TAw acc= 62.6% | Valid: time=  0.5s loss=0.884, TAw acc= 70.0% |
| Epoch  23, lr=2.6e-02 time=  3.7s/  2.5s | Train: loss=1.078, TAw acc= 62.8% | Valid: time=  0.5s loss=0.852, TAw acc= 69.6% |
| Epoch  24, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.067, TAw acc= 64.2% | Valid: time=  0.5s loss=0.850, TAw acc= 70.6% |
| Epoch  25, lr=2.6e-02 time=  2.8s/  2.0s | Train: loss=1.067, TAw acc= 62.7% | Valid: time=  0.5s loss=0.905, TAw acc= 68.6% |
| Epoch  26, lr=2.6e-02 time=  2.8s/  3.1s | Train: loss=1.067, TAw acc= 63.2% | Valid: time=  0.5s loss=0.879, TAw acc= 69.0% |
| Epoch  27, lr=2.6e-02 time=  2.9s/  2.4s | Train: loss=1.087, TAw acc= 63.2% | Valid: time=  0.5s loss=0.806, TAw acc= 70.6% | *
| Epoch  28, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=1.071, TAw acc= 62.8% | Valid: time=  0.4s loss=0.916, TAw acc= 68.6% |
| Epoch  29, lr=2.6e-02 time=  2.8s/  3.0s | Train: loss=1.061, TAw acc= 64.2% | Valid: time=  0.4s loss=0.823, TAw acc= 70.4% |
| Epoch  30, lr=2.6e-02 time=  2.8s/  2.0s | Train: loss=1.068, TAw acc= 63.5% | Valid: time=  0.5s loss=0.858, TAw acc= 70.6% |
| Epoch  31, lr=2.6e-02 time=  2.7s/  1.9s | Train: loss=1.065, TAw acc= 63.6% | Valid: time=  0.4s loss=0.922, TAw acc= 69.8% |
| Epoch  32, lr=2.6e-02 time=  2.8s/  3.2s | Train: loss=1.063, TAw acc= 64.7% | Valid: time=  0.4s loss=0.880, TAw acc= 70.4% |
| Epoch  33, lr=2.6e-02 time=  2.8s/  2.5s | Train: loss=1.083, TAw acc= 63.4% | Valid: time=  0.5s loss=0.830, TAw acc= 71.2% |
| Epoch  34, lr=2.6e-02 time=  3.6s/  2.4s | Train: loss=1.049, TAw acc= 63.8% | Valid: time=  0.5s loss=0.844, TAw acc= 71.0% |
| Epoch  35, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.047, TAw acc= 64.1% | Valid: time=  0.5s loss=0.850, TAw acc= 70.2% |
| Epoch  36, lr=2.6e-02 time=  3.3s/  1.9s | Train: loss=1.080, TAw acc= 62.5% | Valid: time=  0.4s loss=0.915, TAw acc= 69.0% |
| Epoch  37, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=1.044, TAw acc= 63.7% | Valid: time=  0.4s loss=0.877, TAw acc= 69.2% |
| Epoch  38, lr=2.6e-02 time=  2.8s/  3.2s | Train: loss=1.028, TAw acc= 65.3% | Valid: time=  0.4s loss=0.878, TAw acc= 69.8% |
| Epoch  39, lr=2.6e-02 time=  3.1s/  2.5s | Train: loss=1.058, TAw acc= 63.9% | Valid: time=  0.5s loss=0.866, TAw acc= 69.2% |
| Epoch  40, lr=2.6e-02 time=  3.7s/  2.5s | Train: loss=1.032, TAw acc= 64.8% | Valid: time=  0.5s loss=0.845, TAw acc= 71.8% |
| Epoch  41, lr=2.6e-02 time=  3.0s/  1.9s | Train: loss=1.046, TAw acc= 64.4% | Valid: time=  0.4s loss=0.835, TAw acc= 71.0% |
| Epoch  42, lr=2.6e-02 time=  2.8s/  2.0s | Train: loss=1.066, TAw acc= 63.7% | Valid: time=  0.5s loss=0.816, TAw acc= 71.6% |
| Epoch  43, lr=2.6e-02 time=  4.1s/  1.9s | Train: loss=1.052, TAw acc= 64.4% | Valid: time=  0.5s loss=0.858, TAw acc= 70.4% |
| Epoch  44, lr=2.6e-02 time=  3.7s/  2.5s | Train: loss=1.056, TAw acc= 64.1% | Valid: time=  0.5s loss=0.830, TAw acc= 70.6% |
| Epoch  45, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.026, TAw acc= 65.6% | Valid: time=  0.5s loss=0.822, TAw acc= 72.8% |
| Epoch  46, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=1.057, TAw acc= 64.9% | Valid: time=  0.5s loss=0.832, TAw acc= 70.8% |
| Epoch  47, lr=2.6e-02 time=  2.8s/  3.2s | Train: loss=1.058, TAw acc= 63.4% | Valid: time=  0.4s loss=0.868, TAw acc= 70.4% |
| Epoch  48, lr=2.6e-02 time=  3.0s/  2.5s | Train: loss=1.052, TAw acc= 64.4% | Valid: time=  0.5s loss=0.827, TAw acc= 71.2% |
| Epoch  49, lr=2.6e-02 time=  3.7s/  2.5s | Train: loss=1.046, TAw acc= 64.4% | Valid: time=  0.5s loss=0.799, TAw acc= 72.0% | *
| Epoch  50, lr=2.6e-02 time=  3.1s/  1.9s | Train: loss=1.032, TAw acc= 64.3% | Valid: time=  0.4s loss=0.857, TAw acc= 71.4% |
| Epoch  51, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=1.053, TAw acc= 63.9% | Valid: time=  0.5s loss=0.829, TAw acc= 69.6% |
| Epoch  52, lr=2.6e-02 time=  2.8s/  3.2s | Train: loss=1.038, TAw acc= 64.2% | Valid: time=  0.4s loss=0.848, TAw acc= 69.4% |
| Epoch  53, lr=2.6e-02 time=  2.8s/  2.5s | Train: loss=1.047, TAw acc= 64.8% | Valid: time=  0.5s loss=0.817, TAw acc= 71.6% |
| Epoch  54, lr=2.6e-02 time=  3.6s/  2.5s | Train: loss=1.034, TAw acc= 65.0% | Valid: time=  0.5s loss=0.832, TAw acc= 72.4% |
| Epoch  55, lr=2.6e-02 time=  2.9s/  1.9s | Train: loss=1.051, TAw acc= 63.8% | Valid: time=  0.5s loss=0.885, TAw acc= 70.4% |
| Epoch  56, lr=2.6e-02 time=  2.8s/  3.1s | Train: loss=1.025, TAw acc= 65.2% | Valid: time=  0.5s loss=0.851, TAw acc= 71.0% |
| Epoch  57, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=1.042, TAw acc= 64.6% | Valid: time=  0.4s loss=0.797, TAw acc= 71.8% | *
| Epoch  58, lr=2.6e-02 time=  2.8s/  1.9s | Train: loss=1.040, TAw acc= 64.5% | Valid: time=  0.5s loss=0.825, TAw acc= 70.6% |
| Epoch  59, lr=2.6e-02 time=  2.8s/  3.2s | Train: loss=1.061, TAw acc= 64.2% | Valid: time=  0.5s loss=0.814, TAw acc= 71.0% |
| Epoch  60, lr=2.6e-02 time=  2.8s/  2.5s | Train: loss=1.048, TAw acc= 63.8% | Valid: time=  0.5s loss=0.830, TAw acc= 70.0% |
== Rank Reduction [task:9] ==
Debug-0:
  best_loss=0.797,   best_acc=0.718
loss_margin=0.006, acc_margin=0.004
sow: rank=1024, freezed_rank=896
 a:        ['-6.42e+00', '+4.97e+00'] .... ['+5.26e+00', '+5.34e+00'] .... ['+1.29e+00', '-2.74e+00']
 a_backup: ['-6.42e+00', '+4.97e+00'] .... ['+5.26e+00', '+5.34e+00'] .... ['+1.29e+00', '-2.74e+00']
 r=256, loss=2.002, acc=0.300 (rank=256)
 r=288, loss=1.797, acc=0.376 (rank=288)
 r=320, loss=1.613, acc=0.438 (rank=320)
 r=352, loss=1.453, acc=0.534 (rank=352)
 r=384, loss=1.301, acc=0.552 (rank=384)
 r=416, loss=1.162, acc=0.584 (rank=416)
 r=448, loss=1.054, acc=0.644 (rank=448)
 r=480, loss=0.879, acc=0.708 (rank=480)
 r=512, loss=0.791, acc=0.738 (rank=512)
 best_r=512, loss=0.791, acc=0.738
== Header Training for Low Rank [task:9] ==
Fix_Classifier: Done (training=False, fix_classifier=True)
Debug-1: loss=0.791, acc=0.738
sow: rank=512, freezed_rank=896
 a:        ['-6.42e+00', '+4.97e+00'] .... ['-1.80e+308', '-1.80e+308'] .... ['-1.80e+308', '-1.80e+308']
 a_backup: ['-6.42e+00', '+4.97e+00'] .... ['+5.26e+00', '+5.34e+00'] .... ['+1.29e+00', '-2.74e+00']
SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.008767991699104466
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0

Parameter Group 1
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.05
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0
)
 Group[0]  param_num=161, free_param_num=2
 Group[1]  param_num=3, free_param_num=0
sow: rank=512, freezed_rank=896
 a:        ['-6.42e+00', '+4.97e+00'] .... ['-1.80e+308', '-1.80e+308'] .... ['-1.80e+308', '-1.80e+308']
 a_backup: ['-6.42e+00', '+4.97e+00'] .... ['+5.26e+00', '+5.34e+00'] .... ['+1.29e+00', '-2.74e+00']
| Epoch   1, lr=8.8e-03 time=  2.1s/  2.4s | Train: loss=1.112, TAw acc= 63.8% | Valid: time=  0.5s loss=0.844, TAw acc= 70.6% |
| Epoch   2, lr=8.8e-03 time=  1.8s/  1.9s | Train: loss=1.091, TAw acc= 64.0% | Valid: time=  0.4s loss=0.852, TAw acc= 70.0% |
| Epoch   3, lr=8.8e-03 time=  2.1s/  1.9s | Train: loss=1.094, TAw acc= 65.2% | Valid: time=  0.6s loss=0.839, TAw acc= 71.2% |
| Epoch   4, lr=8.8e-03 time=  1.7s/  2.0s | Train: loss=1.096, TAw acc= 63.2% | Valid: time=  0.5s loss=0.837, TAw acc= 70.6% |
| Epoch   5, lr=8.8e-03 time=  1.8s/  1.9s | Train: loss=1.085, TAw acc= 64.0% | Valid: time=  0.4s loss=0.854, TAw acc= 70.2% |
| Epoch   6, lr=8.8e-03 time=  2.2s/  1.9s | Train: loss=1.059, TAw acc= 65.2% | Valid: time=  0.4s loss=0.837, TAw acc= 71.0% |
| Epoch   7, lr=8.8e-03 time=  1.7s/  1.8s | Train: loss=1.059, TAw acc= 64.9% | Valid: time=  0.4s loss=0.829, TAw acc= 70.4% |
| Epoch   8, lr=8.8e-03 time=  1.7s/  1.8s | Train: loss=1.076, TAw acc= 64.3% | Valid: time=  0.4s loss=0.817, TAw acc= 72.2% |
| Epoch   9, lr=8.8e-03 time=  2.9s/  1.9s | Train: loss=1.053, TAw acc= 65.9% | Valid: time=  0.4s loss=0.818, TAw acc= 71.6% |
| Epoch  10, lr=8.8e-03 time=  1.8s/  2.0s | Train: loss=1.063, TAw acc= 64.8% | Valid: time=  0.4s loss=0.823, TAw acc= 72.0% |
| Epoch  11, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=1.057, TAw acc= 64.3% | Valid: time=  0.4s loss=0.819, TAw acc= 72.0% |
| Epoch  12, lr=8.8e-03 time=  1.7s/  2.1s | Train: loss=1.059, TAw acc= 63.8% | Valid: time=  0.4s loss=0.821, TAw acc= 72.0% |
| Epoch  13, lr=8.8e-03 time=  1.8s/  1.9s | Train: loss=1.052, TAw acc= 64.9% | Valid: time=  0.4s loss=0.817, TAw acc= 72.6% |
| Epoch  14, lr=8.8e-03 time=  1.7s/  2.5s | Train: loss=1.048, TAw acc= 64.4% | Valid: time=  0.4s loss=0.829, TAw acc= 72.4% |
| Epoch  15, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=1.055, TAw acc= 64.4% | Valid: time=  0.4s loss=0.824, TAw acc= 72.8% |
| Epoch  16, lr=8.8e-03 time=  2.4s/  1.9s | Train: loss=1.052, TAw acc= 64.8% | Valid: time=  0.5s loss=0.817, TAw acc= 72.8% |
| Epoch  17, lr=8.8e-03 time=  1.9s/  1.9s | Train: loss=1.056, TAw acc= 64.6% | Valid: time=  0.4s loss=0.816, TAw acc= 72.6% |
| Epoch  18, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=1.045, TAw acc= 65.0% | Valid: time=  0.4s loss=0.814, TAw acc= 72.8% |
| Epoch  19, lr=8.8e-03 time=  1.7s/  1.9s | Train: loss=1.030, TAw acc= 64.4% | Valid: time=  0.4s loss=0.811, TAw acc= 72.8% |
| Epoch  20, lr=8.8e-03 time=  1.7s/  3.0s | Train: loss=1.035, TAw acc= 65.0% | Valid: time=  0.4s loss=0.821, TAw acc= 72.6% |
| Epoch  21, lr=8.8e-03 time=  1.6s/  2.0s | Train: loss=1.031, TAw acc= 64.8% | Valid: time=  0.5s loss=0.816, TAw acc= 72.8% |
| Epoch  22, lr=8.8e-03 time=  2.1s/  2.4s | Train: loss=1.044, TAw acc= 64.5% | Valid: time=  0.5s loss=0.822, TAw acc= 73.0% |
| Epoch  23, lr=8.8e-03 time=  2.1s/  2.1s | Train: loss=1.050, TAw acc= 65.0% | Valid: time=  0.4s loss=0.819, TAw acc= 72.4% |
| Epoch  24, lr=8.8e-03 time=  1.7s/  2.5s | Train: loss=1.034, TAw acc= 64.9% | Valid: time=  0.4s loss=0.821, TAw acc= 72.6% |
| Epoch  25, lr=8.8e-03 time=  1.7s/  1.8s | Train: loss=1.028, TAw acc= 66.0% | Valid: time=  0.4s loss=0.816, TAw acc= 72.6% |
| Epoch  26, lr=8.8e-03 time=  2.1s/  1.9s | Train: loss=1.031, TAw acc= 65.0% | Valid: time=  0.4s loss=0.813, TAw acc= 74.0% |
| Epoch  27, lr=8.8e-03 time=  1.6s/  1.9s | Train: loss=1.003, TAw acc= 66.3% | Valid: time=  0.4s loss=0.808, TAw acc= 73.4% |
| Epoch  28, lr=8.8e-03 time=  1.6s/  1.9s | Train: loss=1.027, TAw acc= 65.7% | Valid: time=  0.4s loss=0.820, TAw acc= 73.2% |
| Epoch  29, lr=8.8e-03 time=  1.6s/  3.1s | Train: loss=1.033, TAw acc= 64.8% | Valid: time=  0.4s loss=0.812, TAw acc= 72.4% |
| Epoch  30, lr=8.8e-03 time=  1.6s/  2.2s | Train: loss=1.027, TAw acc= 65.5% | Valid: time=  0.5s loss=0.808, TAw acc= 72.6% | lr=2.9e-03
| Epoch  31, lr=2.9e-03 time=  2.1s/  2.4s | Train: loss=1.112, TAw acc= 63.7% | Valid: time=  0.5s loss=0.814, TAw acc= 72.4% |
| Epoch  32, lr=2.9e-03 time=  2.1s/  2.4s | Train: loss=1.101, TAw acc= 63.6% | Valid: time=  0.5s loss=0.832, TAw acc= 71.0% |
| Epoch  33, lr=2.9e-03 time=  2.1s/  2.0s | Train: loss=1.101, TAw acc= 64.8% | Valid: time=  0.4s loss=0.837, TAw acc= 71.0% |
| Epoch  34, lr=2.9e-03 time=  1.6s/  1.9s | Train: loss=1.092, TAw acc= 64.8% | Valid: time=  0.4s loss=0.842, TAw acc= 70.8% |
| Epoch  35, lr=2.9e-03 time=  1.7s/  1.8s | Train: loss=1.102, TAw acc= 64.3% | Valid: time=  0.4s loss=0.841, TAw acc= 70.4% |
| Epoch  36, lr=2.9e-03 time=  1.8s/  2.6s | Train: loss=1.104, TAw acc= 64.1% | Valid: time=  0.4s loss=0.843, TAw acc= 70.0% |
| Epoch  37, lr=2.9e-03 time=  1.6s/  2.4s | Train: loss=1.092, TAw acc= 64.6% | Valid: time=  0.5s loss=0.842, TAw acc= 70.4% |
| Epoch  38, lr=2.9e-03 time=  2.1s/  2.4s | Train: loss=1.107, TAw acc= 64.0% | Valid: time=  0.5s loss=0.839, TAw acc= 70.2% |
| Epoch  39, lr=2.9e-03 time=  2.1s/  2.4s | Train: loss=1.071, TAw acc= 64.7% | Valid: time=  0.5s loss=0.838, TAw acc= 70.0% |
| Epoch  40, lr=2.9e-03 time=  2.1s/  2.4s | Train: loss=1.087, TAw acc= 64.1% | Valid: time=  0.5s loss=0.838, TAw acc= 70.2% |
| Epoch  41, lr=2.9e-03 time=  1.8s/  2.1s | Train: loss=1.103, TAw acc= 63.6% | Valid: time=  0.4s loss=0.836, TAw acc= 70.2% |
| Epoch  42, lr=2.9e-03 time=  1.7s/  2.2s | Train: loss=1.093, TAw acc= 63.8% | Valid: time=  0.4s loss=0.836, TAw acc= 71.0% |
| Epoch  43, lr=2.9e-03 time=  1.7s/  2.2s | Train: loss=1.080, TAw acc= 63.5% | Valid: time=  0.5s loss=0.833, TAw acc= 71.0% |
| Epoch  44, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=1.067, TAw acc= 64.4% | Valid: time=  0.4s loss=0.835, TAw acc= 70.8% |
| Epoch  45, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=1.072, TAw acc= 64.7% | Valid: time=  0.4s loss=0.835, TAw acc= 71.0% |
| Epoch  46, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=1.065, TAw acc= 65.2% | Valid: time=  0.4s loss=0.833, TAw acc= 71.2% |
| Epoch  47, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=1.067, TAw acc= 64.4% | Valid: time=  0.4s loss=0.832, TAw acc= 71.0% |
| Epoch  48, lr=2.9e-03 time=  2.9s/  1.9s | Train: loss=1.064, TAw acc= 64.4% | Valid: time=  0.4s loss=0.830, TAw acc= 71.2% |
| Epoch  49, lr=2.9e-03 time=  2.1s/  2.4s | Train: loss=1.076, TAw acc= 63.9% | Valid: time=  0.5s loss=0.831, TAw acc= 71.6% |
| Epoch  50, lr=2.9e-03 time=  2.1s/  2.4s | Train: loss=1.076, TAw acc= 64.8% | Valid: time=  0.5s loss=0.831, TAw acc= 71.6% |
| Epoch  51, lr=2.9e-03 time=  1.9s/  1.9s | Train: loss=1.069, TAw acc= 64.0% | Valid: time=  0.4s loss=0.828, TAw acc= 72.2% |
| Epoch  52, lr=2.9e-03 time=  1.7s/  2.9s | Train: loss=1.064, TAw acc= 65.1% | Valid: time=  0.4s loss=0.834, TAw acc= 71.2% |
| Epoch  53, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=1.061, TAw acc= 64.4% | Valid: time=  0.4s loss=0.831, TAw acc= 71.2% |
| Epoch  54, lr=2.9e-03 time=  2.0s/  1.9s | Train: loss=1.077, TAw acc= 64.2% | Valid: time=  0.4s loss=0.835, TAw acc= 71.2% |
| Epoch  55, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=1.075, TAw acc= 63.9% | Valid: time=  0.4s loss=0.832, TAw acc= 71.2% |
| Epoch  56, lr=2.9e-03 time=  1.7s/  1.9s | Train: loss=1.053, TAw acc= 64.0% | Valid: time=  0.4s loss=0.833, TAw acc= 71.4% |
| Epoch  57, lr=2.9e-03 time=  2.8s/  1.9s | Train: loss=1.059, TAw acc= 64.3% | Valid: time=  0.4s loss=0.830, TAw acc= 71.8% |
| Epoch  58, lr=2.9e-03 time=  2.1s/  2.3s | Train: loss=1.050, TAw acc= 64.5% | Valid: time=  0.5s loss=0.829, TAw acc= 72.4% |
| Epoch  59, lr=2.9e-03 time=  2.1s/  2.4s | Train: loss=1.060, TAw acc= 64.6% | Valid: time=  0.5s loss=0.832, TAw acc= 72.0% |
| Epoch  60, lr=2.9e-03 time=  2.1s/  2.4s | Train: loss=1.066, TAw acc= 63.7% | Valid: time=  0.5s loss=0.834, TAw acc= 72.2% | lr=9.7e-04
Debug-2: loss=0.791, acc=0.738
sow: rank=512, freezed_rank=896
 a:        ['-6.42e+00', '+4.97e+00'] .... ['-1.80e+308', '-1.80e+308'] .... ['-1.80e+308', '-1.80e+308']
 a_backup: ['-6.42e+00', '+4.97e+00'] .... ['+5.26e+00', '+5.34e+00'] .... ['+1.29e+00', '-2.74e+00']
Free_Classifier: Done (training=True, fix_classifier=False)
Debug-3: loss=0.797 acc=0.718
sow: rank=1024, freezed_rank=896
 a:        ['-6.42e+00', '+4.97e+00'] .... ['+5.26e+00', '+5.34e+00'] .... ['+1.29e+00', '-2.74e+00']
 a_backup: ['-6.42e+00', '+4.97e+00'] .... ['+5.26e+00', '+5.34e+00'] .... ['+1.29e+00', '-2.74e+00']
Header Training: Finished.
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.649 | TAw acc= 79.2%, forg=  0.0%| TAg acc=  7.2%, forg= 72.0% <<<
>>> Test on task  1 : loss=0.998 | TAw acc= 66.9%, forg=  0.0%| TAg acc= 18.4%, forg= 33.8% <<<
>>> Test on task  2 : loss=0.655 | TAw acc= 77.0%, forg=  0.0%| TAg acc= 18.7%, forg= 33.2% <<<
>>> Test on task  3 : loss=0.620 | TAw acc= 79.1%, forg=  0.0%| TAg acc= 37.2%, forg= 20.0% <<<
>>> Test on task  4 : loss=0.521 | TAw acc= 83.2%, forg=  0.0%| TAg acc= 30.5%, forg= 19.9% <<<
>>> Test on task  5 : loss=0.848 | TAw acc= 66.3%, forg=  0.0%| TAg acc= 20.6%, forg= 12.2% <<<
>>> Test on task  6 : loss=0.602 | TAw acc= 78.7%, forg=  0.0%| TAg acc= 38.4%, forg=  7.4% <<<
>>> Test on task  7 : loss=0.671 | TAw acc= 77.1%, forg=  0.0%| TAg acc= 29.0%, forg=  4.8% <<<
>>> Test on task  8 : loss=0.727 | TAw acc= 74.7%, forg=  0.0%| TAg acc= 22.1%, forg=  1.4% <<<
>>> Test on task  9 : loss=0.848 | TAw acc= 71.0%, forg=  0.0%| TAg acc= 33.0%, forg=  0.0% <<<
Save at ../RESULT_AAAI2025/CR10/0/cifar100_sow
************************************************************************************************************
TAw Acc
	 79.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 79.2% 
	 79.2%  66.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 73.0% 
	 79.2%  66.9%  77.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 74.4% 
	 79.2%  66.9%  77.0%  79.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 75.5% 
	 79.2%  66.9%  77.0%  79.1%  83.2%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 77.1% 
	 79.2%  66.9%  77.0%  79.1%  83.2%  66.3%   0.0%   0.0%   0.0%   0.0% 	Avg.: 75.3% 
	 79.2%  66.9%  77.0%  79.1%  83.2%  66.3%  78.7%   0.0%   0.0%   0.0% 	Avg.: 75.8% 
	 79.2%  66.9%  77.0%  79.1%  83.2%  66.3%  78.7%  77.1%   0.0%   0.0% 	Avg.: 75.9% 
	 79.2%  66.9%  77.0%  79.1%  83.2%  66.3%  78.7%  77.1%  74.7%   0.0% 	Avg.: 75.8% 
	 79.2%  66.9%  77.0%  79.1%  83.2%  66.3%  78.7%  77.1%  74.7%  71.0% 	Avg.: 75.3% 
************************************************************************************************************
TAg Acc
	 79.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 79.2% 
	 56.4%  52.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 54.3% 
	 45.1%  45.3%  51.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 47.4% 
	 28.2%  32.5%  36.5%  57.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 38.6% 
	 22.3%  28.1%  31.1%  52.6%  50.4%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 36.9% 
	 19.1%  24.9%  26.8%  46.4%  45.3%  32.8%   0.0%   0.0%   0.0%   0.0% 	Avg.: 32.6% 
	 15.2%  22.7%  22.8%  41.0%  42.4%  26.6%  45.8%   0.0%   0.0%   0.0% 	Avg.: 30.9% 
	 12.5%  19.7%  21.2%  38.6%  37.0%  23.8%  43.0%  33.8%   0.0%   0.0% 	Avg.: 28.7% 
	  9.3%  19.3%  20.5%  38.1%  34.2%  22.8%  40.9%  30.9%  23.5%   0.0% 	Avg.: 26.6% 
	  7.2%  18.4%  18.7%  37.2%  30.5%  20.6%  38.4%  29.0%  22.1%  33.0% 	Avg.: 25.5% 
************************************************************************************************************
TAw Forg
	  0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 
	  0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.0% 
	  0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.0% 
	  0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.0% 
	  0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.0% 
	  0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.0% 
	  0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.0% 
	  0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.0% 
	  0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.0% 
	  0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.0% 
************************************************************************************************************
TAg Forg
	  0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 
	 22.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 22.8% 
	 34.1%   6.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 20.5% 
	 51.0%  19.7%  15.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 28.7% 
	 56.9%  24.1%  20.8%   4.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 26.6% 
	 60.1%  27.3%  25.1%  10.8%   5.1%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 25.7% 
	 64.0%  29.5%  29.1%  16.2%   8.0%   6.2%   0.0%   0.0%   0.0%   0.0% 	Avg.: 25.5% 
	 66.7%  32.5%  30.7%  18.6%  13.4%   9.0%   2.8%   0.0%   0.0%   0.0% 	Avg.: 24.8% 
	 69.9%  32.9%  31.4%  19.1%  16.2%  10.0%   4.9%   2.9%   0.0%   0.0% 	Avg.: 23.4% 
	 72.0%  33.8%  33.2%  20.0%  19.9%  12.2%   7.4%   4.8%   1.4%   0.0% 	Avg.: 22.7% 
************************************************************************************************************
>> Task 0, SOW-0: Rank = 736 (896)
>> Task 1, SOW-0: Rank = 832 (896)
>> Task 2, SOW-0: Rank = 512 (896)
>> Task 3, SOW-0: Rank = 896 (896)
>> Task 4, SOW-0: Rank = 896 (896)
>> Task 5, SOW-0: Rank = 608 (896)
>> Task 6, SOW-0: Rank = 896 (896)
>> Task 7, SOW-0: Rank = 896 (896)
>> Task 8, SOW-0: Rank = 736 (896)
>> Task 9, SOW-0: Rank = 512 (896)
************************************************************************************************************
[Elapsed time = 1.8 h]
Done!
