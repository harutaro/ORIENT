============================================================================================================
Arguments =
	approach: finetuning
	batch_size: 64
	clipping: 1.0
	datasets: ['cifar100']
	eval_on_train: True
	exp_name: None
	fix_bn: True
	gpu: 1
	gridsearch_tasks: -1
	keep_existing_head: False
	last_layer_analysis: False
	log: ['disk']
	lr: 0.0263039750973134
	lr_factor: 3.0
	lr_first: None
	lr_min: 1e-05
	lr_patience: 30
	momentum: 0.9
	multi_softmax: True
	nc_first_task: None
	nepochs: 200
	network: resnet50_32
	no_cudnn_deterministic: False
	num_tasks: 1
	num_workers: 4
	pin_memory: True
	pretrained: False
	results_path: ../RESULT_AAAI2026/CR1/0
	save_models: True
	seed: 0
	stop_at_task: 0
	validation: 0.1
	warmup_lr_factor: 1.0
	warmup_nepochs: 0
	weight_decay: 0.0
============================================================================================================
	device: cuda:1
============================================================================================================
Network arguments =
	dropout: 0.417598542370663
	fix_features: True
	load_features: True
	pretrained_path: ../Conv-Model/ResNet50-TinyImageNet.pt
============================================================================================================
ResNet50_32.__init__: features is loadded (../Conv-Model/ResNet50-TinyImageNet.pt)
Fix_Features: Done
Approach arguments =
	all_outputs: False
============================================================================================================
Exemplars dataset arguments =
	exemplar_selection: random
	num_exemplars: 0
	num_exemplars_per_class: 0
============================================================================================================
class_indices: [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
[(0, 100)]
************************************************************************************************************
Task  0
************************************************************************************************************
LLL_Net(
  (model): ResNet50_32(
    (features): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
      (3): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (4): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (3): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (5): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (3): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (4): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (5): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (6): Sequential(
        (0): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
        )
        (1): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
        (2): BottleNeck(
          (residual_function): Sequential(
            (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (2): LeakyReLU(negative_slope=0.01, inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (5): LeakyReLU(negative_slope=0.01, inplace=True)
            (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          )
          (shortcut): Sequential()
        )
      )
      (7): AdaptiveAvgPool2d(output_size=(1, 1))
    )
    (classifier): Sequential(
      (0): Dropout(p=0.417598542370663, inplace=False)
      (1): Linear(in_features=1024, out_features=1024, bias=False)
      (2): ReLU()
    )
    (fc): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1024, out_features=100, bias=True)
  )
)
| Epoch   1, lr=2.6e-02 time= 11.0s/ 17.7s | Train: loss=3.215, TAw acc= 22.6% | Valid: time=  2.2s loss=2.678, TAw acc= 29.6% | *
| Epoch   2, lr=2.6e-02 time= 10.3s/ 17.1s | Train: loss=3.045, TAw acc= 25.8% | Valid: time=  2.3s loss=2.561, TAw acc= 33.8% | *
| Epoch   3, lr=2.6e-02 time=  8.4s/ 17.7s | Train: loss=2.955, TAw acc= 27.2% | Valid: time=  2.1s loss=2.467, TAw acc= 36.8% | *
| Epoch   4, lr=2.6e-02 time= 10.1s/ 17.9s | Train: loss=2.902, TAw acc= 28.5% | Valid: time=  2.2s loss=2.411, TAw acc= 38.2% | *
| Epoch   5, lr=2.6e-02 time= 10.2s/ 17.8s | Train: loss=2.862, TAw acc= 29.5% | Valid: time=  2.3s loss=2.416, TAw acc= 38.3% |
| Epoch   6, lr=2.6e-02 time= 10.2s/ 17.7s | Train: loss=2.832, TAw acc= 29.9% | Valid: time=  2.1s loss=2.322, TAw acc= 39.8% | *
| Epoch   7, lr=2.6e-02 time= 10.2s/ 17.0s | Train: loss=2.799, TAw acc= 30.7% | Valid: time=  2.3s loss=2.341, TAw acc= 39.7% |
| Epoch   8, lr=2.6e-02 time= 10.1s/ 18.0s | Train: loss=2.779, TAw acc= 31.1% | Valid: time=  1.9s loss=2.295, TAw acc= 40.6% | *
| Epoch   9, lr=2.6e-02 time= 10.2s/ 18.0s | Train: loss=2.755, TAw acc= 31.4% | Valid: time=  2.3s loss=2.288, TAw acc= 40.9% | *
| Epoch  10, lr=2.6e-02 time=  9.9s/ 17.8s | Train: loss=2.738, TAw acc= 32.3% | Valid: time=  2.4s loss=2.263, TAw acc= 41.7% | *
| Epoch  11, lr=2.6e-02 time= 10.1s/ 18.1s | Train: loss=2.727, TAw acc= 32.4% | Valid: time=  2.1s loss=2.227, TAw acc= 42.1% | *
| Epoch  12, lr=2.6e-02 time=  9.9s/ 17.8s | Train: loss=2.696, TAw acc= 32.7% | Valid: time=  2.3s loss=2.239, TAw acc= 42.4% |
| Epoch  13, lr=2.6e-02 time= 10.4s/ 17.2s | Train: loss=2.674, TAw acc= 33.4% | Valid: time=  2.4s loss=2.214, TAw acc= 43.2% | *
| Epoch  14, lr=2.6e-02 time= 10.0s/ 17.7s | Train: loss=2.675, TAw acc= 33.1% | Valid: time=  2.1s loss=2.214, TAw acc= 42.5% |
| Epoch  15, lr=2.6e-02 time= 10.3s/ 18.0s | Train: loss=2.654, TAw acc= 33.7% | Valid: time=  2.0s loss=2.240, TAw acc= 42.9% |
| Epoch  16, lr=2.6e-02 time= 10.0s/ 17.9s | Train: loss=2.652, TAw acc= 33.4% | Valid: time=  2.3s loss=2.168, TAw acc= 43.5% | *
| Epoch  17, lr=2.6e-02 time= 10.3s/ 17.6s | Train: loss=2.643, TAw acc= 34.3% | Valid: time=  2.3s loss=2.176, TAw acc= 44.0% |
| Epoch  18, lr=2.6e-02 time= 10.1s/ 17.7s | Train: loss=2.640, TAw acc= 34.1% | Valid: time=  2.1s loss=2.197, TAw acc= 42.4% |
| Epoch  19, lr=2.6e-02 time= 10.4s/ 17.7s | Train: loss=2.627, TAw acc= 34.6% | Valid: time=  2.0s loss=2.138, TAw acc= 44.3% | *
| Epoch  20, lr=2.6e-02 time=  9.8s/ 17.9s | Train: loss=2.603, TAw acc= 35.0% | Valid: time=  1.6s loss=2.200, TAw acc= 43.5% |
| Epoch  21, lr=2.6e-02 time= 10.1s/ 17.7s | Train: loss=2.615, TAw acc= 34.7% | Valid: time=  2.3s loss=2.181, TAw acc= 43.3% |
| Epoch  22, lr=2.6e-02 time= 10.1s/ 15.5s | Train: loss=2.599, TAw acc= 34.6% | Valid: time=  2.0s loss=2.167, TAw acc= 44.0% |
| Epoch  23, lr=2.6e-02 time=  9.8s/ 17.5s | Train: loss=2.595, TAw acc= 34.8% | Valid: time=  2.2s loss=2.165, TAw acc= 44.1% |
| Epoch  24, lr=2.6e-02 time=  9.0s/ 18.0s | Train: loss=2.593, TAw acc= 35.0% | Valid: time=  2.0s loss=2.179, TAw acc= 43.9% |
| Epoch  25, lr=2.6e-02 time= 10.3s/ 17.0s | Train: loss=2.577, TAw acc= 35.6% | Valid: time=  2.3s loss=2.208, TAw acc= 43.5% |
| Epoch  26, lr=2.6e-02 time= 10.1s/ 17.3s | Train: loss=2.560, TAw acc= 35.8% | Valid: time=  2.3s loss=2.187, TAw acc= 44.2% |
| Epoch  27, lr=2.6e-02 time= 10.1s/ 17.4s | Train: loss=2.558, TAw acc= 36.3% | Valid: time=  2.1s loss=2.252, TAw acc= 43.1% |
| Epoch  28, lr=2.6e-02 time=  9.9s/ 17.6s | Train: loss=2.561, TAw acc= 36.0% | Valid: time=  2.1s loss=2.197, TAw acc= 43.5% |
| Epoch  29, lr=2.6e-02 time= 10.1s/ 17.0s | Train: loss=2.555, TAw acc= 35.9% | Valid: time=  2.2s loss=2.156, TAw acc= 44.1% |
| Epoch  30, lr=2.6e-02 time=  9.8s/ 17.7s | Train: loss=2.544, TAw acc= 36.3% | Valid: time=  1.9s loss=2.186, TAw acc= 43.8% |
| Epoch  31, lr=2.6e-02 time= 10.3s/ 17.4s | Train: loss=2.545, TAw acc= 36.6% | Valid: time=  2.3s loss=2.135, TAw acc= 44.5% | *
| Epoch  32, lr=2.6e-02 time= 10.3s/ 17.4s | Train: loss=2.548, TAw acc= 36.3% | Valid: time=  2.2s loss=2.140, TAw acc= 45.2% |
| Epoch  33, lr=2.6e-02 time= 10.0s/ 17.3s | Train: loss=2.528, TAw acc= 36.6% | Valid: time=  2.3s loss=2.153, TAw acc= 44.6% |
| Epoch  34, lr=2.6e-02 time= 10.1s/ 17.6s | Train: loss=2.518, TAw acc= 37.1% | Valid: time=  2.1s loss=2.159, TAw acc= 44.5% |
| Epoch  35, lr=2.6e-02 time= 10.0s/ 18.3s | Train: loss=2.523, TAw acc= 36.7% | Valid: time=  1.7s loss=2.195, TAw acc= 43.3% |
| Epoch  36, lr=2.6e-02 time= 10.1s/ 18.0s | Train: loss=2.535, TAw acc= 36.7% | Valid: time=  2.3s loss=2.115, TAw acc= 44.8% | *
| Epoch  37, lr=2.6e-02 time= 10.5s/ 17.3s | Train: loss=2.512, TAw acc= 36.7% | Valid: time=  2.3s loss=2.139, TAw acc= 44.6% |
| Epoch  38, lr=2.6e-02 time= 10.5s/ 17.5s | Train: loss=2.534, TAw acc= 36.4% | Valid: time=  2.1s loss=2.166, TAw acc= 44.4% |
| Epoch  39, lr=2.6e-02 time=  9.8s/ 17.1s | Train: loss=2.510, TAw acc= 36.8% | Valid: time=  2.3s loss=2.145, TAw acc= 45.0% |
| Epoch  40, lr=2.6e-02 time= 10.5s/ 17.7s | Train: loss=2.502, TAw acc= 37.3% | Valid: time=  2.3s loss=2.147, TAw acc= 44.6% |
| Epoch  41, lr=2.6e-02 time= 10.1s/ 17.6s | Train: loss=2.500, TAw acc= 37.6% | Valid: time=  2.2s loss=2.148, TAw acc= 44.4% |
| Epoch  42, lr=2.6e-02 time= 10.4s/ 17.8s | Train: loss=2.490, TAw acc= 37.5% | Valid: time=  2.2s loss=2.160, TAw acc= 44.2% |
| Epoch  43, lr=2.6e-02 time= 10.1s/ 17.2s | Train: loss=2.477, TAw acc= 37.8% | Valid: time=  2.2s loss=2.142, TAw acc= 44.8% |
| Epoch  44, lr=2.6e-02 time=  9.9s/ 17.7s | Train: loss=2.472, TAw acc= 37.9% | Valid: time=  2.3s loss=2.166, TAw acc= 44.5% |
| Epoch  45, lr=2.6e-02 time= 10.3s/ 14.5s | Train: loss=2.497, TAw acc= 37.4% | Valid: time=  2.2s loss=2.154, TAw acc= 44.6% |
| Epoch  46, lr=2.6e-02 time=  9.8s/ 17.7s | Train: loss=2.490, TAw acc= 37.5% | Valid: time=  2.1s loss=2.169, TAw acc= 44.8% |
| Epoch  47, lr=2.6e-02 time= 10.2s/ 17.6s | Train: loss=2.474, TAw acc= 37.5% | Valid: time=  2.3s loss=2.129, TAw acc= 45.2% |
| Epoch  48, lr=2.6e-02 time= 10.2s/ 18.3s | Train: loss=2.471, TAw acc= 37.8% | Valid: time=  2.3s loss=2.121, TAw acc= 45.9% |
| Epoch  49, lr=2.6e-02 time= 10.3s/ 17.8s | Train: loss=2.480, TAw acc= 37.5% | Valid: time=  2.1s loss=2.164, TAw acc= 44.9% |
| Epoch  50, lr=2.6e-02 time= 10.0s/ 17.4s | Train: loss=2.475, TAw acc= 38.1% | Valid: time=  2.2s loss=2.151, TAw acc= 44.8% |
| Epoch  51, lr=2.6e-02 time= 10.3s/ 18.1s | Train: loss=2.457, TAw acc= 38.4% | Valid: time=  2.1s loss=2.148, TAw acc= 44.5% |
| Epoch  52, lr=2.6e-02 time= 10.1s/ 17.4s | Train: loss=2.450, TAw acc= 38.2% | Valid: time=  2.0s loss=2.195, TAw acc= 44.6% |
