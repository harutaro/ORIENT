from argparse import ArgumentParser

import torch
import torch.nn as nn
import torch.nn.functional as F

__all__ = ("alexnet_32")

class AlexNet_32(nn.Module):
    """Cifar-10, Cifar-100 用 Alexnet"""

    def __init__(self, device='cuda', num_classes=1000, dropout=0.5, fix_features=False,
                 load_features=False, pretrained_path='', **kwargs):
        super().__init__()
        # パラメータdropoutが、単数の場合と複数の場合の両方に対応する
        if type(dropout) is list:
            d1 = dropout[0]
            d2 = dropout[1]
        elif type(dropout) is float:
            d1 = dropout
            d2 = dropout
        else:
            assert False

        self.fix_features = fix_features
        self.fix_classifier = False
        self.features = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),  # 64,23,23
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),  # 64,16,16
            nn.Conv2d(64, 192, kernel_size=3, stride=1, padding=1),  # 192,16,16
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),  # 192,8,8
            nn.Conv2d(192, 384, kernel_size=3, stride=1, padding=1),  # 384,8,8
            nn.ReLU(inplace=True),
            nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),  # 256,8,8
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),  # 256,8,8
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2)  # 256,4,4
        )
        self.classifier = nn.Sequential(
            nn.Dropout(p=d1),
            nn.Linear(256 * 4 * 4, 4096, dtype=torch.float64, bias=False),
            nn.ReLU(),
            nn.Dropout(p=d2),
            nn.Linear(4096, 4096, dtype=torch.float64, bias=False),
            nn.ReLU()
        )
        # last classifier layer (head) with as many outputs as classes
        self.fc = nn.Linear(4096, num_classes, bias=True)
        # and `head_var` with the name of the head, so it can be removed when doing incremental learning experiments
        self.head_var = "fc"

        if pretrained_path:
            state_dict = torch.load(pretrained_path, map_location=device)
            # HACK: state_dictからfeatures,classifierを取り出すようにし、
            # model.load_state_dict()に取り出したものを適用するという感じにしたい
            if load_features:
                self.features.load_state_dict(state_dict)
                print(F'features is loaded ({pretrained_path})')
        if self.fix_features: self.Fix_Features()

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = x.to(dtype=torch.float64)	# to float64
        x = self.classifier(x)
        x = x.to(dtype=torch.float)	# to float
        x = self.fc(x)
        return x

    def Fix_Features(self):
        self.features.eval()
        for param in self.features.parameters():
            param.requires_grad = False
        print("Fix_Features: Done")
        #print(F'{self.features} --> freezed')

    def Fix_Classifier(self):
        self.classifier.eval()
        for params in self.classifier.parameters():
            params.requires_grad = False
        print("Fix_Classifier: Done")
        #print(F'{self.classifier} --> freezed')

    @classmethod
    def extra_parser(self, args):
        parser = ArgumentParser()
        parser.add_argument(
            "--pretrained-path",
            type=str,
            help="pretrained model path (use with --pretrained) (default: %(default)s)",
        )
        parser.add_argument(
            "--load-features",
            action='store_true',
            required=False,
            help="Load feature parameters from pretrained model, or no load. (default: %(default)s)",
        )
        parser.add_argument(
            "--fix-features",
            action='store_true',
            required=False,
            help="Fix not to calculate feature gradients when load pretrained model, or no fix. (default: %(default)s)",
        )
        parser.add_argument("--dropout", nargs="+", type=float)	# dropoutの複数指定対応
        return parser.parse_known_args(args)

    def save_pretrained(self, save_path):
        state_dict = self.features.state_dict()
        torch.save(state_dict, save_path)

def alexnet_32(device=None, **kwargs):
    return AlexNet_32(device, **kwargs)
