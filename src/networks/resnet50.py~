from argparse import ArgumentParser

import torch
from torch import nn, Tensor
from typing import Union
#from .base import Classifier

__all__ = ('resnet50_32')


class BasicBlock(nn.Module):
    """Basic Block for resnet 18 and resnet 34"""

    # BasicBlock and BottleNeck block
    # have different output size
    # we use class attribute expansion
    # to distinct
    expansion = 1

    def __init__(self, in_channels: int, out_channels: int, stride: int = 1):
        super().__init__()

        # residual function
        self.residual_function = nn.Sequential(
            nn.Conv2d(
                in_channels,
                out_channels,
                kernel_size=3,
                stride=stride,
                padding=1,
                bias=False,
            ),
            nn.BatchNorm2d(out_channels),
            nn.LeakyReLU(inplace=True),
            nn.Conv2d(
                out_channels,
                out_channels * BasicBlock.expansion,
                kernel_size=3,
                padding=1,
                bias=False,
            ),
            nn.BatchNorm2d(out_channels * BasicBlock.expansion),
        )

        # shortcut
        self.shortcut = nn.Sequential()

        # the shortcut output dimension is not the same with residual function
        # use 1*1 convolution to match the dimension
        if stride != 1 or in_channels != BasicBlock.expansion * out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(
                    in_channels,
                    out_channels * BasicBlock.expansion,
                    kernel_size=1,
                    stride=stride,
                    bias=False,
                ),
                nn.BatchNorm2d(out_channels * BasicBlock.expansion),
            )

    def forward(self, x: Tensor) -> Tensor:
        return nn.LeakyReLU(inplace=True)(self.residual_function(x) + self.shortcut(x))


class BottleNeck(nn.Module):
    """Residual block for resnet over 50 layers"""

    expansion = 2

    def __init__(self, in_channels: int, out_channels: int, stride: int = 1):
        super().__init__()
        self.residual_function = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.LeakyReLU(inplace=True),
            nn.Conv2d(
                out_channels,
                out_channels,
                stride=stride,
                kernel_size=3,
                padding=1,
                bias=False,
            ),
            nn.BatchNorm2d(out_channels),
            nn.LeakyReLU(inplace=True),
            nn.Conv2d(
                out_channels,
                out_channels * BottleNeck.expansion,
                kernel_size=1,
                bias=False,
            ),
            nn.BatchNorm2d(out_channels * BottleNeck.expansion),
        )

        self.shortcut = nn.Sequential()

        if stride != 1 or in_channels != out_channels * BottleNeck.expansion:
            self.shortcut = nn.Sequential(
                nn.Conv2d(
                    in_channels,
                    out_channels * BottleNeck.expansion,
                    stride=stride,
                    kernel_size=1,
                    bias=False,
                ),
                nn.BatchNorm2d(out_channels * BottleNeck.expansion),
            )

    def forward(self, x: Tensor) -> Tensor:
        return nn.LeakyReLU(inplace=True)(self.residual_function(x) + self.shortcut(x))


class ResNet50_32(nn.Module):
    def __init__(self, block: Union[BasicBlock, BottleNeck], num_block: list,
                 device='cuda', num_classes=1000, dropout=0.5, fix_features=False,
                 load_features=False, pretrained_path='', **kwargs):
        super().__init__()

        self.in_channels = 64
        self.features = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(64),
            nn.LeakyReLU(inplace=True),
            self._make_layer(block, 64, num_block[0], 1),  # 64,3
            self._make_layer(block, 128, num_block[1], 2),  # 128,4
            self._make_layer(block, 256, num_block[2], 2),  # 256,6
            self._make_layer(block, 512, num_block[3], 2),  # 512,3
            # we use a different inputsize than the original paper
            # so conv2_x's stride is 1
            nn.AdaptiveAvgPool2d((1, 1))
        )
        self.classifier = nn.Sequential(
            nn.Dropout(p=dropout),
            nn.Linear(512 * block.expansion, 512 * block.expansion, device=device, dtype=torch.float64, bias=False),
            nn.ReLU()
        )  # 512,2
        # last classifier layer (head) with as many outputs as classes
        self.fc = nn.Linear(512 * block.expansion, num_classes, bias=True)
        # and `head_var` with the name of the head, so it can be removed when doing incremental learning experiments
        self.head_var = "fc"

        if pretrained_path:
            state_dict = torch.load(pretrained_path, map_location=device)
            # HACK: state_dictからfeatures,classifierを取り出すようにし、
            # model.load_state_dict()に取り出したものを適用するという感じにしたい
            if load_features:
                self.features.load_state_dict(state_dict)
                print(F'ResNet50_32.__init__: features is loadded ({pretrained_path})')

        if fix_features: self.Fix_Features()

    def _make_layer(
        self,
        block: Union[BasicBlock, BottleNeck],
        out_channels: int,
        num_blocks: int,
        stride: int,
    ):
        """make resnet layers(by layer i didnt mean this 'layer' was the
        same as a neuron netowork layer, ex. conv layer), one layer may
        contain more than one residual block
        Args:
            block: block type, basic block or bottle neck block
            out_channels: output depth channel number of this layer
            num_blocks: how many blocks per layer
            stride: the stride of the first block of this layer
        Return:
            return a resnet layer
        """

        # we have num_block blocks per layer, the first block
        # could be 1 or 2, other blocks would always be 1
        strides = [stride] + [1] * (num_blocks - 1)
        layers = []
        for stride in strides:
            layers.append(block(self.in_channels, out_channels, stride))
            self.in_channels = out_channels * block.expansion

        return nn.Sequential(*layers)

    def forward(self, x: Tensor) -> Tensor:
        #print(type(x))
        #print(x.shape, x.dtype, x.device)
        #print(self.features[0])
        #print(self.features[0].weight)
        #print(self.features[0].weight.shape, self.features[0].weight.dtype, self.features[0].weight.device)
        #print(self.features[0].bias)
        #self.features[0](x)
        #exit()
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = x.to(dtype=torch.float64)  # to float64
        x = self.classifier(x)
        x = x.to(dtype=torch.float)    # to float
        x = self.fc(x)
        return x

    def Fix_Features(self):
        self.features.eval()
        for param in self.features.parameters():
            param.requires_grad = False
        print("Fix_Features: Done")
        #print(F'{self.features} --> freezed')

    @classmethod
    def extra_parser(self, args):
        parser = ArgumentParser()
        parser.add_argument(
            "--pretrained-path",
            type=str,
            help="pretrained model path (use with --pretrained) (default: %(default)s)",
        )
        parser.add_argument(
            "--load-features",
            action='store_true',
            required=False,
            help="Load feature parameters from pretrained model, or no load. (default: %(default)s)",
        )
        parser.add_argument(
            "--fix-features",
            action='store_true',
            required=False,
            help="Fix not to calculate feature gradients when load pretrained model, or no fix. (default: %(default)s)",
        )

        parser.add_argument("--dropout", type=float, default=0.5)
        return parser.parse_known_args(args)

def resnet50_32(device=None, **kwargs):
    return ResNet50_32(BottleNeck, [3, 4, 6, 3], device, **kwargs)
