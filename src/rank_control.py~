#

import os
import time
import torch
import argparse
import importlib
import numpy as np
from functools import reduce
import sys
from collections import OrderedDict
from typing import Union

import utils
import approach
from loggers.exp_logger import MultiLogger
from datasets.data_loader import get_loaders
from datasets.dataset_config import dataset_config
from last_layer_analysis import last_layer_analysis
from networks import tvmodels, allmodels, set_tvmodel_head_var
from networks import extra_parser as network_extra_parser

def main(argv=None):
    tstart = time.time()
    args, extra_args = utils.get_exec_args()
    args.results_path = os.path.expanduser(args.results_path)
    base_kwargs = dict(nepochs=args.nepochs, lr=args.lr, lr_min=args.lr_min, lr_factor=args.lr_factor,
                       lr_patience=args.lr_patience, clipgrad=args.clipping, momentum=args.momentum,
                       wd=args.weight_decay, multi_softmax=args.multi_softmax, wu_nepochs=args.warmup_nepochs,
                       wu_lr_factor=args.warmup_lr_factor, fix_bn=args.fix_bn, eval_on_train=args.eval_on_train)

    # Log all arguments
    full_exp_name = reduce((lambda x, y: x[0] + y[0]), args.datasets) if len(args.datasets) > 0 else args.datasets[0]
    full_exp_name += '_' + args.approach
    if args.exp_name is not None:
        full_exp_name += '_' + args.exp_name
    logger = MultiLogger(args.results_path, full_exp_name, loggers=args.log, save_models=args.save_models)

    if args.no_cudnn_deterministic:
        print('WARNING: CUDNN Deterministic will be disabled.')
        utils.cudnn_deterministic = False

    utils.seed_everything(seed=args.seed)
    print('=' * 108)
    print('Arguments =')
    for arg in np.sort(list(vars(args).keys())):
        print('\t' + arg + ':', getattr(args, arg))
    print('=' * 108)

    # Args -- CUDA
    if torch.cuda.is_available():
        torch.cuda.set_device(args.gpu)
        device = torch.device(F'cuda:{args.gpu}')
    else:
        print('WARNING: [CUDA unavailable] Using CPU instead!')
        device = torch.device('cpu')
    print('\tdevice:', device)
    print('=' * 108)
    # Multiple gpus
    # if torch.cuda.device_count() > 1:
    #     self.C = torch.nn.DataParallel(C)
    #     self.C.to(self.device)
    ####################################################################################################################

    # Args -- Network
    from networks.network import LLL_Net
    network_args, extra_args = network_extra_parser(args.network, extra_args)
    print('Network arguments =')
    for arg in np.sort(list(vars(network_args).keys())):
        print('\t' + arg + ':', getattr(network_args, arg))
    print('=' * 108)
    if args.network in tvmodels:  # torchvision models
        tvnet = getattr(importlib.import_module(name='torchvision.models'), args.network)
        print(tvnet)
        if args.network == 'googlenet':
            init_model = tvnet(pretrained=args.pretrained, aux_logits=False)
        else:
            init_model = tvnet(pretrained=args.pretrained)
        set_tvmodel_head_var(init_model)
    else:  # other models declared in networks package's init
        net = getattr(importlib.import_module(name='networks'), args.network)
        if args.network in ("alexnet_32", "resnet50_32"):
            network_kwargs = {**base_kwargs, **dict(network_args.__dict__)}
            init_model = net(device=device, **network_kwargs)
        elif args.network in ("alexnet_32supsup", "resnet50_32supsup"):
            network_kwargs = {**base_kwargs, **dict(network_args.__dict__)}
            init_model = net(device=device, num_tasks=args.num_tasks, **network_kwargs)
        elif args.network in ("alexnet_32wsn", "resnet50_32wsn"):
            network_kwargs = {**base_kwargs, **dict(network_args.__dict__)}
            init_model = net(device=device, num_tasks=args.num_tasks, **network_kwargs)
        elif args.network in ("alexnet_32hat", "resnet50_32hat"):
            network_kwargs = {**base_kwargs, **dict(network_args.__dict__)}
            init_model = net(device=device, num_tasks=args.num_tasks, **network_kwargs)
        elif args.network in ("alexnet_32sow", "resnet50_32sow"):
            network_kwargs = {**base_kwargs, **dict(network_args.__dict__)}
            init_model = net(device=device, num_tasks=args.num_tasks, **network_kwargs)
        else:
            # WARNING: fixed to pretrained False for other model (non-torchvision)
            init_model = net(pretrained=False)


    # Args -- Continual Learning Approach
    from approach.incremental_learning import Inc_Learning_Appr
    Appr = getattr(importlib.import_module(name='approach.' + args.approach), 'Appr')
    assert issubclass(Appr, Inc_Learning_Appr)
    appr_args, extra_args = Appr.extra_parser(extra_args)
    print('Approach arguments =')
    for arg in np.sort(list(vars(appr_args).keys())):
        print('\t' + arg + ':', getattr(appr_args, arg))
    print('=' * 108)

    # Args -- Exemplars Management
    from datasets.exemplars_dataset import ExemplarsDataset
    Appr_ExemplarsDataset = Appr.exemplars_dataset_class()
    if Appr_ExemplarsDataset:
        assert issubclass(Appr_ExemplarsDataset, ExemplarsDataset)
        appr_exemplars_dataset_args, extra_args = Appr_ExemplarsDataset.extra_parser(extra_args)
        print('Exemplars dataset arguments =')
        for arg in np.sort(list(vars(appr_exemplars_dataset_args).keys())):
            print('\t' + arg + ':', getattr(appr_exemplars_dataset_args, arg))
        print('=' * 108)
    else:
        appr_exemplars_dataset_args = argparse.Namespace()

    logger.log_args(
        argparse.Namespace(
            **args.__dict__,
            **appr_args.__dict__,
            **appr_exemplars_dataset_args.__dict__,
            **network_args.__dict__,
        )
    )

    # Args -- GridSearch
    if args.gridsearch_tasks > 0:
        from facil_kddi.gridsearch import GridSearch
        gs_args, extra_args = GridSearch.extra_parser(extra_args)
        Appr_finetuning = getattr(importlib.import_module(name="facil_kddi.approach.finetuning"), "Appr")
        assert issubclass(Appr_finetuning, Inc_Learning_Appr)
        GridSearch_ExemplarsDataset = Appr.exemplars_dataset_class()
        print('GridSearch arguments =')
        for arg in np.sort(list(vars(gs_args).keys())):
            print('\t' + arg + ':', getattr(gs_args, arg))
        print('=' * 108)

    assert len(extra_args) == 0, "Unused args: {}".format(' '.join(extra_args))
    ####################################################################################################################

    # Loaders
    utils.seed_everything(seed=args.seed)
    trn_loader, val_loader, tst_loader, taskcla = get_loaders(args.datasets, args.num_tasks, args.nc_first_task,
                                                              args.batch_size, num_workers=args.num_workers,
                                                              pin_memory=args.pin_memory, validation=args.validation)
    # Apply arguments for loaders
    max_task = len(taskcla) if args.stop_at_task == 0 else args.stop_at_task

    # Network and Approach instances
    utils.seed_everything(seed=args.seed)
    net = LLL_Net(init_model, remove_existing_head=not args.keep_existing_head)
    utils.seed_everything(seed=args.seed)
    # taking transformations and class indices from first train dataset
    first_train_ds = trn_loader[0].dataset
    transform, class_indices = first_train_ds.transform, first_train_ds.class_indices
    appr_kwargs = {**base_kwargs, **dict(logger=logger, **appr_args.__dict__)}
    if Appr_ExemplarsDataset:
        appr_kwargs['exemplars_dataset'] = Appr_ExemplarsDataset(transform, class_indices,
                                                                 **appr_exemplars_dataset_args.__dict__)
    utils.seed_everything(seed=args.seed)
    appr = Appr(net, device, **appr_kwargs)
    print('class_indices:', class_indices)
    # GridSearch
    if args.gridsearch_tasks > 0:
        ft_kwargs = {**base_kwargs, **dict(logger=logger,
                                           exemplars_dataset=GridSearch_ExemplarsDataset(transform, class_indices))}
        appr_ft = Appr_finetuning(net, device, **ft_kwargs)
        gridsearch = GridSearch(appr_ft, args.seed, gs_args.gridsearch_config, gs_args.gridsearch_acc_drop_thr,
                                gs_args.gridsearch_hparam_decay, gs_args.gridsearch_max_num_searches)

    # Add head for current task
    print(taskcla)
    net.add_head(taskcla[0][1])
    net.to(device)
    print(net)

    # Load Model
    model_path = os.path.join(args.results_path, full_exp_name, "models/task0.ckpt")
    print(model_path)
    exit()

    # Test
    for n, sow in enumerate(appr.model.model.ListCustomModules()) :
        print(f'[{n}] ', sow)
        for r in range(32, net.model.classifier[1].max_rank+1, 32):
            sow.rank = r
            sow.freezed_rank = sow.max_rank
            '''
            state_dict = torch.load(model_path, map_location=device)
            tan_2phi = state_dict['model.classifier.1.tan_2phi']
            a = state_dict['model.classifier.1.a']
            tan_2theta = state_dict['model.classifier.1.tan_2theta']
            for m in [tan_2phi, a, tan_2theta]:
            print(type(m), m.shape, m.dtype, m.device, m.requires_grad)
            '''
            net.load_state_dict(torch.load(model_path, map_location=device))
            #print('a:', net.model.classifier[1].a)
            #print(f' max_rank={sow.max_rank}, rank={sow.rank}, freezed={sow.freezed_rank}')

            #print('== Eval with No Header Training for Low Rank (%d) ==' %(sow.rank))
            test_loss1, acc_taw1, acc_tag = appr.eval(0, tst_loader[0])
            #print(f' n=%d  rank=%4d  loss=%.4f  acc=%.4f' % (n, r, test_loss1, acc_taw1))

            #'''
            #print('== Eval with Header Training for Low Rank (%d) ==' %(sow.rank))
            appr.model.model.Fix_Classifier()
            saved_lr = appr.lr
            saved_sow_lr = appr.sow_lr
            appr.lr /= appr.lr_factor
            appr.sow_lr /= appr.lr_factor
            appr.train_loop(0, trn_loader[0], val_loader[0], None)
            appr.lr = saved_lr
            appr.sow_lr = saved_sow_lr
            test_loss2, acc_taw2, acc_tag = appr.eval(0, tst_loader[0])
            print(f' n=%d  rank=%4d  loss=%.4f  acc=%.4f | loss=%.4f  acc=%.4f'
                  % (n, r, test_loss1, acc_taw1, test_loss2, acc_taw2))
            appr.model.model.Free_Classifier() # Never set 'requires_grad=True' for low_rank_a[t]
            #'''

    return
    ####################################################################################################################


if __name__ == '__main__':
    main()
